{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60bf0efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NOTEBOOK SWEEP CONFIG ===\n",
      "Lookbacks: [90]\n",
      "ATR windows: [20]\n",
      "TOP_PERCENTILES: [0.95]\n",
      "MIN_TRADE_VALUES: [5000.0, 10000.0]\n",
      "MAX_POSITION_WEIGHTS: [0.12]\n",
      "MIN_CASH_RESERVES: [5000.0, 10000.0]\n",
      "DRIFT_THRESHOLDS: [0.05, 0.1, 0.02]\n",
      "MIN_NEW_POSITION_WEIGHTS: [0.005, 0.01, 0.02, 0.03]\n",
      "MAX_WORKERS: 4\n",
      "Total runs expected: 48\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# CELL 1/5 — CONFIG (UPDATED: new sweep grids)\n",
    "# ============================\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------------------\n",
    "# PATHS\n",
    "# ----------------------------\n",
    "UNIVERSE_DIR = \"./12a-multiple-tradable_sp500_universe\"\n",
    "ATR_ROOT     = \"./4-ATR20_adjusted_All_Prices\"\n",
    "SPY_FILE     = \"./8-SPY_200DMA_market_regime/8-SPY_200DMA_regime.parquet\"\n",
    "\n",
    "OUT_DIR      = \"./13a-trading_output_sweep_performance_daily_returns\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# SWEEP GRID (existing)\n",
    "# ----------------------------\n",
    "REG_LOOKBACKS    = [90]\n",
    "ATR_WINDOWS      = [20]\n",
    "TOP_PERCENTILES  = [0.95]\n",
    "MIN_TRADE_VALUES = [5000.0, 10000.0]\n",
    "\n",
    "# ----------------------------\n",
    "# NEW SWEEP GRID (was constant before; now parameterized)\n",
    "# ----------------------------\n",
    "MAX_POSITION_WEIGHTS     = [0.12]\n",
    "MIN_CASH_RESERVES        = [5000.0, 10000.0]\n",
    "DRIFT_THRESHOLDS         = [0.05, 0.1, 0.02]\n",
    "MIN_NEW_POSITION_WEIGHTS = [0.005, 0.01, 0.02, 0.03 ]\n",
    "\n",
    "# ----------------------------\n",
    "# CONSTANTS (remain constant)\n",
    "# ----------------------------\n",
    "START_TRADING         = pd.Timestamp(\"1999-01-01\")\n",
    "INITIAL_CAPITAL       = 360000\n",
    "REBALANCE_DAY         = \"Wednesday\"\n",
    "TRADING_DAYS_PER_YEAR = 252\n",
    "\n",
    "# ----------------------------\n",
    "# OUTPUT: per-config daily returns\n",
    "# ----------------------------\n",
    "WRITE_DAILY_RETURNS = True         # <- turn off if you just want summary\n",
    "DAILY_RETURNS_FMT   = \"parquet\"    # \"parquet\" recommended; \"csv\" also supported\n",
    "DAILY_RETURNS_DIRNAME = \"daily_returns\"  # created as OUT_DIR/daily_returns/<RUN_ID>/\n",
    "\n",
    "# ----------------------------\n",
    "# PARALLELISM (safe notebook default)\n",
    "# If RAM spikes, reduce to 2 or 3.\n",
    "# ----------------------------\n",
    "MAX_WORKERS = 4\n",
    "\n",
    "total_runs_expected = (\n",
    "    len(REG_LOOKBACKS) * len(ATR_WINDOWS) *\n",
    "    len(TOP_PERCENTILES) * len(MIN_TRADE_VALUES) *\n",
    "    len(MAX_POSITION_WEIGHTS) * len(MIN_CASH_RESERVES) *\n",
    "    len(DRIFT_THRESHOLDS) * len(MIN_NEW_POSITION_WEIGHTS)\n",
    ")\n",
    "\n",
    "print(\"=== NOTEBOOK SWEEP CONFIG ===\")\n",
    "print(\"Lookbacks:\", REG_LOOKBACKS)\n",
    "print(\"ATR windows:\", ATR_WINDOWS)\n",
    "print(\"TOP_PERCENTILES:\", TOP_PERCENTILES)\n",
    "print(\"MIN_TRADE_VALUES:\", MIN_TRADE_VALUES)\n",
    "print(\"MAX_POSITION_WEIGHTS:\", MAX_POSITION_WEIGHTS)\n",
    "print(\"MIN_CASH_RESERVES:\", MIN_CASH_RESERVES)\n",
    "print(\"DRIFT_THRESHOLDS:\", DRIFT_THRESHOLDS)\n",
    "print(\"MIN_NEW_POSITION_WEIGHTS:\", MIN_NEW_POSITION_WEIGHTS)\n",
    "print(\"MAX_WORKERS:\", MAX_WORKERS)\n",
    "print(\"Total runs expected:\", total_runs_expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f67c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# CELL 2/5 — HELPERS + JOBLIB (UPDATED: config id includes new params)\n",
    "# ============================\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def resolve_universe_file(lookback: int) -> str:\n",
    "    \"\"\"\n",
    "    Tries common patterns. Falls back to glob search for anything containing '{lookback}D'.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        os.path.join(UNIVERSE_DIR, f\"12-tradable_sp500_universe_{lookback}D.parquet\"),\n",
    "        os.path.join(UNIVERSE_DIR, f\"tradable_sp500_universe_{lookback}D.parquet\"),\n",
    "    ]\n",
    "    for f in candidates:\n",
    "        if os.path.exists(f):\n",
    "            return f\n",
    "\n",
    "    hits = sorted(glob.glob(os.path.join(UNIVERSE_DIR, f\"*{lookback}D*.parquet\")))\n",
    "    if hits:\n",
    "        return hits[0]\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find universe parquet for lookback={lookback} in {UNIVERSE_DIR}\")\n",
    "\n",
    "def resolve_atr_subdir(w: int) -> str:\n",
    "    \"\"\"\n",
    "    Tries a few folder naming conventions. Adjust here if your ATR folders differ.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        os.path.join(ATR_ROOT, f\"atr_{w}D\"),\n",
    "        os.path.join(ATR_ROOT, f\"atr_{w}d\"),\n",
    "        os.path.join(ATR_ROOT, f\"ATR_{w}D\"),\n",
    "        os.path.join(ATR_ROOT, f\"atr{w}D\"),\n",
    "    ]\n",
    "    for d in candidates:\n",
    "        if os.path.isdir(d):\n",
    "            return d\n",
    "\n",
    "    hits = [p for p in glob.glob(os.path.join(ATR_ROOT, \"*\")) if os.path.isdir(p)]\n",
    "    hits = [p for p in hits if (\"atr\" in os.path.basename(p).lower() and str(w) in os.path.basename(p))]\n",
    "    if hits:\n",
    "        return hits[0]\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find ATR folder for w={w} under {ATR_ROOT}\")\n",
    "\n",
    "def infer_atr_col(columns, w: int):\n",
    "    \"\"\"\n",
    "    Infer ATR column name from a parquet file's columns.\n",
    "    Supports: atr20, atr_20, atr20D, atr_20D, atr_20d, atr_20_day, etc.\n",
    "    \"\"\"\n",
    "    cols = list(columns)\n",
    "    cols_l = [c.lower() for c in cols]\n",
    "    w_str = str(w)\n",
    "\n",
    "    preferred = [\n",
    "        f\"atr{w_str}\",\n",
    "        f\"atr_{w_str}\",\n",
    "        f\"atr{w_str}d\",\n",
    "        f\"atr_{w_str}d\",\n",
    "        f\"atr{w_str}_d\",\n",
    "        f\"atr_{w_str}_d\",\n",
    "        f\"atr{w_str}day\",\n",
    "        f\"atr_{w_str}day\",\n",
    "        f\"atr_{w_str}_day\",\n",
    "    ]\n",
    "    for p in preferred:\n",
    "        if p in cols_l:\n",
    "            return cols[cols_l.index(p)]\n",
    "\n",
    "    for i, c in enumerate(cols_l):\n",
    "        if \"atr\" in c and w_str in c:\n",
    "            return cols[i]\n",
    "\n",
    "    return None\n",
    "\n",
    "def load_spy_regime_map(spy_file: str):\n",
    "    spy = pd.read_parquet(spy_file)\n",
    "\n",
    "    if spy.index.name in [\"Date\", \"date\", None]:\n",
    "        spy = spy.reset_index().rename(columns={\"index\": \"date\", \"Date\": \"date\"})\n",
    "\n",
    "    spy[\"date\"] = pd.to_datetime(spy[\"date\"])\n",
    "    if \"spy_close\" not in spy.columns:\n",
    "        raise ValueError(\"SPY file missing 'spy_close' column\")\n",
    "\n",
    "    spy[\"spy_above_200dma\"] = spy[\"market_regime\"].astype(int) == 1\n",
    "    spy_regime_map = spy.set_index(\"date\")[\"spy_above_200dma\"].to_dict()\n",
    "    return spy, spy_regime_map\n",
    "\n",
    "def make_config_id(\n",
    "    lookback: int,\n",
    "    atr_w: int,\n",
    "    top_p: float,\n",
    "    mtv: float,\n",
    "    max_position_weight: float,\n",
    "    min_cash_reserve: float,\n",
    "    drift_threshold: float,\n",
    "    min_new_position_weight: float,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Stable per-config ID used for filenames.\n",
    "    Encodes floats in bps to avoid filename instability.\n",
    "    \"\"\"\n",
    "    tp = int(round(float(top_p) * 1000))          # 0.95 -> 950\n",
    "    mtv_i = int(round(float(mtv)))\n",
    "\n",
    "    cap_bps = int(round(float(max_position_weight) * 10000))       # 0.15 -> 1500\n",
    "    cash_i  = int(round(float(min_cash_reserve)))                  # 4000.0 -> 4000\n",
    "    dr_bps  = int(round(float(drift_threshold) * 10000))           # 0.01 -> 100\n",
    "    mnw_bps = int(round(float(min_new_position_weight) * 10000))   # 0.005 -> 50\n",
    "\n",
    "    return (\n",
    "        f\"lb{int(lookback)}_atr{int(atr_w)}_tp{tp:04d}_mtv{mtv_i}\"\n",
    "        f\"_cap{cap_bps:04d}_cash{cash_i}_dr{dr_bps:04d}_mnw{mnw_bps:04d}\"\n",
    "    )\n",
    "\n",
    "# ---------- joblib progress in notebooks ----------\n",
    "from contextlib import contextmanager\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "@contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_cb = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_cb\n",
    "        tqdm_object.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f92ccd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# CELL 3/5 — ENGINE + METRICS (UPDATED: engine uses passed params)\n",
    "# ============================\n",
    "def fast_price_lookup(px_array, date_val):\n",
    "    date_val = np.datetime64(date_val, \"ns\")\n",
    "    dates = px_array[\"date\"]\n",
    "    idx = np.searchsorted(dates, date_val, side=\"right\") - 1\n",
    "    if idx < 0:\n",
    "        return np.nan\n",
    "    return px_array[\"px\"][idx]\n",
    "\n",
    "def snapshot_portfolio_close(date, cash, positions, px_by_ticker):\n",
    "    equity = 0.0\n",
    "    for t, pos in positions.items():\n",
    "        arr = px_by_ticker.get(t)\n",
    "        if arr is None:\n",
    "            continue\n",
    "        px = fast_price_lookup(arr, date)\n",
    "        if not np.isnan(px):\n",
    "            equity += pos[\"shares\"] * px\n",
    "    return equity, cash + equity, len(positions)\n",
    "\n",
    "def snapshot_portfolio_exec_proxy(asof_date, cash, positions, exec_px_map, px_by_ticker_fallback):\n",
    "    equity = 0.0\n",
    "    for t, pos in positions.items():\n",
    "        px = exec_px_map.get(t, np.nan)\n",
    "        if pd.isna(px) or px <= 0:\n",
    "            arr = px_by_ticker_fallback.get(t)\n",
    "            if arr is not None:\n",
    "                px = fast_price_lookup(arr, asof_date)\n",
    "        if pd.notna(px) and px > 0:\n",
    "            equity += int(pos[\"shares\"]) * float(px)\n",
    "    return equity, cash + equity, len(positions)\n",
    "\n",
    "def is_rebalance_day(date: pd.Timestamp) -> bool:\n",
    "    return date.day_name() == REBALANCE_DAY\n",
    "\n",
    "def cap_and_redistribute_weights(w: np.ndarray, cap: float) -> np.ndarray:\n",
    "    w = np.asarray(w, dtype=float).copy()\n",
    "    if w.size == 0:\n",
    "        return w\n",
    "\n",
    "    s = w.sum()\n",
    "    if s > 0:\n",
    "        w /= s\n",
    "\n",
    "    if w.size * cap < 1.0:\n",
    "        return np.minimum(w, cap)\n",
    "\n",
    "    for _ in range(10_000):\n",
    "        over = w > cap\n",
    "        if not over.any():\n",
    "            break\n",
    "        excess = (w[over] - cap).sum()\n",
    "        w[over] = cap\n",
    "        under = ~over\n",
    "        under_sum = w[under].sum()\n",
    "        if under_sum <= 0:\n",
    "            break\n",
    "        w[under] += excess * (w[under] / under_sum)\n",
    "\n",
    "    return w\n",
    "\n",
    "@dataclass\n",
    "class PreparedContext:\n",
    "    df_by_date: dict\n",
    "    px_by_ticker: dict\n",
    "    dates: list\n",
    "    next_date_map: dict\n",
    "\n",
    "def prepare_context(df: pd.DataFrame) -> PreparedContext:\n",
    "    df_by_date = {d: sub for d, sub in df.groupby(\"date\", sort=False)}\n",
    "\n",
    "    px_by_ticker = {}\n",
    "    for t, sub in df.groupby(\"ticker\", sort=False):\n",
    "        sub = sub.sort_values(\"date\")\n",
    "        arr = np.zeros(len(sub), dtype=[(\"date\", \"datetime64[ns]\"), (\"px\", \"float64\")])\n",
    "        arr[\"date\"] = sub[\"date\"].values.astype(\"datetime64[ns]\")\n",
    "        arr[\"px\"]   = sub[\"close_adj\"].astype(float).values\n",
    "        px_by_ticker[t] = arr\n",
    "\n",
    "    dates = sorted(df_by_date.keys())\n",
    "    next_date_map = {d: dates[i + 1] if i + 1 < len(dates) else None for i, d in enumerate(dates)}\n",
    "    return PreparedContext(df_by_date=df_by_date, px_by_ticker=px_by_ticker, dates=dates, next_date_map=next_date_map)\n",
    "\n",
    "def compute_perf_and_daily(equity_df: pd.DataFrame, spy_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      metrics: dict\n",
    "      daily_df: DataFrame with date, portfolio_value, strat_ret, spy_close, spy_ret, cash, num_positions\n",
    "    \"\"\"\n",
    "    equity_df = equity_df.copy()\n",
    "    equity_df[\"date\"] = pd.to_datetime(equity_df[\"date\"])\n",
    "\n",
    "    spy_use = spy_df[[\"date\", \"spy_close\"]].copy()\n",
    "    spy_use[\"date\"] = pd.to_datetime(spy_use[\"date\"])\n",
    "\n",
    "    df_perf = equity_df.merge(spy_use, on=\"date\", how=\"inner\").sort_values(\"date\")\n",
    "    if df_perf.empty:\n",
    "        return None, None\n",
    "\n",
    "    df_perf[\"strat_ret\"] = df_perf[\"portfolio_value\"].pct_change().fillna(0.0)\n",
    "    df_perf[\"spy_ret\"]   = df_perf[\"spy_close\"].pct_change().fillna(0.0)\n",
    "\n",
    "    def cagr(total_return, n_years):\n",
    "        return (1 + total_return) ** (1 / n_years) - 1 if n_years > 0 else np.nan\n",
    "\n",
    "    def max_drawdown(series):\n",
    "        roll_max = series.cummax()\n",
    "        dd = series / roll_max - 1\n",
    "        return dd.min()\n",
    "\n",
    "    def sharpe(returns, rf=0.0):\n",
    "        sd = returns.std()\n",
    "        if sd == 0 or np.isnan(sd):\n",
    "            return 0.0\n",
    "        return (returns.mean() - rf) / sd * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "\n",
    "    def sortino(returns, rf=0.0):\n",
    "        downside = returns[returns < 0]\n",
    "        sd = downside.std()\n",
    "        if sd == 0 or np.isnan(sd):\n",
    "            return 0.0\n",
    "        return (returns.mean() - rf) / sd * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "\n",
    "    start_val = float(df_perf[\"portfolio_value\"].iloc[0])\n",
    "    end_val   = float(df_perf[\"portfolio_value\"].iloc[-1])\n",
    "    total_ret = end_val / start_val - 1.0\n",
    "    n_years   = (df_perf[\"date\"].iloc[-1] - df_perf[\"date\"].iloc[0]).days / 365.25\n",
    "\n",
    "    strat_cagr    = float(cagr(total_ret, n_years))\n",
    "    strat_vol     = float(df_perf[\"strat_ret\"].std() * np.sqrt(TRADING_DAYS_PER_YEAR))\n",
    "    strat_sharpe  = float(sharpe(df_perf[\"strat_ret\"]))\n",
    "    strat_sortino = float(sortino(df_perf[\"strat_ret\"]))\n",
    "    strat_maxdd   = float(max_drawdown(df_perf[\"portfolio_value\"]))\n",
    "    strat_calmar  = float(strat_cagr / abs(strat_maxdd)) if strat_maxdd != 0 else np.nan\n",
    "\n",
    "    spy_total_ret = float(df_perf[\"spy_close\"].iloc[-1] / df_perf[\"spy_close\"].iloc[0] - 1.0)\n",
    "    spy_cagr      = float(cagr(spy_total_ret, n_years))\n",
    "    spy_vol       = float(df_perf[\"spy_ret\"].std() * np.sqrt(TRADING_DAYS_PER_YEAR))\n",
    "    spy_sharpe    = float(sharpe(df_perf[\"spy_ret\"]))\n",
    "    spy_sortino   = float(sortino(df_perf[\"spy_ret\"]))\n",
    "    spy_maxdd     = float(max_drawdown(df_perf[\"spy_close\"]))\n",
    "    spy_calmar    = float(spy_cagr / abs(spy_maxdd)) if spy_maxdd != 0 else np.nan\n",
    "\n",
    "    metrics = {\n",
    "        \"rows_perf\": int(len(df_perf)),\n",
    "        \"start_date\": str(df_perf[\"date\"].iloc[0].date()),\n",
    "        \"end_date\": str(df_perf[\"date\"].iloc[-1].date()),\n",
    "        \"start_value\": start_val,\n",
    "        \"end_value\": end_val,\n",
    "\n",
    "        \"strat_cagr\": strat_cagr,\n",
    "        \"strat_vol\": strat_vol,\n",
    "        \"strat_sharpe\": strat_sharpe,\n",
    "        \"strat_sortino\": strat_sortino,\n",
    "        \"strat_maxdd\": strat_maxdd,\n",
    "        \"strat_calmar\": strat_calmar,\n",
    "\n",
    "        \"spy_cagr\": spy_cagr,\n",
    "        \"spy_vol\": spy_vol,\n",
    "        \"spy_sharpe\": spy_sharpe,\n",
    "        \"spy_sortino\": spy_sortino,\n",
    "        \"spy_maxdd\": spy_maxdd,\n",
    "        \"spy_calmar\": spy_calmar,\n",
    "    }\n",
    "\n",
    "    daily_df = df_perf[[\n",
    "        \"date\",\n",
    "        \"portfolio_value\",\n",
    "        \"strat_ret\",\n",
    "        \"spy_close\",\n",
    "        \"spy_ret\",\n",
    "        \"cash\",\n",
    "        \"num_positions\",\n",
    "    ]].copy()\n",
    "\n",
    "    return metrics, daily_df\n",
    "\n",
    "def run_backtest_with_daily(\n",
    "    ctx: PreparedContext,\n",
    "    spy_df: pd.DataFrame,\n",
    "    spy_regime_map: dict,\n",
    "    top_percentile: float,\n",
    "    min_trade_value: float,\n",
    "    max_position_weight: float,\n",
    "    min_cash_reserve: float,\n",
    "    drift_threshold: float,\n",
    "    min_new_position_weight: float,\n",
    "):\n",
    "    cash = float(INITIAL_CAPITAL)\n",
    "    positions = {}  # ticker -> {\"shares\": int, \"entry\": float}\n",
    "    equity_curve = []\n",
    "    pending_orders = {}\n",
    "\n",
    "    exec_diag = {\n",
    "        \"orders_seen\": 0,\n",
    "        \"orders_executed\": 0,\n",
    "        \"dropped_missing_open\": 0,\n",
    "        \"dropped_cash_floor_buy_plan\": 0,\n",
    "        \"dropped_cash_floor_buy_exec\": 0,\n",
    "        \"clipped_cash_floor_buy_exec\": 0,\n",
    "    }\n",
    "\n",
    "    total_trades = 0\n",
    "\n",
    "    for date in ctx.dates:\n",
    "        if date < START_TRADING:\n",
    "            continue\n",
    "\n",
    "        day = ctx.df_by_date.get(date)\n",
    "        if day is None or day.empty:\n",
    "            continue\n",
    "\n",
    "        # -------------------------\n",
    "        # 0) EXECUTE pending orders\n",
    "        # -------------------------\n",
    "        if date in pending_orders:\n",
    "            payload = pending_orders.pop(date)\n",
    "            open_px_map = payload[\"open_px_map\"]\n",
    "            planned_trades = payload[\"planned_trades\"]\n",
    "\n",
    "            planned_trades = sorted(planned_trades, key=lambda x: (0 if x[\"side\"] == \"SELL\" else 1, x[\"rank\"]))\n",
    "\n",
    "            for tr in planned_trades:\n",
    "                t = tr[\"ticker\"]\n",
    "                side = tr[\"side\"]\n",
    "                sh_plan = int(tr[\"shares\"])\n",
    "                if not t or sh_plan <= 0:\n",
    "                    continue\n",
    "\n",
    "                px = open_px_map.get(t, np.nan)\n",
    "                if pd.isna(px) or px <= 0:\n",
    "                    arr = ctx.px_by_ticker.get(t)\n",
    "                    if arr is not None:\n",
    "                        px = fast_price_lookup(arr, date)\n",
    "                if pd.isna(px) or px <= 0:\n",
    "                    exec_diag[\"dropped_missing_open\"] += 1\n",
    "                    continue\n",
    "\n",
    "                px = float(px)\n",
    "\n",
    "                if side == \"SELL\":\n",
    "                    cur = int(positions.get(t, {}).get(\"shares\", 0))\n",
    "                    sh_exec = min(sh_plan, cur)\n",
    "                    if sh_exec <= 0:\n",
    "                        continue\n",
    "                    cash += sh_exec * px\n",
    "                    new_sh = cur - sh_exec\n",
    "                    if new_sh <= 0:\n",
    "                        positions.pop(t, None)\n",
    "                    else:\n",
    "                        positions[t][\"shares\"] = new_sh\n",
    "                    total_trades += 1\n",
    "\n",
    "                else:  # BUY\n",
    "                    available = cash - min_cash_reserve\n",
    "                    if available <= 0:\n",
    "                        exec_diag[\"dropped_cash_floor_buy_exec\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    max_affordable = int(np.floor(available / px))\n",
    "                    sh_exec = min(sh_plan, max_affordable)\n",
    "                    sh_exec = int(sh_exec)\n",
    "                    if sh_exec <= 0:\n",
    "                        exec_diag[\"dropped_cash_floor_buy_exec\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    if sh_exec < sh_plan:\n",
    "                        exec_diag[\"clipped_cash_floor_buy_exec\"] += 1\n",
    "\n",
    "                    cash -= sh_exec * px\n",
    "                    if t in positions:\n",
    "                        positions[t][\"shares\"] = int(positions[t][\"shares\"]) + sh_exec\n",
    "                    else:\n",
    "                        positions[t] = {\"shares\": sh_exec, \"entry\": px}\n",
    "                    total_trades += 1\n",
    "\n",
    "            exec_diag[\"orders_executed\"] += len(planned_trades)\n",
    "\n",
    "        # -------------------------\n",
    "        # 1) PLAN on Wednesday close\n",
    "        # -------------------------\n",
    "        if is_rebalance_day(date):\n",
    "            trade_date = ctx.next_date_map.get(date)\n",
    "            if trade_date is not None and trade_date in ctx.df_by_date:\n",
    "                spy_above_200 = bool(spy_regime_map.get(date, True))\n",
    "                can_buy_next_open = spy_above_200\n",
    "\n",
    "                in_sp = day[\"in_sp500\"] if \"in_sp500\" in day.columns else True\n",
    "                rankable = day[(day[\"slope_adj\"].notna()) & (in_sp == True)].copy()\n",
    "\n",
    "                if not rankable.empty:\n",
    "                    rankable = rankable.sort_values(\"slope_adj\", ascending=False)\n",
    "                    cutoff = rankable[\"slope_adj\"].quantile(top_percentile)\n",
    "                    top_group = rankable[rankable[\"slope_adj\"] >= cutoff].copy()\n",
    "\n",
    "                    if not top_group.empty:\n",
    "                        top_group = top_group.sort_values(\"slope_adj\", ascending=False)\n",
    "                        top_group[\"slope_rank_within_top\"] = np.arange(1, len(top_group) + 1)\n",
    "                        rank_map = dict(zip(top_group[\"ticker\"], top_group[\"slope_rank_within_top\"]))\n",
    "                        top_tickers = set(top_group[\"ticker\"].values)\n",
    "\n",
    "                        exec_px_map = day.set_index(\"ticker\")[\"close_adj\"].to_dict()\n",
    "                        trade_day = ctx.df_by_date[trade_date]\n",
    "                        open_px_map = trade_day.set_index(\"ticker\")[\"open_adj\"].to_dict()\n",
    "\n",
    "                        cash_plan = float(cash)\n",
    "                        pos_plan = {t: int(p[\"shares\"]) for t, p in positions.items()}\n",
    "                        planned = []\n",
    "\n",
    "                        def px_est(ticker: str) -> float:\n",
    "                            p = exec_px_map.get(ticker, np.nan)\n",
    "                            if pd.isna(p) or p <= 0:\n",
    "                                arr = ctx.px_by_ticker.get(ticker)\n",
    "                                if arr is not None:\n",
    "                                    p = fast_price_lookup(arr, date)\n",
    "                            return float(p) if (pd.notna(p) and p > 0) else np.nan\n",
    "\n",
    "                        # (A) exit sells\n",
    "                        exit_tickers = [t for t in list(pos_plan.keys()) if t not in top_tickers]\n",
    "                        for t in exit_tickers:\n",
    "                            sh0 = int(pos_plan.get(t, 0))\n",
    "                            if sh0 <= 0:\n",
    "                                continue\n",
    "                            p = px_est(t)\n",
    "                            if pd.isna(p):\n",
    "                                continue\n",
    "                            cash_plan += sh0 * p\n",
    "                            pos_plan.pop(t, None)\n",
    "                            planned.append({\"ticker\": t, \"side\": \"SELL\", \"shares\": sh0, \"rank\": int(rank_map.get(t, 9999))})\n",
    "\n",
    "                        # (B) revalue (proxy)\n",
    "                        pos_plan_struct = {t: {\"shares\": sh} for t, sh in pos_plan.items()}\n",
    "                        _, portfolio_exec, _ = snapshot_portfolio_exec_proxy(\n",
    "                            date, cash_plan, pos_plan_struct, exec_px_map, ctx.px_by_ticker\n",
    "                        )\n",
    "                        effective_equity = max(portfolio_exec - min_cash_reserve, 0.0)\n",
    "\n",
    "                        # (C) targets\n",
    "                        tg = top_group.copy()\n",
    "                        tg = tg[\n",
    "                            tg[\"atr\"].notna() & (tg[\"atr\"] > 0) &\n",
    "                            tg[\"close_adj\"].notna() & (tg[\"close_adj\"] > 0)\n",
    "                        ].copy()\n",
    "\n",
    "                        if not tg.empty:\n",
    "                            inv_vol = 1.0 / tg[\"atr\"].astype(float)\n",
    "                            total_inv_vol = inv_vol.sum()\n",
    "\n",
    "                            if total_inv_vol > 0:\n",
    "                                tg[\"raw_weight\"] = inv_vol / total_inv_vol\n",
    "                                tg[\"weight\"] = cap_and_redistribute_weights(tg[\"raw_weight\"].to_numpy(), max_position_weight)\n",
    "\n",
    "                                tg[\"target_value\"] = effective_equity * tg[\"weight\"]\n",
    "                                tg[\"exec_px_est\"] = tg[\"ticker\"].map(exec_px_map)\n",
    "                                tg = tg[tg[\"exec_px_est\"].notna() & (tg[\"exec_px_est\"] > 0)].copy()\n",
    "\n",
    "                                tg[\"target_value\"] = np.minimum(tg[\"target_value\"], max_position_weight * portfolio_exec)\n",
    "                                tg[\"target_shares\"] = np.floor(tg[\"target_value\"] / tg[\"exec_px_est\"]).astype(int)\n",
    "                                tg = tg[tg[\"target_shares\"] > 0].copy()\n",
    "                                tg = tg.sort_values(\"slope_adj\", ascending=False)\n",
    "\n",
    "                                total_portfolio_value_exec = portfolio_exec\n",
    "\n",
    "                                for _, r in tg.iterrows():\n",
    "                                    t = str(r[\"ticker\"])\n",
    "                                    rank = int(rank_map.get(t, 9999))\n",
    "                                    p = float(r[\"exec_px_est\"])\n",
    "                                    if not (p > 0):\n",
    "                                        continue\n",
    "\n",
    "                                    target_sh = int(r[\"target_shares\"])\n",
    "                                    cur_sh = int(pos_plan.get(t, 0))\n",
    "\n",
    "                                    max_sh_allowed = (\n",
    "                                        int(np.floor((max_position_weight * total_portfolio_value_exec) / p))\n",
    "                                        if total_portfolio_value_exec > 0 else 0\n",
    "                                    )\n",
    "                                    target_sh = min(target_sh, max_sh_allowed)\n",
    "\n",
    "                                    target_val = target_sh * p\n",
    "                                    target_w = (target_val / total_portfolio_value_exec) if total_portfolio_value_exec > 0 else 0.0\n",
    "                                    cur_val = cur_sh * p\n",
    "                                    cur_w = (cur_val / total_portfolio_value_exec) if total_portfolio_value_exec > 0 else 0.0\n",
    "\n",
    "                                    weight_diff = abs(target_w - cur_w)\n",
    "                                    is_new = (cur_sh == 0)\n",
    "                                    cap_breach = (cur_w > max_position_weight + 1e-9)\n",
    "\n",
    "                                    if (weight_diff < drift_threshold) and (not cap_breach):\n",
    "                                        continue\n",
    "\n",
    "                                    # SELL\n",
    "                                    if target_sh < cur_sh:\n",
    "                                        trade_sh = cur_sh - target_sh\n",
    "                                        est_value = trade_sh * p\n",
    "                                        if est_value < min_trade_value:\n",
    "                                            continue\n",
    "\n",
    "                                        cash_plan += est_value\n",
    "                                        new_sh = cur_sh - trade_sh\n",
    "                                        if new_sh <= 0:\n",
    "                                            pos_plan.pop(t, None)\n",
    "                                        else:\n",
    "                                            pos_plan[t] = new_sh\n",
    "\n",
    "                                        planned.append({\"ticker\": t, \"side\": \"SELL\", \"shares\": int(trade_sh), \"rank\": rank})\n",
    "\n",
    "                                    # BUY\n",
    "                                    elif target_sh > cur_sh:\n",
    "                                        if not can_buy_next_open:\n",
    "                                            continue\n",
    "\n",
    "                                        trade_sh = target_sh - cur_sh\n",
    "                                        est_value = trade_sh * p\n",
    "\n",
    "                                        if is_new and target_w < min_new_position_weight:\n",
    "                                            continue\n",
    "                                        if est_value < min_trade_value:\n",
    "                                            continue\n",
    "                                        if est_value > (cash_plan - min_cash_reserve):\n",
    "                                            exec_diag[\"dropped_cash_floor_buy_plan\"] += 1\n",
    "                                            continue\n",
    "\n",
    "                                        cash_plan -= est_value\n",
    "                                        pos_plan[t] = cur_sh + trade_sh\n",
    "                                        planned.append({\"ticker\": t, \"side\": \"BUY\", \"shares\": int(trade_sh), \"rank\": rank})\n",
    "\n",
    "                        pending_orders[trade_date] = {\"open_px_map\": open_px_map, \"planned_trades\": planned}\n",
    "                        exec_diag[\"orders_seen\"] += len(planned)\n",
    "\n",
    "        # -------------------------\n",
    "        # 2) daily mark-to-market\n",
    "        # -------------------------\n",
    "        _, pv, npos = snapshot_portfolio_close(date, cash, positions, ctx.px_by_ticker)\n",
    "        equity_curve.append({\n",
    "            \"date\": date,\n",
    "            \"portfolio_value\": float(pv),\n",
    "            \"cash\": float(cash),\n",
    "            \"num_positions\": int(npos),\n",
    "        })\n",
    "\n",
    "    equity_df = pd.DataFrame(equity_curve)\n",
    "    metrics, daily_df = compute_perf_and_daily(equity_df, spy_df)\n",
    "\n",
    "    if metrics is None:\n",
    "        metrics = {\"rows_perf\": 0}\n",
    "    metrics[\"total_trades\"] = int(total_trades)\n",
    "    for k, v in exec_diag.items():\n",
    "        metrics[f\"diag_{k}\"] = int(v)\n",
    "\n",
    "    return metrics, daily_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1e44843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# CELL 4/5 — LOADERS + WORKER (UPDATED: loops across new grids + writes config columns)\n",
    "# ============================\n",
    "def load_universe_df(lookback: int) -> pd.DataFrame:\n",
    "    f = resolve_universe_file(lookback)\n",
    "    df = pd.read_parquet(f)\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    keep = [c for c in [\"date\", \"ticker\", \"open_adj\", \"close_adj\", \"slope_adj\", \"in_sp500\"] if c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "\n",
    "    df[\"slope_adj\"] = pd.to_numeric(df[\"slope_adj\"], errors=\"coerce\")\n",
    "    df[\"close_adj\"] = pd.to_numeric(df[\"close_adj\"], errors=\"coerce\")\n",
    "    df[\"open_adj\"]  = pd.to_numeric(df[\"open_adj\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def load_atr_all(atr_w: int) -> pd.DataFrame:\n",
    "    atr_dir = resolve_atr_subdir(atr_w)\n",
    "    files = [f for f in os.listdir(atr_dir) if f.endswith(\".parquet\")]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No ATR parquet files found in {atr_dir}\")\n",
    "\n",
    "    atr_col = None\n",
    "    for fn in files[:50]:\n",
    "        tmp = pd.read_parquet(os.path.join(atr_dir, fn))\n",
    "        atr_col = infer_atr_col(tmp.columns, atr_w)\n",
    "        if atr_col is not None and \"date\" in tmp.columns:\n",
    "            break\n",
    "    if atr_col is None:\n",
    "        raise ValueError(f\"Could not infer ATR column for window={atr_w} in {atr_dir}\")\n",
    "\n",
    "    rows = []\n",
    "    for fn in files:\n",
    "        t = fn.replace(\".parquet\", \"\")\n",
    "        tmp = pd.read_parquet(os.path.join(atr_dir, fn), columns=[\"date\", atr_col])\n",
    "        tmp[\"date\"] = pd.to_datetime(tmp[\"date\"])\n",
    "        tmp[\"ticker\"] = t\n",
    "        tmp = tmp.rename(columns={atr_col: \"atr\"})\n",
    "        rows.append(tmp)\n",
    "\n",
    "    atr_all = pd.concat(rows, ignore_index=True)\n",
    "    atr_all[\"atr\"] = pd.to_numeric(atr_all[\"atr\"], errors=\"coerce\")\n",
    "    return atr_all\n",
    "\n",
    "def _write_daily_returns(daily_df: pd.DataFrame, daily_dir: str, cfg: dict):\n",
    "    \"\"\"\n",
    "    Writes per-config daily returns file.\n",
    "    \"\"\"\n",
    "    if daily_df is None or daily_df.empty:\n",
    "        return None\n",
    "\n",
    "    cfg_id = make_config_id(\n",
    "        cfg[\"lookback_days\"], cfg[\"atr_days\"], cfg[\"top_percentile\"], cfg[\"min_trade_value\"],\n",
    "        cfg[\"max_position_weight\"], cfg[\"min_cash_reserve\"], cfg[\"drift_threshold\"], cfg[\"min_new_position_weight\"],\n",
    "    )\n",
    "\n",
    "    out = daily_df.copy()\n",
    "\n",
    "    # Attach config columns to each row (useful for later concatenation / WFO)\n",
    "    out[\"lookback_days\"]          = int(cfg[\"lookback_days\"])\n",
    "    out[\"atr_days\"]               = int(cfg[\"atr_days\"])\n",
    "    out[\"top_percentile\"]         = float(cfg[\"top_percentile\"])\n",
    "    out[\"min_trade_value\"]        = float(cfg[\"min_trade_value\"])\n",
    "    out[\"max_position_weight\"]    = float(cfg[\"max_position_weight\"])\n",
    "    out[\"min_cash_reserve\"]       = float(cfg[\"min_cash_reserve\"])\n",
    "    out[\"drift_threshold\"]        = float(cfg[\"drift_threshold\"])\n",
    "    out[\"min_new_position_weight\"]= float(cfg[\"min_new_position_weight\"])\n",
    "\n",
    "    os.makedirs(daily_dir, exist_ok=True)\n",
    "    if DAILY_RETURNS_FMT.lower() == \"csv\":\n",
    "        path = os.path.join(daily_dir, f\"{cfg_id}.csv\")\n",
    "        out.to_csv(path, index=False)\n",
    "        return path\n",
    "    else:\n",
    "        path = os.path.join(daily_dir, f\"{cfg_id}.parquet\")\n",
    "        out.to_parquet(path, index=False)\n",
    "        return path\n",
    "\n",
    "def run_job(lookback: int, atr_w: int, daily_dir: str | None = None) -> list[dict]:\n",
    "    # load spy inside worker (avoids pickling + notebook weirdness)\n",
    "    spy_df, spy_regime_map = load_spy_regime_map(SPY_FILE)\n",
    "\n",
    "    df_univ = load_universe_df(lookback)\n",
    "    atr_all = load_atr_all(atr_w)\n",
    "\n",
    "    # merge ATR once\n",
    "    df = df_univ.merge(atr_all, on=[\"date\", \"ticker\"], how=\"left\")\n",
    "\n",
    "    # prepare context once\n",
    "    ctx = prepare_context(df)\n",
    "\n",
    "    out_rows = []\n",
    "    for top_p in TOP_PERCENTILES:\n",
    "        for mtv in MIN_TRADE_VALUES:\n",
    "            for mpw in MAX_POSITION_WEIGHTS:\n",
    "                for mcr in MIN_CASH_RESERVES:\n",
    "                    for drift in DRIFT_THRESHOLDS:\n",
    "                        for mnw in MIN_NEW_POSITION_WEIGHTS:\n",
    "                            metrics, daily_df = run_backtest_with_daily(\n",
    "                                ctx=ctx,\n",
    "                                spy_df=spy_df,\n",
    "                                spy_regime_map=spy_regime_map,\n",
    "                                top_percentile=float(top_p),\n",
    "                                min_trade_value=float(mtv),\n",
    "                                max_position_weight=float(mpw),\n",
    "                                min_cash_reserve=float(mcr),\n",
    "                                drift_threshold=float(drift),\n",
    "                                min_new_position_weight=float(mnw),\n",
    "                            )\n",
    "\n",
    "                            row = {\n",
    "                                \"lookback_days\": int(lookback),\n",
    "                                \"atr_days\": int(atr_w),\n",
    "                                \"top_percentile\": float(top_p),\n",
    "                                \"min_trade_value\": float(mtv),\n",
    "\n",
    "                                \"max_position_weight\": float(mpw),\n",
    "                                \"min_cash_reserve\": float(mcr),\n",
    "                                \"drift_threshold\": float(drift),\n",
    "                                \"min_new_position_weight\": float(mnw),\n",
    "\n",
    "                                **metrics,\n",
    "                            }\n",
    "\n",
    "                            if WRITE_DAILY_RETURNS and daily_dir is not None:\n",
    "                                _ = _write_daily_returns(daily_df=daily_df, daily_dir=daily_dir, cfg=row)\n",
    "\n",
    "                            out_rows.append(row)\n",
    "\n",
    "    return out_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f798f8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs (lookback x ATR): 1\n",
      "Runs per job: 48\n",
      "Total runs: 48\n",
      "Daily returns dir: ./13a-trading_output_sweep_performance_daily_returns\\daily_returns\\20251231-100856\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c422cbb3b6e04e18839d5823bd185cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Jobs completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. Rows: 48 (expected 48)\n",
      "Elapsed: 387.0s\n",
      "Saved: ./13a-trading_output_sweep_performance_daily_returns\\sweep_summary_20251231-100856.csv\n",
      "Saved: ./13a-trading_output_sweep_performance_daily_returns\\sweep_summary_20251231-100856.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lookback_days</th>\n",
       "      <th>atr_days</th>\n",
       "      <th>top_percentile</th>\n",
       "      <th>min_trade_value</th>\n",
       "      <th>max_position_weight</th>\n",
       "      <th>min_cash_reserve</th>\n",
       "      <th>drift_threshold</th>\n",
       "      <th>min_new_position_weight</th>\n",
       "      <th>rows_perf</th>\n",
       "      <th>start_date</th>\n",
       "      <th>...</th>\n",
       "      <th>spy_sortino</th>\n",
       "      <th>spy_maxdd</th>\n",
       "      <th>spy_calmar</th>\n",
       "      <th>total_trades</th>\n",
       "      <th>diag_orders_seen</th>\n",
       "      <th>diag_orders_executed</th>\n",
       "      <th>diag_dropped_missing_open</th>\n",
       "      <th>diag_dropped_cash_floor_buy_plan</th>\n",
       "      <th>diag_dropped_cash_floor_buy_exec</th>\n",
       "      <th>diag_clipped_cash_floor_buy_exec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6790</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663035</td>\n",
       "      <td>-0.551894</td>\n",
       "      <td>0.15337</td>\n",
       "      <td>7439</td>\n",
       "      <td>7439</td>\n",
       "      <td>7439</td>\n",
       "      <td>0</td>\n",
       "      <td>287</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.010</td>\n",
       "      <td>6790</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663035</td>\n",
       "      <td>-0.551894</td>\n",
       "      <td>0.15337</td>\n",
       "      <td>7439</td>\n",
       "      <td>7439</td>\n",
       "      <td>7439</td>\n",
       "      <td>0</td>\n",
       "      <td>287</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.020</td>\n",
       "      <td>6790</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663035</td>\n",
       "      <td>-0.551894</td>\n",
       "      <td>0.15337</td>\n",
       "      <td>7439</td>\n",
       "      <td>7439</td>\n",
       "      <td>7439</td>\n",
       "      <td>0</td>\n",
       "      <td>287</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6790</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663035</td>\n",
       "      <td>-0.551894</td>\n",
       "      <td>0.15337</td>\n",
       "      <td>7430</td>\n",
       "      <td>7430</td>\n",
       "      <td>7430</td>\n",
       "      <td>0</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.010</td>\n",
       "      <td>6790</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663035</td>\n",
       "      <td>-0.551894</td>\n",
       "      <td>0.15337</td>\n",
       "      <td>7430</td>\n",
       "      <td>7430</td>\n",
       "      <td>7430</td>\n",
       "      <td>0</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.020</td>\n",
       "      <td>6790</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663035</td>\n",
       "      <td>-0.551894</td>\n",
       "      <td>0.15337</td>\n",
       "      <td>7430</td>\n",
       "      <td>7430</td>\n",
       "      <td>7430</td>\n",
       "      <td>0</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6790</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663035</td>\n",
       "      <td>-0.551894</td>\n",
       "      <td>0.15337</td>\n",
       "      <td>7510</td>\n",
       "      <td>7510</td>\n",
       "      <td>7510</td>\n",
       "      <td>0</td>\n",
       "      <td>297</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.010</td>\n",
       "      <td>6790</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663035</td>\n",
       "      <td>-0.551894</td>\n",
       "      <td>0.15337</td>\n",
       "      <td>7510</td>\n",
       "      <td>7510</td>\n",
       "      <td>7510</td>\n",
       "      <td>0</td>\n",
       "      <td>297</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.020</td>\n",
       "      <td>6790</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663035</td>\n",
       "      <td>-0.551894</td>\n",
       "      <td>0.15337</td>\n",
       "      <td>7510</td>\n",
       "      <td>7510</td>\n",
       "      <td>7510</td>\n",
       "      <td>0</td>\n",
       "      <td>297</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6790</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663035</td>\n",
       "      <td>-0.551894</td>\n",
       "      <td>0.15337</td>\n",
       "      <td>7495</td>\n",
       "      <td>7495</td>\n",
       "      <td>7495</td>\n",
       "      <td>0</td>\n",
       "      <td>291</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    lookback_days  atr_days  top_percentile  min_trade_value  \\\n",
       "32             90        20            0.95          10000.0   \n",
       "33             90        20            0.95          10000.0   \n",
       "34             90        20            0.95          10000.0   \n",
       "44             90        20            0.95          10000.0   \n",
       "45             90        20            0.95          10000.0   \n",
       "46             90        20            0.95          10000.0   \n",
       "8              90        20            0.95           5000.0   \n",
       "9              90        20            0.95           5000.0   \n",
       "10             90        20            0.95           5000.0   \n",
       "20             90        20            0.95           5000.0   \n",
       "\n",
       "    max_position_weight  min_cash_reserve  drift_threshold  \\\n",
       "32                 0.12            5000.0             0.02   \n",
       "33                 0.12            5000.0             0.02   \n",
       "34                 0.12            5000.0             0.02   \n",
       "44                 0.12           10000.0             0.02   \n",
       "45                 0.12           10000.0             0.02   \n",
       "46                 0.12           10000.0             0.02   \n",
       "8                  0.12            5000.0             0.02   \n",
       "9                  0.12            5000.0             0.02   \n",
       "10                 0.12            5000.0             0.02   \n",
       "20                 0.12           10000.0             0.02   \n",
       "\n",
       "    min_new_position_weight  rows_perf  start_date  ... spy_sortino  \\\n",
       "32                    0.005       6790  1999-01-04  ...    0.663035   \n",
       "33                    0.010       6790  1999-01-04  ...    0.663035   \n",
       "34                    0.020       6790  1999-01-04  ...    0.663035   \n",
       "44                    0.005       6790  1999-01-04  ...    0.663035   \n",
       "45                    0.010       6790  1999-01-04  ...    0.663035   \n",
       "46                    0.020       6790  1999-01-04  ...    0.663035   \n",
       "8                     0.005       6790  1999-01-04  ...    0.663035   \n",
       "9                     0.010       6790  1999-01-04  ...    0.663035   \n",
       "10                    0.020       6790  1999-01-04  ...    0.663035   \n",
       "20                    0.005       6790  1999-01-04  ...    0.663035   \n",
       "\n",
       "    spy_maxdd  spy_calmar  total_trades  diag_orders_seen  \\\n",
       "32  -0.551894     0.15337          7439              7439   \n",
       "33  -0.551894     0.15337          7439              7439   \n",
       "34  -0.551894     0.15337          7439              7439   \n",
       "44  -0.551894     0.15337          7430              7430   \n",
       "45  -0.551894     0.15337          7430              7430   \n",
       "46  -0.551894     0.15337          7430              7430   \n",
       "8   -0.551894     0.15337          7510              7510   \n",
       "9   -0.551894     0.15337          7510              7510   \n",
       "10  -0.551894     0.15337          7510              7510   \n",
       "20  -0.551894     0.15337          7495              7495   \n",
       "\n",
       "    diag_orders_executed  diag_dropped_missing_open  \\\n",
       "32                  7439                          0   \n",
       "33                  7439                          0   \n",
       "34                  7439                          0   \n",
       "44                  7430                          0   \n",
       "45                  7430                          0   \n",
       "46                  7430                          0   \n",
       "8                   7510                          0   \n",
       "9                   7510                          0   \n",
       "10                  7510                          0   \n",
       "20                  7495                          0   \n",
       "\n",
       "    diag_dropped_cash_floor_buy_plan  diag_dropped_cash_floor_buy_exec  \\\n",
       "32                               287                                 0   \n",
       "33                               287                                 0   \n",
       "34                               287                                 0   \n",
       "44                               288                                 0   \n",
       "45                               288                                 0   \n",
       "46                               288                                 0   \n",
       "8                                297                                 0   \n",
       "9                                297                                 0   \n",
       "10                               297                                 0   \n",
       "20                               291                                 0   \n",
       "\n",
       "    diag_clipped_cash_floor_buy_exec  \n",
       "32                                 1  \n",
       "33                                 1  \n",
       "34                                 1  \n",
       "44                                 1  \n",
       "45                                 1  \n",
       "46                                 1  \n",
       "8                                  2  \n",
       "9                                  2  \n",
       "10                                 2  \n",
       "20                                 1  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# CELL 5/5 — RUN SWEEP + SAVE SUMMARY (+ per-config daily returns)\n",
    "# ============================\n",
    "jobs = [(lb, aw) for lb in REG_LOOKBACKS for aw in ATR_WINDOWS]\n",
    "runs_per_job = (\n",
    "    len(TOP_PERCENTILES) * len(MIN_TRADE_VALUES) *\n",
    "    len(MAX_POSITION_WEIGHTS) * len(MIN_CASH_RESERVES) *\n",
    "    len(DRIFT_THRESHOLDS) * len(MIN_NEW_POSITION_WEIGHTS)\n",
    ")\n",
    "total_runs = len(jobs) * runs_per_job\n",
    "\n",
    "print(\"Jobs (lookback x ATR):\", len(jobs))\n",
    "print(\"Runs per job:\", runs_per_job)\n",
    "print(\"Total runs:\", total_runs)\n",
    "\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "DAILY_DIR = None\n",
    "if WRITE_DAILY_RETURNS:\n",
    "    DAILY_DIR = os.path.join(OUT_DIR, DAILY_RETURNS_DIRNAME, RUN_ID)\n",
    "    os.makedirs(DAILY_DIR, exist_ok=True)\n",
    "    print(\"Daily returns dir:\", DAILY_DIR)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "with tqdm_joblib(tqdm(total=len(jobs), desc=\"Jobs completed\")):\n",
    "    results = Parallel(n_jobs=MAX_WORKERS, backend=\"loky\")(\n",
    "        delayed(run_job)(lb, aw, daily_dir=DAILY_DIR) for (lb, aw) in jobs\n",
    "    )\n",
    "\n",
    "summary_rows = [r for job_rows in results for r in job_rows]\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nDone. Rows: {len(summary_df):,} (expected {total_runs})\")\n",
    "print(f\"Elapsed: {elapsed:.1f}s\")\n",
    "\n",
    "# Save summary\n",
    "csv_path = os.path.join(OUT_DIR, f\"sweep_summary_{RUN_ID}.csv\")\n",
    "pq_path  = os.path.join(OUT_DIR, f\"sweep_summary_{RUN_ID}.parquet\")\n",
    "\n",
    "summary_df.to_csv(csv_path, index=False)\n",
    "summary_df.to_parquet(pq_path, index=False)\n",
    "\n",
    "print(\"Saved:\", csv_path)\n",
    "print(\"Saved:\", pq_path)\n",
    "\n",
    "# quick peek\n",
    "summary_df.sort_values([\"strat_cagr\", \"strat_sharpe\"], ascending=False).head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
