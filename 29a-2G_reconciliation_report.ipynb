{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4744bff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farty\\AppData\\Local\\Temp\\ipykernel_14660\\470971253.py:1203: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  slip_by_day = weekly_grp.apply(_slip_apply).reset_index().rename(columns={\"exec_date_only\": \"exec_date\"})\n",
      "C:\\Users\\farty\\AppData\\Local\\Temp\\ipykernel_14660\\470971253.py:215: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat([out, pd.DataFrame([totals])], ignore_index=True)\n",
      "C:\\Users\\farty\\AppData\\Local\\Temp\\ipykernel_14660\\470971253.py:215: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat([out, pd.DataFrame([totals])], ignore_index=True)\n",
      "C:\\Users\\farty\\AppData\\Local\\Temp\\ipykernel_14660\\470971253.py:215: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat([out, pd.DataFrame([totals])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPLETE ===\n",
      "Reconciliation report saved → ./29a-2G_reconciliation_reports\\reconciliation_report_20251230-121713.pdf\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "29 – Reconciliation Reporting (Audit + Ongoing Diagnostics)\n",
    "\n",
    "Reads reconciliation outputs and produces a PDF report:\n",
    "- executed_trades.csv (append-only ledger)\n",
    "- reconciliation_log.csv (diagnostic mirror; optional)\n",
    "- broker_fills_manual.csv (optional; pending fill detection)\n",
    "- live_portfolio.csv (state snapshot; drift check)\n",
    "\n",
    "Slippage Reporting (REPORT BOTH):\n",
    "- SigFill Slip: signal_price -> fill_price\n",
    "- OrdFill Slip: order_price  -> fill_price\n",
    "\n",
    "Assumptions for slippage dollars:\n",
    "  BUY  : (fill - ref) * shares\n",
    "  SELL : (ref - fill) * shares\n",
    "Positive = cost (worse), negative = improvement (better)\n",
    "\n",
    "Portfolio Reporting:\n",
    "- Average cost per share (moving average, handles multiple buys/sells)\n",
    "- Current price from your price database (parquet)\n",
    "- Date first acquired (for current open position)\n",
    "- Current return % vs avg cost\n",
    "- Weight % using current price\n",
    "\n",
    "NEW:\n",
    "- Front-page YTD performance table comparing Strategy vs SPY (like your screenshot).\n",
    "  SPY is sourced ONLY from 8-SPY_200DMA_regime.parquet (spy_close).\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# Imports\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from reportlab.lib.pagesizes import letter, landscape\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import (\n",
    "    SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle,\n",
    "    PageBreak, Image\n",
    ")\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "\n",
    "LIVE_ROOT = \"./27a-2G_live_trading\"\n",
    "\n",
    "LIVE_PORTFOLIO_FILE  = f\"{LIVE_ROOT}/live_portfolio.csv\"\n",
    "MANUAL_FILLS_FILE    = f\"{LIVE_ROOT}/broker_fills_manual.csv\"\n",
    "EXECUTED_TRADES_FILE = f\"{LIVE_ROOT}/executed_trades.csv\"\n",
    "RECON_LOG_FILE       = f\"{LIVE_ROOT}/reconciliation_log.csv\"\n",
    "\n",
    "# source for slope_rank\n",
    "MASTER_TRADES_FILE   = f\"{LIVE_ROOT}/master_trades.csv\"\n",
    "MASTER_RANKINGS_FILE = f\"{LIVE_ROOT}/master_rankings.csv\"\n",
    "\n",
    "OUTPUT_DIR = \"./29a-2G_reconciliation_reports\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "MIN_YEAR_FOR_REPORT = 1999\n",
    "MAX_YEAR_FOR_REPORT = 2025\n",
    "TRADING_DAYS_PER_YEAR = 252\n",
    "\n",
    "# ---- Price database (your maintained price history) ----\n",
    "PRICE_DB_FILE = \"./12-tradable_sp500_universe/12-tradable_sp500_universe.parquet\"\n",
    "PRICE_DATE_COL = \"date\"\n",
    "PRICE_TICKER_COL = \"ticker\"\n",
    "PRICE_PX_COL = \"close_adj\"\n",
    "\n",
    "# ---- SPY regime file (FOR SPY PERFORMANCE TABLE) ----\n",
    "SPY_REGIME_FILE_PRIMARY = r\"C:\\TWS API\\source\\pythonclient\\TradingIdeas\\MomentumSystem\\8-SPY_200DMA_market_regime\\8-SPY_200DMA_regime.parquet\"\n",
    "SPY_REGIME_FILE_FALLBACK = \"./8-SPY_200DMA_market_regime/8-SPY_200DMA_regime.parquet\"\n",
    "\n",
    "# ============================================================\n",
    "# Slippage column names (define EARLY so helpers can reference)\n",
    "# ============================================================\n",
    "\n",
    "SIG_SLIP_COL = \"slip_sigfill_dollars\"   # signal_price -> fill_price\n",
    "ORD_SLIP_COL = \"slip_ordfill_dollars\"   # order_price  -> fill_price\n",
    "\n",
    "SIG_LABEL = \"SigFill Slip (signal_price→fill_price)\"\n",
    "ORD_LABEL = \"OrdFill Slip (order_price→fill_price)\"\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "\n",
    "def _safe_read_csv(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except EmptyDataError:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _to_dt(s) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def _norm_side(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).str.upper().str.strip()\n",
    "\n",
    "def _norm_ticker(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).str.upper().str.strip()\n",
    "\n",
    "def _fmt(v, col=None):\n",
    "    if v is None:\n",
    "        return \"N/A\"\n",
    "    if isinstance(v, (float, np.floating)) and np.isnan(v):\n",
    "        return \"N/A\"\n",
    "    if isinstance(v, (pd.Timestamp, np.datetime64)):\n",
    "        return str(pd.Timestamp(v).date())\n",
    "    if isinstance(v, (int, np.integer)):\n",
    "        return str(int(v))\n",
    "    if isinstance(v, (float, np.floating)):\n",
    "        money_cols = {\n",
    "            \"fees\",\"broker_fee\",\"net_cash_impact\",\"gross_notional\",\"cash\",\n",
    "            \"sig_slip_net_dollars\",\"sig_slip_gross_cost_dollars\",\"sig_slip_gross_improve_dollars\",\n",
    "            \"ord_slip_net_dollars\",\"ord_slip_gross_cost_dollars\",\"ord_slip_gross_improve_dollars\",\n",
    "            \"ytd_sig_slip_net_dollars\",\"ytd_sig_slip_gross_cost_dollars\",\"ytd_sig_slip_gross_improve_dollars\",\n",
    "            \"ytd_ord_slip_net_dollars\",\"ytd_ord_slip_gross_cost_dollars\",\"ytd_ord_slip_gross_improve_dollars\",\n",
    "            \"sig_slip_per_trade_dollars\",\"ytd_sig_slip_per_trade_dollars\",\n",
    "            \"ord_slip_per_trade_dollars\",\"ytd_ord_slip_per_trade_dollars\",\n",
    "            SIG_SLIP_COL, ORD_SLIP_COL,\n",
    "            \"avg_cost\",\"current_price\",\"market_value\",\n",
    "                \"target_value\",           # ADD\n",
    "            \"current_value\",          # ADD\n",
    "        }\n",
    "        bps_cols = {\n",
    "            \"sig_slip_net_bps\",\"ord_slip_net_bps\",\n",
    "            \"ytd_sig_slip_net_bps\",\"ytd_ord_slip_net_bps\",\n",
    "        }\n",
    "        pct_cols = {\n",
    "            \"weight_pct\",\n",
    "            \"position_return_pct\",\n",
    "             \"target_weight\",          # ADD\n",
    "            \"current_weight\",         # ADD\n",
    "            \"weight_change\",          # ADD\n",
    "            \"capped_weight\",          # ADD\n",
    "            \"raw_weight\"              # ADD\n",
    "            }\n",
    "\n",
    "        if col in pct_cols:\n",
    "            return f\"{float(v):,.2f}%\"\n",
    "        if col in money_cols:\n",
    "            return f\"{float(v):,.2f}\"\n",
    "        if col in bps_cols:\n",
    "            return f\"{float(v):,.2f}\"\n",
    "        return f\"{float(v):0.4f}\"\n",
    "    return str(v)\n",
    "\n",
    "def _fmt_pct(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"N/A\"\n",
    "    return f\"{x*100.0:,.2f}%\"\n",
    "\n",
    "def _fmt_num(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"N/A\"\n",
    "    return f\"{x:,.2f}\"\n",
    "\n",
    "def make_table(story, title, df: pd.DataFrame, cols, font_size=8, bold_last_row=False):\n",
    "    story.append(Paragraph(title, styles[\"Small\"]))\n",
    "    if df is None or df.empty:\n",
    "        story.append(Paragraph(\"None\", styles[\"Tiny\"]))\n",
    "        story.append(Spacer(1, 0.10 * inch))\n",
    "        return\n",
    "\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    data = [cols]\n",
    "    for _, row in df[cols].iterrows():\n",
    "        data.append([_fmt(row[c], c) for c in cols])\n",
    "\n",
    "    tbl = Table(data, hAlign=\"LEFT\", repeatRows=1)\n",
    "\n",
    "    style_cmds = [\n",
    "        (\"BACKGROUND\",(0,0),(-1,0),colors.lightgrey),\n",
    "        (\"GRID\",(0,0),(-1,-1),0.25,colors.grey),\n",
    "        (\"FONTNAME\",(0,0),(-1,-1),\"Helvetica\"),\n",
    "        (\"FONTSIZE\",(0,0),(-1,-1),font_size),\n",
    "        (\"ALIGN\",(1,1),(-1,-1),\"RIGHT\"),\n",
    "    ]\n",
    "\n",
    "    if bold_last_row and len(data) > 1:\n",
    "        last = len(data) - 1\n",
    "        style_cmds += [\n",
    "            (\"LINEABOVE\",(0,last),(-1,last),0.75,colors.black),\n",
    "            (\"BACKGROUND\",(0,last),(-1,last),colors.whitesmoke),\n",
    "            (\"FONTNAME\",(0,last),(-1,last),\"Helvetica-Bold\"),\n",
    "        ]\n",
    "\n",
    "    tbl.setStyle(TableStyle(style_cmds))\n",
    "    story.append(tbl)\n",
    "    story.append(Spacer(1, 0.12 * inch))\n",
    "\n",
    "def append_totals_row(df: pd.DataFrame, sum_cols: list, label_col: str = \"ticker\", label: str = \"TOTAL\") -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    totals = {c: np.nan for c in out.columns}\n",
    "    if label_col in out.columns:\n",
    "        totals[label_col] = label\n",
    "    for c in sum_cols:\n",
    "        if c in out.columns:\n",
    "            totals[c] = pd.to_numeric(out[c], errors=\"coerce\").sum(skipna=True)\n",
    "    return pd.concat([out, pd.DataFrame([totals])], ignore_index=True)\n",
    "\n",
    "def add_sequential_display_cash(\n",
    "    df: pd.DataFrame,\n",
    "    start_cash: float,\n",
    "    *,\n",
    "    side_col: str = \"side\",\n",
    "    shares_col: str = \"shares\",\n",
    "    fill_col: str = \"fill_price\",\n",
    "    fee_col: str = \"broker_fee\",\n",
    ") -> tuple[pd.DataFrame, float]:\n",
    "    \"\"\"\n",
    "    Recomputes sequential cash_before/cash_after in *row order* of df.\n",
    "\n",
    "    Cash impact rule:\n",
    "      BUY  : cash -= shares*fill_price + fee\n",
    "      SELL : cash += shares*fill_price - fee\n",
    "\n",
    "    Returns (df_with_cols, ending_cash).\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df, float(start_cash)\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # normalize inputs\n",
    "    side = _norm_side(out[side_col])\n",
    "    shares = pd.to_numeric(out.get(shares_col, 0), errors=\"coerce\").fillna(0.0)\n",
    "    fill = pd.to_numeric(out.get(fill_col, np.nan), errors=\"coerce\")\n",
    "    fee = pd.to_numeric(out.get(fee_col, 0.0), errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    cash = float(start_cash) if pd.notna(start_cash) else 0.0\n",
    "    cb = []\n",
    "    ca = []\n",
    "    delta_list = []\n",
    "\n",
    "    for i in range(len(out)):\n",
    "        cb.append(cash)\n",
    "\n",
    "        sh = float(shares.iloc[i])\n",
    "        px = float(fill.iloc[i]) if pd.notna(fill.iloc[i]) else np.nan\n",
    "        f  = float(fee.iloc[i]) if pd.notna(fee.iloc[i]) else 0.0\n",
    "\n",
    "        # if we can't price the trade, don't move cash (but keep the sequence intact)\n",
    "        if sh <= 0 or pd.isna(px) or px <= 0:\n",
    "            delta = 0.0\n",
    "        else:\n",
    "            if side.iloc[i] == \"BUY\":\n",
    "                delta = -(sh * px) - f\n",
    "            elif side.iloc[i] == \"SELL\":\n",
    "                delta = (sh * px) - f\n",
    "            else:\n",
    "                delta = 0.0\n",
    "\n",
    "        cash = cash + delta\n",
    "\n",
    "        delta_list.append(delta)\n",
    "        ca.append(cash)\n",
    "\n",
    "    out[\"cash_before_display\"] = cb\n",
    "    out[\"cash_after_display\"] = ca\n",
    "    out[\"cash_delta_display\"] = delta_list\n",
    "\n",
    "    return out, float(cash)\n",
    "\n",
    "def reconstruct_positions_from_ledger(exec_df: pd.DataFrame) -> dict:\n",
    "    pos = {}\n",
    "    if exec_df is None or exec_df.empty:\n",
    "        return pos\n",
    "\n",
    "    df = exec_df.copy()\n",
    "    df[\"side\"] = _norm_side(df[\"side\"])\n",
    "    df[\"ticker\"] = _norm_ticker(df[\"ticker\"])\n",
    "    df[\"shares\"] = pd.to_numeric(df[\"shares\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    if \"exec_ts\" in df.columns:\n",
    "        df[\"exec_ts\"] = _to_dt(df[\"exec_ts\"])\n",
    "        df = df.sort_values([\"exec_ts\", \"broker_order_id\"], kind=\"mergesort\")\n",
    "    else:\n",
    "        df = df.sort_values([\"exec_date\", \"broker_order_id\"], kind=\"mergesort\")\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        t = r[\"ticker\"]\n",
    "        sh = int(r[\"shares\"])\n",
    "        side = r[\"side\"]\n",
    "        if not t or sh <= 0:\n",
    "            continue\n",
    "        if side == \"BUY\":\n",
    "            pos[t] = pos.get(t, 0) + sh\n",
    "        elif side == \"SELL\":\n",
    "            pos[t] = pos.get(t, 0) - sh\n",
    "            if pos.get(t, 0) <= 0:\n",
    "                pos.pop(t, None)\n",
    "    return pos\n",
    "\n",
    "def read_live_portfolio_snapshot(path: str) -> tuple[float, dict]:\n",
    "    port = _safe_read_csv(path)\n",
    "    if port.empty or \"cash\" not in port.columns:\n",
    "        return (np.nan, {})\n",
    "    cash = float(port.iloc[0][\"cash\"])\n",
    "    pos = {}\n",
    "    if \"ticker\" in port.columns and \"shares\" in port.columns:\n",
    "        for _, r in port.iterrows():\n",
    "            t = str(r.get(\"ticker\", \"\")).strip().upper()\n",
    "            sh = int(pd.to_numeric(r.get(\"shares\", 0), errors=\"coerce\") or 0)\n",
    "            if t and sh != 0:\n",
    "                pos[t] = sh\n",
    "    return cash, pos\n",
    "\n",
    "def prev_trading_day(d: pd.Timestamp, cal: pd.DatetimeIndex) -> pd.Timestamp:\n",
    "    d = pd.Timestamp(d).normalize()\n",
    "    if cal is None or len(cal) == 0:\n",
    "        return pd.NaT\n",
    "    i = int(cal.searchsorted(d) - 1)  # strictly before d\n",
    "    return cal[i] if i >= 0 else pd.NaT\n",
    "\n",
    "def trading_day_on_or_before(d: pd.Timestamp, cal: pd.DatetimeIndex) -> pd.Timestamp:\n",
    "    d = pd.Timestamp(d).normalize()\n",
    "    if cal is None or len(cal) == 0:\n",
    "        return pd.NaT\n",
    "    i = int(cal.searchsorted(d, side=\"right\") - 1)  # <= d\n",
    "    return cal[i] if i >= 0 else pd.NaT\n",
    "\n",
    "# ============================================================\n",
    "# Slippage Breakdown (cost vs improvement)\n",
    "# ============================================================\n",
    "\n",
    "def slippage_breakdown(df: pd.DataFrame, slip_col: str, notional_col: str = \"gross_notional\") -> dict:\n",
    "    if df is None or df.empty or slip_col not in df.columns:\n",
    "        return {\"net_dollars\": np.nan, \"gross_cost_dollars\": np.nan, \"gross_improve_dollars\": np.nan, \"net_bps\": np.nan}\n",
    "\n",
    "    slip = pd.to_numeric(df[slip_col], errors=\"coerce\")\n",
    "    pos = slip.where(slip > 0, 0.0)\n",
    "    neg = slip.where(slip < 0, 0.0)\n",
    "\n",
    "    net = float(slip.sum(skipna=True))\n",
    "    gross_cost = float(pos.sum(skipna=True))\n",
    "    gross_improve = float((-neg).sum(skipna=True))\n",
    "\n",
    "    if notional_col in df.columns:\n",
    "        notional = pd.to_numeric(df[notional_col], errors=\"coerce\").abs()\n",
    "        denom = float(notional.sum(skipna=True))\n",
    "    else:\n",
    "        denom = 0.0\n",
    "\n",
    "    net_bps = (net / denom) * 1e4 if denom > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"net_dollars\": net,\n",
    "        \"gross_cost_dollars\": gross_cost,\n",
    "        \"gross_improve_dollars\": gross_improve,\n",
    "        \"net_bps\": net_bps,\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# Price DB: fast last-known lookup\n",
    "# ============================================================\n",
    "\n",
    "def fast_price_lookup(px_array, date_val):\n",
    "    date_val = np.datetime64(pd.Timestamp(date_val), \"ns\")\n",
    "    dates = px_array[\"date\"]\n",
    "    idx = np.searchsorted(dates, date_val, side=\"right\") - 1\n",
    "    if idx < 0:\n",
    "        return np.nan\n",
    "    return float(px_array[\"px\"][idx])\n",
    "\n",
    "def fast_price_lookup_with_date(px_array, date_val):\n",
    "    \"\"\"\n",
    "    Returns (price, date_used) where date_used is the actual price date chosen (last <= date_val).\n",
    "    \"\"\"\n",
    "    if px_array is None:\n",
    "        return (np.nan, pd.NaT)\n",
    "\n",
    "    date_val = np.datetime64(pd.Timestamp(date_val), \"ns\")\n",
    "    dates = px_array[\"date\"]\n",
    "    idx = np.searchsorted(dates, date_val, side=\"right\") - 1\n",
    "    if idx < 0:\n",
    "        return (np.nan, pd.NaT)\n",
    "\n",
    "    px = float(px_array[\"px\"][idx])\n",
    "    dt_used = pd.Timestamp(dates[idx]).normalize()\n",
    "    return (px, dt_used)\n",
    "\n",
    "\n",
    "def build_portfolio_snapshot_from_ledger(exec_df_slice: pd.DataFrame, px_by_ticker: dict, value_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds holdings from the executed ledger slice (positions as of the end of that slice),\n",
    "    and prices each ticker using the last close <= value_date.\n",
    "    \"\"\"\n",
    "    basis = compute_cost_basis_from_ledger(exec_df_slice)\n",
    "\n",
    "    rows = []\n",
    "    for t, info in basis.items():\n",
    "        sh = int(info.get(\"shares_ledger\", 0))\n",
    "        if sh <= 0:\n",
    "            continue\n",
    "\n",
    "        avg_cost = info.get(\"avg_cost\", np.nan)\n",
    "        first_acq = info.get(\"first_acquired\", pd.NaT)\n",
    "\n",
    "        px = np.nan\n",
    "        px_dt = pd.NaT\n",
    "        arr = px_by_ticker.get(t)\n",
    "        if arr is not None and pd.notna(value_date):\n",
    "            px, px_dt = fast_price_lookup_with_date(arr, value_date)\n",
    "\n",
    "        mv = (px * sh) if pd.notna(px) else np.nan\n",
    "        ret_pct = np.nan\n",
    "        if pd.notna(avg_cost) and avg_cost > 0 and pd.notna(px):\n",
    "            ret_pct = (px / avg_cost - 1.0) * 100.0\n",
    "\n",
    "        rows.append({\n",
    "            \"ticker\": t,\n",
    "            \"shares\": sh,\n",
    "            \"avg_cost\": avg_cost,\n",
    "            \"current_price\": px,\n",
    "            \"price_date_used\": px_dt,\n",
    "            \"position_return_pct\": ret_pct,\n",
    "            \"first_acquired\": first_acq,\n",
    "            \"market_value\": mv,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    total_mv = float(df[\"market_value\"].sum(skipna=True))\n",
    "    df[\"weight_pct\"] = np.where(total_mv > 0, (df[\"market_value\"] / total_mv) * 100.0, np.nan)\n",
    "    df = df.sort_values(\"weight_pct\", ascending=False, na_position=\"last\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# Average cost / first acquired from ledger (moving average)\n",
    "# ============================================================\n",
    "\n",
    "def compute_cost_basis_from_ledger(exec_df: pd.DataFrame) -> dict:\n",
    "    state = {}\n",
    "    if exec_df is None or exec_df.empty:\n",
    "        return state\n",
    "\n",
    "    df = exec_df.copy()\n",
    "    df[\"ticker\"] = _norm_ticker(df.get(\"ticker\", \"\"))\n",
    "    df[\"side\"] = _norm_side(df.get(\"side\", \"\"))\n",
    "    df[\"shares\"] = pd.to_numeric(df.get(\"shares\", 0), errors=\"coerce\").fillna(0).astype(int)\n",
    "    df[\"fill_price\"] = pd.to_numeric(df.get(\"fill_price\", np.nan), errors=\"coerce\")\n",
    "    df[\"broker_fee\"] = pd.to_numeric(df.get(\"broker_fee\", 0.0), errors=\"coerce\").fillna(0.0)\n",
    "    df[\"exec_date\"] = _to_dt(df.get(\"exec_date\"))\n",
    "\n",
    "    if \"exec_ts\" in df.columns and df[\"exec_ts\"].notna().any():\n",
    "        df[\"exec_ts\"] = _to_dt(df[\"exec_ts\"])\n",
    "        df = df.sort_values([\"exec_ts\", \"broker_order_id\"], kind=\"mergesort\")\n",
    "    else:\n",
    "        df = df.sort_values([\"exec_date\", \"broker_order_id\"], kind=\"mergesort\")\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        t = r[\"ticker\"]\n",
    "        if not t:\n",
    "            continue\n",
    "        side = r[\"side\"]\n",
    "        sh = int(r[\"shares\"])\n",
    "        if sh <= 0:\n",
    "            continue\n",
    "\n",
    "        px = r[\"fill_price\"]\n",
    "        fee = float(r[\"broker_fee\"] if pd.notna(r[\"broker_fee\"]) else 0.0)\n",
    "        dt = r[\"exec_date\"]\n",
    "\n",
    "        if t not in state:\n",
    "            state[t] = {\"shares\": 0, \"total_cost\": 0.0, \"first_acquired\": pd.NaT}\n",
    "\n",
    "        s = state[t]\n",
    "        shares_before = int(s[\"shares\"])\n",
    "        total_cost_before = float(s[\"total_cost\"])\n",
    "        avg_cost_before = (total_cost_before / shares_before) if shares_before > 0 else np.nan\n",
    "\n",
    "        if side == \"BUY\":\n",
    "            if pd.isna(px) or px <= 0:\n",
    "                continue\n",
    "            if shares_before == 0 and pd.notna(dt):\n",
    "                s[\"first_acquired\"] = dt.normalize()\n",
    "            s[\"shares\"] = shares_before + sh\n",
    "            s[\"total_cost\"] = total_cost_before + (sh * float(px)) + fee\n",
    "\n",
    "        elif side == \"SELL\":\n",
    "            if shares_before <= 0:\n",
    "                continue\n",
    "            sell_sh = min(sh, shares_before)\n",
    "            if not np.isnan(avg_cost_before):\n",
    "                s[\"total_cost\"] = max(0.0, total_cost_before - (avg_cost_before * sell_sh))\n",
    "            s[\"shares\"] = shares_before - sell_sh\n",
    "            if s[\"shares\"] <= 0:\n",
    "                s[\"shares\"] = 0\n",
    "                s[\"total_cost\"] = 0.0\n",
    "                s[\"first_acquired\"] = pd.NaT\n",
    "\n",
    "    out = {}\n",
    "    for t, s in state.items():\n",
    "        sh = int(s[\"shares\"])\n",
    "        avg_cost = (float(s[\"total_cost\"]) / sh) if sh > 0 else np.nan\n",
    "        out[t] = {\"shares_ledger\": sh, \"avg_cost\": avg_cost, \"first_acquired\": s[\"first_acquired\"]}\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# Attach slope_rank into executed ledger (from master_trades.csv)\n",
    "# ============================================================\n",
    "\n",
    "def attach_slope_rank(exec_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    mt = _safe_read_csv(MASTER_TRADES_FILE)\n",
    "    if mt is None or mt.empty:\n",
    "        return exec_df\n",
    "\n",
    "    if \"ticker\" in mt.columns:\n",
    "        mt[\"ticker\"] = _norm_ticker(mt[\"ticker\"])\n",
    "    if \"side\" in mt.columns:\n",
    "        mt[\"side\"] = _norm_side(mt[\"side\"])\n",
    "    for c in (\"signal_date\", \"exec_date\"):\n",
    "        if c in mt.columns:\n",
    "            mt[c] = _to_dt(mt[c]).dt.normalize()\n",
    "\n",
    "    if \"slope_rank\" in mt.columns:\n",
    "        mt[\"slope_rank\"] = pd.to_numeric(mt[\"slope_rank\"], errors=\"coerce\")\n",
    "    else:\n",
    "        return exec_df\n",
    "\n",
    "    out = exec_df.copy()\n",
    "    if \"ticker\" in out.columns:\n",
    "        out[\"ticker\"] = _norm_ticker(out[\"ticker\"])\n",
    "    if \"side\" in out.columns:\n",
    "        out[\"side\"] = _norm_side(out[\"side\"])\n",
    "    for c in (\"signal_date\", \"exec_date\"):\n",
    "        if c in out.columns:\n",
    "            out[c] = _to_dt(out[c]).dt.normalize()\n",
    "\n",
    "    if \"plan_id\" in out.columns and \"plan_id\" in mt.columns:\n",
    "        mt1 = mt[[\"plan_id\", \"slope_rank\"]].drop_duplicates(subset=[\"plan_id\"], keep=\"last\")\n",
    "        return out.merge(mt1, on=\"plan_id\", how=\"left\")\n",
    "\n",
    "    keys = [k for k in [\"signal_date\", \"exec_date\", \"ticker\", \"side\"] if k in out.columns and k in mt.columns]\n",
    "    if len(keys) == 4:\n",
    "        mt2 = mt[keys + [\"slope_rank\"]].drop_duplicates(subset=keys, keep=\"last\")\n",
    "        return out.merge(mt2, on=keys, how=\"left\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# Sort trades by slope-rank (best rank -> worst), buys then sells\n",
    "# ============================================================\n",
    "\n",
    "RANK_COL_CANDIDATES = [\"slope_rank\", \"slope_rank_within_top\", \"rank\"]\n",
    "\n",
    "def sort_trades_by_rank(day_df: pd.DataFrame) -> tuple[pd.DataFrame, str | None]:\n",
    "    if day_df is None or day_df.empty:\n",
    "        return day_df, None\n",
    "\n",
    "    rank_col = next((c for c in RANK_COL_CANDIDATES if c in day_df.columns), None)\n",
    "    if rank_col is None:\n",
    "        return day_df, None\n",
    "\n",
    "    out = day_df.copy()\n",
    "    out[rank_col] = pd.to_numeric(out[rank_col], errors=\"coerce\")\n",
    "\n",
    "    buys  = out[_norm_side(out[\"side\"]) == \"BUY\"].copy()\n",
    "    sells = out[_norm_side(out[\"side\"]) == \"SELL\"].copy()\n",
    "\n",
    "    buys  = buys.sort_values(rank_col, ascending=True, na_position=\"last\", kind=\"mergesort\")\n",
    "    sells = sells.sort_values(rank_col, ascending=True, na_position=\"last\", kind=\"mergesort\")\n",
    "\n",
    "    return pd.concat([buys, sells], ignore_index=True), rank_col\n",
    "\n",
    "# ============================================================\n",
    "# NEW: SPY series loader (from regime parquet)\n",
    "# ============================================================\n",
    "\n",
    "def load_spy_close_series() -> pd.Series:\n",
    "    path = SPY_REGIME_FILE_PRIMARY if os.path.exists(SPY_REGIME_FILE_PRIMARY) else SPY_REGIME_FILE_FALLBACK\n",
    "    if not os.path.exists(path):\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    df = pd.read_parquet(path)\n",
    "    # Your file shows index named \"Date\"\n",
    "    if \"spy_close\" not in df.columns:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    if df.index.name is None or str(df.index.name).lower() not in (\"date\", \"datetime\"):\n",
    "        # if Date is a column, try using it\n",
    "        if \"Date\" in df.columns:\n",
    "            df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "            df = df.dropna(subset=[\"Date\"]).set_index(\"Date\")\n",
    "        else:\n",
    "            # last resort: coerce index to datetime\n",
    "            df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "\n",
    "    df = df.sort_index()\n",
    "    s = pd.to_numeric(df[\"spy_close\"], errors=\"coerce\").dropna()\n",
    "    s.index = pd.to_datetime(s.index).normalize()\n",
    "    return s\n",
    "\n",
    "# ============================================================\n",
    "# NEW: Strategy equity curve + performance metrics\n",
    "# ============================================================\n",
    "\n",
    "def build_strategy_equity_curve(exec_df: pd.DataFrame, px_by_ticker: dict, date_index: pd.DatetimeIndex) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Builds end-of-day equity = cash + sum(shares * close_adj) using:\n",
    "      - cash simulated from fills (fill_price, broker_fee)\n",
    "      - positions updated from BUY/SELL shares\n",
    "      - marks to market using px_by_ticker fast lookup\n",
    "    \"\"\"\n",
    "    if exec_df is None or exec_df.empty or len(date_index) == 0:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    df = exec_df.copy()\n",
    "    df[\"exec_ts\"] = _to_dt(df.get(\"exec_ts\"))\n",
    "    df[\"exec_date\"] = _to_dt(df.get(\"exec_date\")).dt.normalize()\n",
    "    df[\"ticker\"] = _norm_ticker(df.get(\"ticker\", \"\"))\n",
    "    df[\"side\"] = _norm_side(df.get(\"side\", \"\"))\n",
    "    df[\"shares\"] = pd.to_numeric(df.get(\"shares\", 0), errors=\"coerce\").fillna(0).astype(int)\n",
    "    df[\"fill_price\"] = pd.to_numeric(df.get(\"fill_price\", np.nan), errors=\"coerce\")\n",
    "    df[\"broker_fee\"] = pd.to_numeric(df.get(\"broker_fee\", 0.0), errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # sort trades\n",
    "    if df[\"exec_ts\"].notna().any():\n",
    "        df = df.sort_values([\"exec_ts\", \"broker_order_id\"], kind=\"mergesort\")\n",
    "    else:\n",
    "        df = df.sort_values([\"exec_date\", \"broker_order_id\"], kind=\"mergesort\")\n",
    "\n",
    "    # start cash: first cash_before if present, else first cash_after, else 0\n",
    "    cash0 = np.nan\n",
    "    if \"cash_before\" in df.columns and df[\"cash_before\"].notna().any():\n",
    "        cash0 = float(pd.to_numeric(df[\"cash_before\"], errors=\"coerce\").dropna().iloc[0])\n",
    "    elif \"cash_after\" in df.columns and df[\"cash_after\"].notna().any():\n",
    "        cash0 = float(pd.to_numeric(df[\"cash_after\"], errors=\"coerce\").dropna().iloc[0])\n",
    "    if np.isnan(cash0):\n",
    "        cash0 = 0.0\n",
    "\n",
    "    # group trades by exec_date for fast daily application\n",
    "    trades_by_day = {d: g for d, g in df.groupby(\"exec_date\")}\n",
    "\n",
    "    cash = float(cash0)\n",
    "    pos = {}  # ticker -> shares\n",
    "    equity = []\n",
    "\n",
    "    for d in date_index:\n",
    "        # apply trades for day d\n",
    "        g = trades_by_day.get(pd.Timestamp(d).normalize())\n",
    "        if g is not None:\n",
    "            for _, r in g.iterrows():\n",
    "                t = r[\"ticker\"]\n",
    "                side = r[\"side\"]\n",
    "                sh = int(r[\"shares\"])\n",
    "                px = r[\"fill_price\"]\n",
    "                fee = float(r[\"broker_fee\"]) if pd.notna(r[\"broker_fee\"]) else 0.0\n",
    "                if not t or sh <= 0 or pd.isna(px) or px <= 0:\n",
    "                    continue\n",
    "\n",
    "                if side == \"BUY\":\n",
    "                    cash -= (sh * float(px)) + fee\n",
    "                    pos[t] = pos.get(t, 0) + sh\n",
    "                elif side == \"SELL\":\n",
    "                    sell_sh = min(sh, pos.get(t, 0))\n",
    "                    if sell_sh <= 0:\n",
    "                        continue\n",
    "                    cash += (sell_sh * float(px)) - fee\n",
    "                    pos[t] = pos.get(t, 0) - sell_sh\n",
    "                    if pos[t] <= 0:\n",
    "                        pos.pop(t, None)\n",
    "\n",
    "        # mark-to-market\n",
    "        mv = 0.0\n",
    "        for t, sh in pos.items():\n",
    "            arr = px_by_ticker.get(t)\n",
    "            if arr is None:\n",
    "                continue\n",
    "            p = fast_price_lookup(arr, d)\n",
    "            if pd.notna(p):\n",
    "                mv += float(p) * float(sh)\n",
    "\n",
    "        equity.append(cash + mv)\n",
    "\n",
    "    return pd.Series(equity, index=date_index, name=\"strategy_equity\")\n",
    "\n",
    "def build_strategy_state_curves(exec_df: pd.DataFrame, px_by_ticker: dict, date_index: pd.DatetimeIndex) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds daily end-of-day curves:\n",
    "      - cash (uses audited cash_after when available)\n",
    "      - securities_value (MTM using close_adj via px_by_ticker)\n",
    "      - total_value = cash + securities_value\n",
    "\n",
    "    Trades are applied intraday; then we overwrite cash with the last audited cash_after for that day (if present),\n",
    "    so cash matches broker/ledger reconciliation when possible.\n",
    "    \"\"\"\n",
    "    if exec_df is None or exec_df.empty or len(date_index) == 0:\n",
    "        return pd.DataFrame(columns=[\"cash\", \"securities_value\", \"total_value\"])\n",
    "\n",
    "    df = exec_df.copy()\n",
    "    df[\"exec_ts\"] = _to_dt(df.get(\"exec_ts\"))\n",
    "    df[\"exec_date\"] = _to_dt(df.get(\"exec_date\")).dt.normalize()\n",
    "    df[\"ticker\"] = _norm_ticker(df.get(\"ticker\", \"\"))\n",
    "    df[\"side\"] = _norm_side(df.get(\"side\", \"\"))\n",
    "    df[\"shares\"] = pd.to_numeric(df.get(\"shares\", 0), errors=\"coerce\").fillna(0).astype(int)\n",
    "    df[\"fill_price\"] = pd.to_numeric(df.get(\"fill_price\", np.nan), errors=\"coerce\")\n",
    "    df[\"broker_fee\"] = pd.to_numeric(df.get(\"broker_fee\", 0.0), errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # audited cash columns (optional)\n",
    "    if \"cash_before\" in df.columns:\n",
    "        df[\"cash_before\"] = pd.to_numeric(df[\"cash_before\"], errors=\"coerce\")\n",
    "    if \"cash_after\" in df.columns:\n",
    "        df[\"cash_after\"] = pd.to_numeric(df[\"cash_after\"], errors=\"coerce\")\n",
    "\n",
    "    # sort trades\n",
    "    # sort trades: prefer ledger order; fallback if missing\n",
    "    if \"_ledger_seq\" in df.columns:\n",
    "        df = df.sort_values(\"_ledger_seq\", kind=\"mergesort\")\n",
    "    else:\n",
    "        df[\"_boid_num\"] = pd.to_numeric(df.get(\"broker_order_id\"), errors=\"coerce\")\n",
    "        if df[\"exec_ts\"].notna().any():\n",
    "            df = df.sort_values([\"exec_ts\", \"_boid_num\", \"broker_order_id\"], kind=\"mergesort\")\n",
    "        else:\n",
    "            df = df.sort_values([\"exec_date\", \"_boid_num\", \"broker_order_id\"], kind=\"mergesort\")\n",
    "        df = df.drop(columns=[\"_boid_num\"])\n",
    "\n",
    "\n",
    "    # start cash\n",
    "    cash0 = np.nan\n",
    "    if \"cash_before\" in df.columns and df[\"cash_before\"].notna().any():\n",
    "        cash0 = float(df[\"cash_before\"].dropna().iloc[0])\n",
    "    elif \"cash_after\" in df.columns and df[\"cash_after\"].notna().any():\n",
    "        cash0 = float(df[\"cash_after\"].dropna().iloc[0])\n",
    "    if np.isnan(cash0):\n",
    "        cash0 = 0.0\n",
    "\n",
    "    trades_by_day = {d: g for d, g in df.groupby(\"exec_date\")}\n",
    "\n",
    "    cash = float(cash0)\n",
    "    pos = {}  # ticker -> shares\n",
    "\n",
    "    rows = []\n",
    "    for d in date_index:\n",
    "        d0 = pd.Timestamp(d).normalize()\n",
    "\n",
    "        g = trades_by_day.get(d0)\n",
    "        if g is not None and not g.empty:\n",
    "            # apply trades to cash/pos\n",
    "            for _, r in g.iterrows():\n",
    "                t = r[\"ticker\"]\n",
    "                side = r[\"side\"]\n",
    "                sh = int(r[\"shares\"])\n",
    "                px = r[\"fill_price\"]\n",
    "                fee = float(r[\"broker_fee\"]) if pd.notna(r[\"broker_fee\"]) else 0.0\n",
    "\n",
    "                if not t or sh <= 0 or pd.isna(px) or px <= 0:\n",
    "                    continue\n",
    "\n",
    "                if side == \"BUY\":\n",
    "                    cash -= (sh * float(px)) + fee\n",
    "                    pos[t] = pos.get(t, 0) + sh\n",
    "                elif side == \"SELL\":\n",
    "                    sell_sh = min(sh, pos.get(t, 0))\n",
    "                    if sell_sh <= 0:\n",
    "                        continue\n",
    "                    cash += (sell_sh * float(px)) - fee\n",
    "                    pos[t] = pos.get(t, 0) - sell_sh\n",
    "                    if pos[t] <= 0:\n",
    "                        pos.pop(t, None)\n",
    "\n",
    "            # overwrite with audited cash_after if present (last trade of the day)\n",
    "            # overwrite with audited cash_after if present (last trade of the day)\n",
    "            if \"cash_after\" in g.columns and g[\"cash_after\"].notna().any():\n",
    "                if \"_ledger_seq\" in g.columns:\n",
    "                    g = g.sort_values(\"_ledger_seq\", kind=\"mergesort\")\n",
    "                audited_cash = float(g[\"cash_after\"].dropna().iloc[-1])\n",
    "                cash = audited_cash  # keep future days consistent too\n",
    "\n",
    "        # mark-to-market securities value\n",
    "        mv = 0.0\n",
    "        for t, sh in pos.items():\n",
    "            arr = px_by_ticker.get(t)\n",
    "            if arr is None:\n",
    "                continue\n",
    "            p = fast_price_lookup(arr, d0)\n",
    "            if pd.notna(p):\n",
    "                mv += float(p) * float(sh)\n",
    "\n",
    "        total = cash + mv\n",
    "        rows.append({\"date\": d0, \"cash\": cash, \"securities_value\": mv, \"total_value\": total})\n",
    "\n",
    "    out = pd.DataFrame(rows).set_index(\"date\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def perf_metrics_from_equity(equity: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    Returns YTD metrics:\n",
    "      - return (simple)\n",
    "      - max drawdown\n",
    "      - sharpe (annualized)\n",
    "      - sortino (annualized)\n",
    "      - calmar (annualized_return / max_dd)\n",
    "    \"\"\"\n",
    "    if equity is None or equity.empty:\n",
    "        return {\"ret\": np.nan, \"max_dd\": np.nan, \"sharpe\": np.nan, \"sortino\": np.nan, \"calmar\": np.nan}\n",
    "\n",
    "    eq = pd.to_numeric(equity, errors=\"coerce\").dropna()\n",
    "    if len(eq) < 2:\n",
    "        return {\"ret\": 0.0, \"max_dd\": 0.0, \"sharpe\": np.nan, \"sortino\": np.nan, \"calmar\": np.nan}\n",
    "\n",
    "    rets = eq.pct_change().dropna()\n",
    "    if rets.empty:\n",
    "        return {\"ret\": 0.0, \"max_dd\": 0.0, \"sharpe\": np.nan, \"sortino\": np.nan, \"calmar\": np.nan}\n",
    "\n",
    "    total_ret = float(eq.iloc[-1] / eq.iloc[0] - 1.0)\n",
    "\n",
    "    roll_max = eq.cummax()\n",
    "    dd = (eq / roll_max) - 1.0\n",
    "    max_dd = float(dd.min())  # negative number (e.g. -0.12)\n",
    "\n",
    "    mu = float(rets.mean())\n",
    "    sd = float(rets.std(ddof=0))\n",
    "    sharpe = (np.sqrt(TRADING_DAYS_PER_YEAR) * mu / sd) if sd > 0 else np.nan\n",
    "\n",
    "    neg = rets[rets < 0]\n",
    "    sd_neg = float(neg.std(ddof=0)) if len(neg) > 0 else 0.0\n",
    "    sortino = (np.sqrt(TRADING_DAYS_PER_YEAR) * mu / sd_neg) if sd_neg > 0 else np.nan\n",
    "\n",
    "    # annualized return over observed days\n",
    "    n = len(rets)\n",
    "    ann_ret = ((1.0 + total_ret) ** (TRADING_DAYS_PER_YEAR / max(1, n))) - 1.0\n",
    "    calmar = (ann_ret / abs(max_dd)) if (pd.notna(max_dd) and max_dd < 0) else np.nan\n",
    "\n",
    "    return {\"ret\": total_ret, \"max_dd\": max_dd, \"sharpe\": sharpe, \"sortino\": sortino, \"calmar\": calmar}\n",
    "\n",
    "def save_perf_chart(path: str, dates: pd.DatetimeIndex, strat_eq: pd.Series, spy_eq: pd.Series, title: str):\n",
    "    fig = plt.figure(figsize=(10, 3.2))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ok = True\n",
    "    if dates is None or len(dates) < 2:\n",
    "        ok = False\n",
    "    if strat_eq is None or strat_eq.dropna().shape[0] < 2:\n",
    "        ok = False\n",
    "    if spy_eq is None or spy_eq.dropna().shape[0] < 2:\n",
    "        ok = False\n",
    "\n",
    "    if not ok:\n",
    "        ax.text(0.5, 0.5, \"NO DATA TO PLOT\", ha=\"center\", va=\"center\")\n",
    "        ax.set_title(title)\n",
    "        ax.set_axis_off()\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(path, dpi=150)\n",
    "        plt.close(fig)\n",
    "        return\n",
    "\n",
    "    # align + normalize to 1.0 at start\n",
    "    s1 = pd.to_numeric(strat_eq.reindex(dates), errors=\"coerce\").dropna()\n",
    "    s2 = pd.to_numeric(spy_eq.reindex(dates), errors=\"coerce\").dropna()\n",
    "    common = s1.index.intersection(s2.index)\n",
    "\n",
    "    if len(common) < 2:\n",
    "        ax.text(0.5, 0.5, \"NO DATA TO PLOT\", ha=\"center\", va=\"center\")\n",
    "        ax.set_title(title)\n",
    "        ax.set_axis_off()\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(path, dpi=150)\n",
    "        plt.close(fig)\n",
    "        return\n",
    "\n",
    "    s1 = s1.loc[common]\n",
    "    s2 = s2.loc[common]\n",
    "\n",
    "    s1n = s1 / float(s1.iloc[0])\n",
    "    s2n = s2 / float(s2.iloc[0])\n",
    "\n",
    "    ax.plot(common, s1n, marker=\"o\", label=\"Strategy\")\n",
    "    ax.plot(common, s2n, marker=\"o\", label=\"SPY\")\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load data\n",
    "# ============================================================\n",
    "\n",
    "exec_df = _safe_read_csv(EXECUTED_TRADES_FILE)\n",
    "# Preserve the physical row order of executed_trades.csv (append-only ledger order)\n",
    "exec_df[\"_ledger_seq\"] = np.arange(len(exec_df), dtype=int)\n",
    "\n",
    "recon_log = _safe_read_csv(RECON_LOG_FILE)\n",
    "fills_df  = _safe_read_csv(MANUAL_FILLS_FILE)\n",
    "live_cash, live_pos = read_live_portfolio_snapshot(LIVE_PORTFOLIO_FILE)\n",
    "\n",
    "if exec_df.empty:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing or empty executed trades ledger: {EXECUTED_TRADES_FILE}\\n\"\n",
    "        f\"Run your reconciliation script at least once.\"\n",
    "    )\n",
    "\n",
    "# normalize columns/types\n",
    "exec_df[\"exec_ts\"] = _to_dt(exec_df.get(\"exec_ts\"))\n",
    "exec_df[\"exec_date\"] = _to_dt(exec_df.get(\"exec_date\"))\n",
    "exec_df[\"signal_date\"] = _to_dt(exec_df.get(\"signal_date\"))\n",
    "\n",
    "exec_df[\"ticker\"] = _norm_ticker(exec_df.get(\"ticker\", \"\"))\n",
    "exec_df[\"side\"] = _norm_side(exec_df.get(\"side\", \"\"))\n",
    "exec_df[\"broker_order_id\"] = exec_df.get(\"broker_order_id\", \"\").astype(str).str.strip()\n",
    "\n",
    "for c in (\"shares\", \"broker_fee\", \"gross_notional\", \"net_cash_impact\", \"order_price\", \"fill_price\", \"signal_price\", \"cash_before\", \"cash_after\"):\n",
    "    if c in exec_df.columns:\n",
    "        exec_df[c] = pd.to_numeric(exec_df[c], errors=\"coerce\")\n",
    "\n",
    "# attach slope_rank so the table can sort by it\n",
    "# attach slope_rank so the table can sort by it\n",
    "exec_df = attach_slope_rank(exec_df)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: Load rankings data (around line 715, after exec_df load)\n",
    "# ============================================================\n",
    "# ADD after the line: exec_df = attach_slope_rank(exec_df)\n",
    "\n",
    "# NEW: Load master rankings\n",
    "rankings_df = _safe_read_csv(MASTER_RANKINGS_FILE)\n",
    "\n",
    "# NEW: Normalize rankings data\n",
    "if not rankings_df.empty:\n",
    "    rankings_df[\"signal_date\"] = _to_dt(rankings_df.get(\"signal_date\"))\n",
    "    rankings_df[\"exec_date\"] = _to_dt(rankings_df.get(\"exec_date\"))\n",
    "    rankings_df[\"ticker\"] = _norm_ticker(rankings_df.get(\"ticker\", \"\"))\n",
    "    \n",
    "    for c in (\"slope_rank\", \"slope_adj\", \"atr20\", \"close_adj\", \"raw_weight\", \"capped_weight\",\n",
    "              \"target_value\", \"target_weight\", \"target_shares\", \"current_shares\",\n",
    "              \"current_value\", \"current_weight\", \"weight_change\", \"shares_change\"):\n",
    "        if c in rankings_df.columns:\n",
    "            rankings_df[c] = pd.to_numeric(rankings_df[c], errors=\"coerce\")\n",
    "\n",
    "# Group rankings by signal_date for weekly lookup\n",
    "rankings_by_signal_date = {}\n",
    "if not rankings_df.empty and \"signal_date\" in rankings_df.columns:\n",
    "    rankings_by_signal_date = {\n",
    "        d: sub for d, sub in rankings_df.groupby(rankings_df[\"signal_date\"].dt.normalize())\n",
    "    }\n",
    "\n",
    "\n",
    "# IMPORTANT: merges can reorder rows; restore append-only ledger order\n",
    "if \"_ledger_seq\" in exec_df.columns:\n",
    "    exec_df = exec_df.sort_values(\"_ledger_seq\", kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Build BOTH slippage columns (into SIG_SLIP_COL / ORD_SLIP_COL)\n",
    "# ============================================================\n",
    "\n",
    "side = _norm_side(exec_df[\"side\"])\n",
    "sh   = pd.to_numeric(exec_df.get(\"shares\", np.nan), errors=\"coerce\")\n",
    "\n",
    "exec_df[SIG_SLIP_COL] = np.nan\n",
    "if {\"signal_price\",\"fill_price\",\"shares\",\"side\"}.issubset(exec_df.columns):\n",
    "    s_px = pd.to_numeric(exec_df[\"signal_price\"], errors=\"coerce\")\n",
    "    f_px = pd.to_numeric(exec_df[\"fill_price\"], errors=\"coerce\")\n",
    "    exec_df[SIG_SLIP_COL] = np.where(\n",
    "        side == \"BUY\",  (f_px - s_px) * sh,\n",
    "        np.where(side == \"SELL\", (s_px - f_px) * sh, np.nan)\n",
    "    )\n",
    "\n",
    "exec_df[ORD_SLIP_COL] = np.nan\n",
    "if {\"order_price\",\"fill_price\",\"shares\",\"side\"}.issubset(exec_df.columns):\n",
    "    o_px = pd.to_numeric(exec_df[\"order_price\"], errors=\"coerce\")\n",
    "    f_px = pd.to_numeric(exec_df[\"fill_price\"], errors=\"coerce\")\n",
    "    exec_df[ORD_SLIP_COL] = np.where(\n",
    "        side == \"BUY\",  (f_px - o_px) * sh,\n",
    "        np.where(side == \"SELL\", (o_px - f_px) * sh, np.nan)\n",
    "    )\n",
    "\n",
    "FIRST_EXEC_DATE = exec_df[\"exec_date\"].min()\n",
    "LAST_EXEC_DATE  = exec_df[\"exec_date\"].max()\n",
    "\n",
    "# ============================================================\n",
    "# Load price DB and build fast lookup by ticker\n",
    "# ============================================================\n",
    "\n",
    "price_asof_date = pd.NaT\n",
    "px_by_ticker = {}\n",
    "try:\n",
    "    if os.path.exists(PRICE_DB_FILE):\n",
    "        price_df = pd.read_parquet(PRICE_DB_FILE)\n",
    "        if PRICE_DATE_COL in price_df.columns and PRICE_TICKER_COL in price_df.columns and PRICE_PX_COL in price_df.columns:\n",
    "            price_df[PRICE_DATE_COL] = pd.to_datetime(price_df[PRICE_DATE_COL], errors=\"coerce\")\n",
    "            price_df[PRICE_TICKER_COL] = price_df[PRICE_TICKER_COL].astype(str).str.upper().str.strip()\n",
    "            price_df[PRICE_PX_COL] = pd.to_numeric(price_df[PRICE_PX_COL], errors=\"coerce\")\n",
    "            price_df = price_df.dropna(subset=[PRICE_DATE_COL, PRICE_TICKER_COL]).sort_values([PRICE_TICKER_COL, PRICE_DATE_COL])\n",
    "            price_asof_date = price_df[PRICE_DATE_COL].max()\n",
    "\n",
    "            for t, sub in price_df.groupby(PRICE_TICKER_COL, sort=False):\n",
    "                sub = sub.sort_values(PRICE_DATE_COL)\n",
    "                arr = np.zeros(len(sub), dtype=[(\"date\",\"datetime64[ns]\"), (\"px\",\"float64\")])\n",
    "                arr[\"date\"] = sub[PRICE_DATE_COL].values.astype(\"datetime64[ns]\")\n",
    "                arr[\"px\"] = sub[PRICE_PX_COL].astype(float).values\n",
    "                px_by_ticker[t] = arr\n",
    "        else:\n",
    "            price_df = pd.DataFrame()\n",
    "    else:\n",
    "        price_df = pd.DataFrame()\n",
    "except Exception:\n",
    "    price_df = pd.DataFrame()\n",
    "    \n",
    "# ============================================================\n",
    "# ASOF (needed early) + Load SPY close series (needed for state_curves)\n",
    "# ============================================================\n",
    "\n",
    "# Use the later of (price DB last date) and (last exec date) so state_curves includes all exec days.\n",
    "_asof_price = price_asof_date if pd.notna(price_asof_date) else pd.NaT\n",
    "_asof_exec  = LAST_EXEC_DATE.normalize() if pd.notna(LAST_EXEC_DATE) else pd.NaT\n",
    "\n",
    "if pd.notna(_asof_price) and pd.notna(_asof_exec):\n",
    "    asof = max(_asof_price, _asof_exec)\n",
    "elif pd.notna(_asof_price):\n",
    "    asof = _asof_price\n",
    "elif pd.notna(_asof_exec):\n",
    "    asof = _asof_exec\n",
    "else:\n",
    "    asof = pd.Timestamp.today().normalize()\n",
    "\n",
    "\n",
    "spy_close = load_spy_close_series()\n",
    "spy_last_date = spy_close.index.max() if not spy_close.empty else pd.NaT\n",
    "\n",
    "# ============================================================\n",
    "# Trading calendar for valuation-date logic\n",
    "# ============================================================\n",
    "\n",
    "if \"price_df\" in globals() and isinstance(price_df, pd.DataFrame) and (not price_df.empty) and (PRICE_DATE_COL in price_df.columns):\n",
    "    _cal = pd.to_datetime(price_df[PRICE_DATE_COL], errors=\"coerce\").dropna().dt.normalize().unique()\n",
    "    TRADING_CAL = pd.DatetimeIndex(sorted(_cal))\n",
    "elif spy_close is not None and not spy_close.empty:\n",
    "    TRADING_CAL = pd.DatetimeIndex(sorted(pd.to_datetime(spy_close.index).normalize().unique()))\n",
    "else:\n",
    "    TRADING_CAL = pd.DatetimeIndex(sorted(exec_dates_norm.unique()))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NEW: Daily state curves for per-week portfolio & cash blocks\n",
    "# ============================================================\n",
    "\n",
    "state_curves = pd.DataFrame()\n",
    "\n",
    "# Make sure ALL exec dates exist in the date index (even if SPY is missing that date)\n",
    "exec_dates_norm = exec_df[\"exec_date\"].dropna().dt.normalize()\n",
    "base_dates = pd.DatetimeIndex([])\n",
    "\n",
    "if not spy_close.empty:\n",
    "    base_dates = spy_close.index\n",
    "else:\n",
    "    # fallback: use exec dates only\n",
    "    base_dates = exec_dates_norm.sort_values().unique()\n",
    "\n",
    "# union to guarantee coverage\n",
    "all_dates = pd.DatetimeIndex(sorted(set(pd.to_datetime(base_dates).normalize()) | set(exec_dates_norm)))\n",
    "\n",
    "# optional clamp to a reasonable range\n",
    "# optional clamp to a reasonable range\n",
    "first_needed = pd.Timestamp(FIRST_EXEC_DATE).normalize()\n",
    "last_exec_needed = exec_dates_norm.max() if len(exec_dates_norm) else pd.Timestamp(asof).normalize()\n",
    "last_needed = max(pd.Timestamp(asof).normalize(), pd.Timestamp(last_exec_needed).normalize())\n",
    "\n",
    "all_dates = all_dates[(all_dates >= first_needed) & (all_dates <= last_needed)]\n",
    "\n",
    "\n",
    "if len(all_dates) >= 1:\n",
    "    state_curves = build_strategy_state_curves(exec_df, px_by_ticker, all_dates)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Reconciliation quality checks\n",
    "# ============================================================\n",
    "\n",
    "dup_counts = exec_df[\"broker_order_id\"].value_counts(dropna=False)\n",
    "dups = dup_counts[dup_counts > 1]\n",
    "dup_df = (\n",
    "    exec_df[exec_df[\"broker_order_id\"].isin(dups.index)]\n",
    "    .sort_values([\"broker_order_id\", \"exec_ts\"], kind=\"mergesort\")\n",
    "    if not dups.empty else pd.DataFrame()\n",
    ")\n",
    "\n",
    "pending_df = pd.DataFrame()\n",
    "if not fills_df.empty and \"broker_order_id\" in fills_df.columns:\n",
    "    fills_df[\"broker_order_id\"] = fills_df[\"broker_order_id\"].astype(str).str.strip()\n",
    "    executed_ids = set(exec_df[\"broker_order_id\"].astype(str))\n",
    "    pending_df = fills_df[~fills_df[\"broker_order_id\"].isin(executed_ids)].copy()\n",
    "\n",
    "ledger_pos = reconstruct_positions_from_ledger(exec_df)\n",
    "\n",
    "all_tickers = sorted(set(ledger_pos.keys()) | set(live_pos.keys()))\n",
    "drift_rows = []\n",
    "for t in all_tickers:\n",
    "    drift_rows.append({\n",
    "        \"ticker\": t,\n",
    "        \"ledger_shares\": ledger_pos.get(t, 0),\n",
    "        \"live_shares\": live_pos.get(t, 0),\n",
    "        \"diff_shares\": live_pos.get(t, 0) - ledger_pos.get(t, 0),\n",
    "    })\n",
    "drift_df = pd.DataFrame(drift_rows)\n",
    "\n",
    "ledger_cash = float(exec_df[\"cash_after\"].dropna().iloc[-1]) if \"cash_after\" in exec_df.columns and exec_df[\"cash_after\"].notna().any() else np.nan\n",
    "cash_drift = (live_cash - ledger_cash) if (not np.isnan(live_cash) and not np.isnan(ledger_cash)) else np.nan\n",
    "\n",
    "# ============================================================\n",
    "# Build CURRENT portfolio table with avg cost + current price + returns\n",
    "# ============================================================\n",
    "\n",
    "basis_map = compute_cost_basis_from_ledger(exec_df)\n",
    "\n",
    "port_rows = []\n",
    "# asof already defined above (after price DB load)\n",
    "\n",
    "for t, sh_ in live_pos.items():\n",
    "    row = {\n",
    "        \"ticker\": t,\n",
    "        \"shares\": int(sh_),\n",
    "        \"avg_cost\": np.nan,\n",
    "        \"first_acquired\": pd.NaT,\n",
    "        \"current_price\": np.nan,\n",
    "        \"market_value\": np.nan,\n",
    "        \"weight_pct\": np.nan,\n",
    "        \"position_return_pct\": np.nan,\n",
    "    }\n",
    "\n",
    "    if t in basis_map:\n",
    "        row[\"avg_cost\"] = basis_map[t][\"avg_cost\"]\n",
    "        row[\"first_acquired\"] = basis_map[t][\"first_acquired\"]\n",
    "\n",
    "    if t in px_by_ticker and pd.notna(asof):\n",
    "        row[\"current_price\"] = fast_price_lookup(px_by_ticker[t], asof)\n",
    "\n",
    "    if pd.notna(row[\"current_price\"]):\n",
    "        row[\"market_value\"] = row[\"current_price\"] * row[\"shares\"]\n",
    "\n",
    "    if pd.notna(row[\"avg_cost\"]) and row[\"avg_cost\"] > 0 and pd.notna(row[\"current_price\"]):\n",
    "        row[\"position_return_pct\"] = (row[\"current_price\"] / row[\"avg_cost\"] - 1.0) * 100.0\n",
    "\n",
    "    port_rows.append(row)\n",
    "\n",
    "port_df = pd.DataFrame(port_rows)\n",
    "if not port_df.empty:\n",
    "    total_mv = float(port_df[\"market_value\"].sum(skipna=True))\n",
    "    port_df[\"weight_pct\"] = np.where(total_mv > 0, (port_df[\"market_value\"] / total_mv) * 100.0, np.nan)\n",
    "    port_df = port_df.sort_values(\"weight_pct\", ascending=False, na_position=\"last\").reset_index(drop=True)\n",
    "\n",
    "if drift_df is not None and not drift_df.empty:\n",
    "    drift_df = drift_df.merge(port_df[[\"ticker\", \"weight_pct\"]], on=\"ticker\", how=\"left\")\n",
    "    drift_df = drift_df[drift_df[\"diff_shares\"] != 0].sort_values(\"ticker\") if not drift_df.empty else drift_df\n",
    "\n",
    "# ============================================================\n",
    "# Weekly aggregation (by exec_date)\n",
    "# ============================================================\n",
    "\n",
    "weekly = exec_df.copy()\n",
    "weekly = weekly[weekly[\"exec_date\"].notna()].copy()\n",
    "weekly[\"exec_date_only\"] = weekly[\"exec_date\"].dt.normalize()\n",
    "weekly_grp = weekly.groupby(\"exec_date_only\", dropna=True)\n",
    "\n",
    "weekly_summary = weekly_grp.agg(\n",
    "    trades=(\"broker_order_id\", \"count\"),\n",
    "    buys=(\"side\", lambda s: int((_norm_side(s) == \"BUY\").sum())),\n",
    "    sells=(\"side\", lambda s: int((_norm_side(s) == \"SELL\").sum())),\n",
    "    gross_notional=(\"gross_notional\", \"sum\"),\n",
    "    fees=(\"broker_fee\", \"sum\"),\n",
    "    missing_signal_price=(\"signal_price\", lambda s: int(pd.to_numeric(s, errors=\"coerce\").isna().sum())) if \"signal_price\" in weekly.columns else (\"broker_order_id\", lambda s: 0),\n",
    "    missing_order_price=(\"order_price\", lambda s: int(pd.to_numeric(s, errors=\"coerce\").isna().sum())) if \"order_price\" in weekly.columns else (\"broker_order_id\", lambda s: 0),\n",
    ").reset_index().rename(columns={\"exec_date_only\": \"exec_date\"})\n",
    "\n",
    "def _slip_apply(g: pd.DataFrame) -> pd.Series:\n",
    "    sig = slippage_breakdown(g, slip_col=SIG_SLIP_COL, notional_col=\"gross_notional\")\n",
    "    ord_ = slippage_breakdown(g, slip_col=ORD_SLIP_COL, notional_col=\"gross_notional\")\n",
    "    return pd.Series({\n",
    "        \"sig_slip_net_dollars\": sig[\"net_dollars\"],\n",
    "        \"sig_slip_gross_cost_dollars\": sig[\"gross_cost_dollars\"],\n",
    "        \"sig_slip_gross_improve_dollars\": sig[\"gross_improve_dollars\"],\n",
    "        \"sig_slip_net_bps\": sig[\"net_bps\"],\n",
    "        \"ord_slip_net_dollars\": ord_[\"net_dollars\"],\n",
    "        \"ord_slip_gross_cost_dollars\": ord_[\"gross_cost_dollars\"],\n",
    "        \"ord_slip_gross_improve_dollars\": ord_[\"gross_improve_dollars\"],\n",
    "        \"ord_slip_net_bps\": ord_[\"net_bps\"],\n",
    "    })\n",
    "\n",
    "slip_by_day = weekly_grp.apply(_slip_apply).reset_index().rename(columns={\"exec_date_only\": \"exec_date\"})\n",
    "weekly_summary = weekly_summary.merge(slip_by_day, on=\"exec_date\", how=\"left\")\n",
    "\n",
    "weekly_summary = weekly_summary[\n",
    "    (weekly_summary[\"exec_date\"].dt.year >= MIN_YEAR_FOR_REPORT) &\n",
    "    (weekly_summary[\"exec_date\"].dt.year <= MAX_YEAR_FOR_REPORT)\n",
    "].sort_values(\"exec_date\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NEW: previous execution-day mapping (for weekly portfolio return)\n",
    "# ============================================================\n",
    "\n",
    "_exec_days = (\n",
    "    pd.to_datetime(weekly_summary[\"exec_date\"], errors=\"coerce\")\n",
    "      .dropna()\n",
    "      .dt.normalize()\n",
    "      .sort_values()\n",
    "      .tolist()\n",
    ")\n",
    "\n",
    "prev_exec_map = {d: (_exec_days[i-1] if i > 0 else pd.NaT) for i, d in enumerate(_exec_days)}\n",
    "\n",
    "\n",
    "def ytd_slice(dt: pd.Timestamp) -> pd.DataFrame:\n",
    "    start = max(pd.Timestamp(dt.year, 1, 1), FIRST_EXEC_DATE.normalize())\n",
    "    return weekly[(weekly[\"exec_date_only\"] >= start) & (weekly[\"exec_date_only\"] <= dt)]\n",
    "\n",
    "# ============================================================\n",
    "# Charts (existing)\n",
    "# ============================================================\n",
    "\n",
    "chart_dir = OUTPUT_DIR\n",
    "fee_chart_path = os.path.join(chart_dir, \"cum_fees.png\")\n",
    "slip_chart_path = os.path.join(chart_dir, \"cum_slippage_breakout.png\")\n",
    "count_chart_path = os.path.join(chart_dir, \"trade_counts.png\")\n",
    "\n",
    "ws = weekly_summary.copy()\n",
    "ws = ws[ws[\"exec_date\"].notna()].copy()\n",
    "ws = ws.sort_values(\"exec_date\")\n",
    "\n",
    "plot_cols = [\n",
    "    \"fees\",\"trades\",\n",
    "    \"sig_slip_net_dollars\",\"sig_slip_gross_cost_dollars\",\"sig_slip_gross_improve_dollars\",\n",
    "    \"ord_slip_net_dollars\",\"ord_slip_gross_cost_dollars\",\"ord_slip_gross_improve_dollars\",\n",
    "]\n",
    "for c in plot_cols:\n",
    "    if c not in ws.columns:\n",
    "        ws[c] = 0.0\n",
    "    ws[c] = pd.to_numeric(ws[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "if ws.empty:\n",
    "    for path, title in [\n",
    "        (fee_chart_path, \"Cumulative Broker Fees (no data)\"),\n",
    "        (slip_chart_path, \"Cumulative Slippage (no data)\"),\n",
    "        (count_chart_path, \"Trades per Execution Day (no data)\"),\n",
    "    ]:\n",
    "        fig = plt.figure(figsize=(10, 3.2))\n",
    "        ax = plt.gca()\n",
    "        ax.text(0.5, 0.5, \"NO DATA TO PLOT\", ha=\"center\", va=\"center\")\n",
    "        ax.set_title(title)\n",
    "        ax.set_axis_off()\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(path, dpi=150)\n",
    "        plt.close(fig)\n",
    "else:\n",
    "    # Fees\n",
    "    fig = plt.figure(figsize=(10, 3.2))\n",
    "    ax = plt.gca()\n",
    "    ax.plot(ws[\"exec_date\"], ws[\"fees\"].cumsum(), marker=\"o\")\n",
    "    ax.set_title(\"Cumulative Broker Fees (Ledger)\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fee_chart_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Slippage\n",
    "    fig = plt.figure(figsize=(10, 3.2))\n",
    "    ax = plt.gca()\n",
    "    ax.plot(ws[\"exec_date\"], ws[\"sig_slip_net_dollars\"].cumsum(), marker=\"o\", label=\"SigFill Net ($)\")\n",
    "    ax.plot(ws[\"exec_date\"], ws[\"ord_slip_net_dollars\"].cumsum(), marker=\"o\", label=\"OrdFill Net ($)\")\n",
    "    ax.plot(ws[\"exec_date\"], ws[\"sig_slip_gross_cost_dollars\"].cumsum(), marker=\"o\", label=\"SigFill Gross Cost ($)\")\n",
    "    ax.plot(ws[\"exec_date\"], ws[\"sig_slip_gross_improve_dollars\"].cumsum(), marker=\"o\", label=\"SigFill Gross Improve ($)\")\n",
    "    ax.plot(ws[\"exec_date\"], ws[\"ord_slip_gross_cost_dollars\"].cumsum(), marker=\"o\", label=\"OrdFill Gross Cost ($)\")\n",
    "    ax.plot(ws[\"exec_date\"], ws[\"ord_slip_gross_improve_dollars\"].cumsum(), marker=\"o\", label=\"OrdFill Gross Improve ($)\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Cumulative Slippage — SigFill vs OrdFill (Net + Gross Cost/Improve)\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(slip_chart_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Trade counts\n",
    "    fig = plt.figure(figsize=(10, 3.2))\n",
    "    ax = plt.gca()\n",
    "    ax.plot(ws[\"exec_date\"], ws[\"trades\"], marker=\"o\")\n",
    "    ax.set_title(\"Trades per Execution Day (Ledger)\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(count_chart_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ============================================================\n",
    "# PDF Styles\n",
    "# ============================================================\n",
    "\n",
    "styles = getSampleStyleSheet()\n",
    "styles.add(ParagraphStyle(name=\"Small\",  fontSize=8, leading=9))\n",
    "styles.add(ParagraphStyle(name=\"Tiny\",   fontSize=7, leading=8))\n",
    "styles.add(ParagraphStyle(name=\"Header\", fontSize=12, leading=14, spaceAfter=6, spaceBefore=6))\n",
    "styles.add(ParagraphStyle(name=\"TitleBig\", fontSize=16, leading=20, spaceAfter=10))\n",
    "\n",
    "# ============================================================\n",
    "# NEW: Build YTD Performance Table (Strategy vs SPY)\n",
    "# ============================================================\n",
    "# ============================================================\n",
    "# NEW: Build YTD + WEEKLY Performance (Strategy vs SPY) + Charts\n",
    "# ============================================================\n",
    "\n",
    "WEEKLY_LOOKBACK_TRADING_DAYS = 5\n",
    "\n",
    "# choose as-of for perf table: must exist in SPY data if possible\n",
    "asof_perf = asof\n",
    "if pd.notna(spy_last_date):\n",
    "    asof_perf = min(pd.Timestamp(asof).normalize(), pd.Timestamp(spy_last_date).normalize())\n",
    "\n",
    "# pick YTD start: max(Jan1, first exec date, first spy date)\n",
    "if pd.notna(asof_perf):\n",
    "    jan1 = pd.Timestamp(asof_perf.year, 1, 1)\n",
    "else:\n",
    "    jan1 = pd.Timestamp.today().normalize().replace(month=1, day=1)\n",
    "\n",
    "first_exec_norm = pd.Timestamp(FIRST_EXEC_DATE).normalize() if pd.notna(FIRST_EXEC_DATE) else jan1\n",
    "first_spy_norm  = pd.Timestamp(spy_close.index.min()).normalize() if not spy_close.empty else jan1\n",
    "ytd_start = max(jan1, first_exec_norm, first_spy_norm)\n",
    "\n",
    "# Build a FULL curve (so YTD/Weekly slices start from correct portfolio state)\n",
    "curve_dates = pd.DatetimeIndex([])\n",
    "strat_eq_full = pd.Series(dtype=float)\n",
    "spy_eq_full = pd.Series(dtype=float)\n",
    "\n",
    "if not spy_close.empty and pd.notna(asof_perf):\n",
    "    curve_start = max(first_exec_norm, first_spy_norm)  # ensures we have SPY dates\n",
    "    curve_dates = spy_close.loc[(spy_close.index >= curve_start) & (spy_close.index <= asof_perf)].index\n",
    "\n",
    "    if len(curve_dates) >= 2:\n",
    "        strat_eq_full = build_strategy_equity_curve(exec_df, px_by_ticker, curve_dates)\n",
    "        spy_eq_full = spy_close.loc[curve_dates].astype(float)\n",
    "\n",
    "# ---- YTD metrics ----\n",
    "strategy_metrics = {\"ret\": np.nan, \"max_dd\": np.nan, \"sharpe\": np.nan, \"sortino\": np.nan, \"calmar\": np.nan}\n",
    "spy_metrics      = {\"ret\": np.nan, \"max_dd\": np.nan, \"sharpe\": np.nan, \"sortino\": np.nan, \"calmar\": np.nan}\n",
    "\n",
    "ytd_dates = pd.DatetimeIndex([])\n",
    "if len(curve_dates) >= 2:\n",
    "    ytd_dates = curve_dates[curve_dates >= ytd_start]\n",
    "    if len(ytd_dates) >= 2:\n",
    "        strategy_metrics = perf_metrics_from_equity(strat_eq_full.loc[ytd_dates])\n",
    "        spy_metrics      = perf_metrics_from_equity(spy_eq_full.loc[ytd_dates])\n",
    "\n",
    "# ---- WEEKLY metrics (last 5 SPY trading sessions ending at asof_perf) ----\n",
    "weekly_metrics_strat = {\"ret\": np.nan, \"max_dd\": np.nan, \"sharpe\": np.nan, \"sortino\": np.nan, \"calmar\": np.nan}\n",
    "weekly_metrics_spy   = {\"ret\": np.nan, \"max_dd\": np.nan, \"sharpe\": np.nan, \"sortino\": np.nan, \"calmar\": np.nan}\n",
    "\n",
    "weekly_dates = pd.DatetimeIndex([])\n",
    "weekly_start = pd.NaT\n",
    "if len(curve_dates) >= 2:\n",
    "    weekly_dates = curve_dates[-WEEKLY_LOOKBACK_TRADING_DAYS:] if len(curve_dates) >= WEEKLY_LOOKBACK_TRADING_DAYS else curve_dates\n",
    "    if len(weekly_dates) >= 2:\n",
    "        weekly_start = pd.Timestamp(weekly_dates.min()).normalize()\n",
    "        weekly_metrics_strat = perf_metrics_from_equity(strat_eq_full.loc[weekly_dates])\n",
    "        weekly_metrics_spy   = perf_metrics_from_equity(spy_eq_full.loc[weekly_dates])\n",
    "\n",
    "# ---- Performance charts (saved alongside your other charts) ----\n",
    "ytd_perf_chart_path    = os.path.join(OUTPUT_DIR, \"perf_ytd_strategy_vs_spy.png\")\n",
    "weekly_perf_chart_path = os.path.join(OUTPUT_DIR, \"perf_weekly_strategy_vs_spy.png\")\n",
    "\n",
    "# always write chart files (even if \"NO DATA\")\n",
    "save_perf_chart(\n",
    "    ytd_perf_chart_path,\n",
    "    ytd_dates,\n",
    "    strat_eq_full if len(curve_dates) else pd.Series(dtype=float),\n",
    "    spy_eq_full if len(curve_dates) else pd.Series(dtype=float),\n",
    "    f\"YTD Strategy vs SPY (Normalized) — {ytd_start.date()} → {pd.Timestamp(asof_perf).date() if pd.notna(asof_perf) else 'N/A'}\"\n",
    ")\n",
    "\n",
    "wk_title_start = pd.Timestamp(weekly_start).date() if pd.notna(weekly_start) else \"N/A\"\n",
    "save_perf_chart(\n",
    "    weekly_perf_chart_path,\n",
    "    weekly_dates,\n",
    "    strat_eq_full if len(curve_dates) else pd.Series(dtype=float),\n",
    "    spy_eq_full if len(curve_dates) else pd.Series(dtype=float),\n",
    "    f\"Weekly Strategy vs SPY (Normalized, last {WEEKLY_LOOKBACK_TRADING_DAYS} trading sessions) — {wk_title_start} → {pd.Timestamp(asof_perf).date() if pd.notna(asof_perf) else 'N/A'}\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Build PDF\n",
    "# ============================================================\n",
    "\n",
    "pdf_name = f\"reconciliation_report_{datetime.now():%Y%m%d-%H%M%S}.pdf\"\n",
    "pdf_path = os.path.join(OUTPUT_DIR, pdf_name)\n",
    "\n",
    "doc = SimpleDocTemplate(\n",
    "    pdf_path,\n",
    "    pagesize=landscape(letter),\n",
    "    rightMargin=36, leftMargin=36,\n",
    "    topMargin=36,  bottomMargin=36,\n",
    ")\n",
    "\n",
    "story = []\n",
    "\n",
    "# ---------- Front Page ----------\n",
    "story.append(Paragraph(\"Reconciliation Report (Executed Ledger + Diagnostics)\", styles[\"TitleBig\"]))\n",
    "story.append(Paragraph(f\"Generated: {datetime.now().isoformat(timespec='seconds')}\", styles[\"Small\"]))\n",
    "story.append(Spacer(1, 0.10 * inch))\n",
    "\n",
    "front_rows = [\n",
    "    [\"Item\", \"Value\"],\n",
    "    [\"Ledger rows (executed_trades.csv)\", f\"{len(exec_df):,}\"],\n",
    "    [\"First exec date\", _fmt(FIRST_EXEC_DATE)],\n",
    "    [\"Last exec date\", _fmt(LAST_EXEC_DATE)],\n",
    "    [\"Unique broker_order_id\", f\"{exec_df['broker_order_id'].nunique(dropna=True):,}\"],\n",
    "    [\"Duplicate broker_order_id (count)\", f\"{int((exec_df['broker_order_id'].duplicated()).sum()):,}\"],\n",
    "    [\"Pending fills (manual not in ledger)\", f\"{len(pending_df):,}\"],\n",
    "    [\"Live cash (snapshot)\", \"N/A\" if np.isnan(live_cash) else f\"{live_cash:,.2f}\"],\n",
    "    [\"Ledger cash (last cash_after)\", \"N/A\" if np.isnan(ledger_cash) else f\"{ledger_cash:,.2f}\"],\n",
    "    [\"Cash drift (live - ledger)\", \"N/A\" if np.isnan(cash_drift) else f\"{cash_drift:,.2f}\"],\n",
    "    [\"Position drift tickers (count)\", f\"{0 if drift_df is None or drift_df.empty else len(drift_df):,}\"],\n",
    "    [\"Price DB as-of date\", \"N/A\" if pd.isna(asof) else str(pd.Timestamp(asof).date())],\n",
    "    [\"Slope-rank source (master_trades.csv)\", \"FOUND\" if os.path.exists(MASTER_TRADES_FILE) else \"MISSING\"],\n",
    "    [\"Rankings source (master_rankings.csv)\", \"FOUND\" if os.path.exists(MASTER_RANKINGS_FILE) else \"MISSING\"],\n",
    "    [\"SPY regime file\", \"FOUND\" if (os.path.exists(SPY_REGIME_FILE_PRIMARY) or os.path.exists(SPY_REGIME_FILE_FALLBACK)) else \"MISSING\"],\n",
    "]\n",
    "tbl = Table(front_rows, hAlign=\"LEFT\")\n",
    "tbl.setStyle(TableStyle([\n",
    "    (\"BACKGROUND\",(0,0),(-1,0),colors.lightgrey),\n",
    "    (\"GRID\",(0,0),(-1,-1),0.25,colors.grey),\n",
    "    (\"FONTNAME\",(0,0),(-1,-1),\"Helvetica\"),\n",
    "    (\"FONTSIZE\",(0,0),(-1,-1),9),\n",
    "    (\"ALIGN\",(1,1),(-1,-1),\"RIGHT\"),\n",
    "]))\n",
    "story.append(tbl)\n",
    "story.append(Spacer(1, 0.12 * inch))\n",
    "\n",
    "# ---------- NEW: Strategy vs SPY Performance Table (like screenshot) ----------\n",
    "# use YTD slippage based on SigFill (your report’s main slippage set)\n",
    "# ---------- NEW: Strategy vs SPY Performance Table (like screenshot) ----------\n",
    "# use YTD slippage based on SigFill (your report’s main slippage set)\n",
    "\n",
    "ytd_weekly_slice = weekly\n",
    "if pd.notna(ytd_start) and pd.notna(asof_perf):\n",
    "    ytd_weekly_slice = weekly[\n",
    "        (weekly[\"exec_date_only\"] >= pd.Timestamp(ytd_start).normalize()) &\n",
    "        (weekly[\"exec_date_only\"] <= pd.Timestamp(asof_perf).normalize())\n",
    "    ]\n",
    "\n",
    "ytd_sig = slippage_breakdown(\n",
    "    ytd_weekly_slice,\n",
    "    slip_col=SIG_SLIP_COL,\n",
    "    notional_col=\"gross_notional\"\n",
    ")\n",
    "\n",
    "\n",
    "perf_table = [\n",
    "    [\"Metric\", \"Strategy\", \"SPY\"],\n",
    "    [\"YTD Return\", _fmt_pct(strategy_metrics[\"ret\"]), _fmt_pct(spy_metrics[\"ret\"])],\n",
    "    [\"YTD Max Drawdown\", _fmt_pct(strategy_metrics[\"max_dd\"]), _fmt_pct(spy_metrics[\"max_dd\"])],\n",
    "    [\"YTD Sharpe\", _fmt_num(strategy_metrics[\"sharpe\"]), _fmt_num(spy_metrics[\"sharpe\"])],\n",
    "    [\"YTD Sortino\", _fmt_num(strategy_metrics[\"sortino\"]), _fmt_num(spy_metrics[\"sortino\"])],\n",
    "    [\"YTD Calmar\", _fmt_num(strategy_metrics[\"calmar\"]), _fmt_num(spy_metrics[\"calmar\"])],\n",
    "    [\"YTD Slippage (net $)\", _fmt(ytd_sig[\"net_dollars\"], \"ytd_sig_slip_net_dollars\"), \"N/A\"],\n",
    "    [\"YTD Slippage (gross cost $)\", _fmt(ytd_sig[\"gross_cost_dollars\"], \"ytd_sig_slip_gross_cost_dollars\"), \"N/A\"],\n",
    "    [\"YTD Slippage (gross improve $)\", _fmt(ytd_sig[\"gross_improve_dollars\"], \"ytd_sig_slip_gross_improve_dollars\"), \"N/A\"],\n",
    "    [\"YTD Slippage (net bps)\", _fmt(ytd_sig[\"net_bps\"], \"ytd_sig_slip_net_bps\"), \"N/A\"],\n",
    "    # keep these duplicate-style rows to match your screenshot feel\n",
    "    [\"YTD Slippage ($, cost)\", _fmt(ytd_sig[\"net_dollars\"], \"ytd_sig_slip_net_dollars\"), \"N/A\"],\n",
    "    [\"YTD Slippage (bps)\", _fmt(ytd_sig[\"net_bps\"], \"ytd_sig_slip_net_bps\"), \"N/A\"],\n",
    "]\n",
    "\n",
    "story.append(Paragraph(f\"YTD Performance (from {ytd_start.date()} to {pd.Timestamp(asof_perf).date() if pd.notna(asof_perf) else 'N/A'})\", styles[\"Header\"]))\n",
    "pt = Table(perf_table, hAlign=\"LEFT\")\n",
    "pt.setStyle(TableStyle([\n",
    "    (\"BACKGROUND\",(0,0),(-1,0),colors.lightgrey),\n",
    "    (\"GRID\",(0,0),(-1,-1),0.25,colors.grey),\n",
    "    (\"FONTNAME\",(0,0),(-1,-1),\"Helvetica\"),\n",
    "    (\"FONTSIZE\",(0,0),(-1,-1),9),\n",
    "    (\"ALIGN\",(1,1),(-1,-1),\"RIGHT\"),\n",
    "]))\n",
    "story.append(pt)\n",
    "story.append(Spacer(1, 0.15 * inch))\n",
    "\n",
    "# ---------- NEW: Weekly Performance Table (Strategy vs SPY) ----------\n",
    "weekly_sig = slippage_breakdown(\n",
    "    weekly[(weekly[\"exec_date_only\"] >= weekly_start) & (weekly[\"exec_date_only\"] <= asof_perf)]\n",
    "    if (pd.notna(weekly_start) and pd.notna(asof_perf)) else weekly,\n",
    "    slip_col=SIG_SLIP_COL,\n",
    "    notional_col=\"gross_notional\"\n",
    ")\n",
    "\n",
    "weekly_perf_table = [\n",
    "    [\"Metric\", \"Strategy\", \"SPY\"],\n",
    "    [\"Weekly Return\", _fmt_pct(weekly_metrics_strat[\"ret\"]), _fmt_pct(weekly_metrics_spy[\"ret\"])],\n",
    "    [\"Weekly Max Drawdown\", _fmt_pct(weekly_metrics_strat[\"max_dd\"]), _fmt_pct(weekly_metrics_spy[\"max_dd\"])],\n",
    "    [\"Weekly Sharpe\", _fmt_num(weekly_metrics_strat[\"sharpe\"]), _fmt_num(weekly_metrics_spy[\"sharpe\"])],\n",
    "    [\"Weekly Sortino\", _fmt_num(weekly_metrics_strat[\"sortino\"]), _fmt_num(weekly_metrics_spy[\"sortino\"])],\n",
    "    [\"Weekly Calmar\", _fmt_num(weekly_metrics_strat[\"calmar\"]), _fmt_num(weekly_metrics_spy[\"calmar\"])],\n",
    "    [\"Weekly Slippage (net $)\", _fmt(weekly_sig[\"net_dollars\"], \"sig_slip_net_dollars\"), \"N/A\"],\n",
    "    [\"Weekly Slippage (gross cost $)\", _fmt(weekly_sig[\"gross_cost_dollars\"], \"sig_slip_gross_cost_dollars\"), \"N/A\"],\n",
    "    [\"Weekly Slippage (gross improve $)\", _fmt(weekly_sig[\"gross_improve_dollars\"], \"sig_slip_gross_improve_dollars\"), \"N/A\"],\n",
    "    [\"Weekly Slippage (net bps)\", _fmt(weekly_sig[\"net_bps\"], \"sig_slip_net_bps\"), \"N/A\"],\n",
    "]\n",
    "\n",
    "wk_end = pd.Timestamp(asof_perf).date() if pd.notna(asof_perf) else \"N/A\"\n",
    "wk_start = pd.Timestamp(weekly_start).date() if pd.notna(weekly_start) else \"N/A\"\n",
    "story.append(Paragraph(f\"Weekly Performance (from {wk_start} to {wk_end})\", styles[\"Header\"]))\n",
    "\n",
    "wpt = Table(weekly_perf_table, hAlign=\"LEFT\")\n",
    "wpt.setStyle(TableStyle([\n",
    "    (\"BACKGROUND\",(0,0),(-1,0),colors.lightgrey),\n",
    "    (\"GRID\",(0,0),(-1,-1),0.25,colors.grey),\n",
    "    (\"FONTNAME\",(0,0),(-1,-1),\"Helvetica\"),\n",
    "    (\"FONTSIZE\",(0,0),(-1,-1),9),\n",
    "    (\"ALIGN\",(1,1),(-1,-1),\"RIGHT\"),\n",
    "]))\n",
    "story.append(wpt)\n",
    "story.append(Spacer(1, 0.15 * inch))\n",
    "\n",
    "\n",
    "# keep existing charts page (unchanged)\n",
    "story.append(Paragraph(\"Overview Charts\", styles[\"Header\"]))\n",
    "\n",
    "# NEW: performance charts (YTD + Weekly) above trades\n",
    "story.append(Image(ytd_perf_chart_path, width=9.0 * inch, height=2.6 * inch))\n",
    "story.append(Spacer(1, 0.10 * inch))\n",
    "story.append(Image(weekly_perf_chart_path, width=9.0 * inch, height=2.6 * inch))\n",
    "story.append(Spacer(1, 0.10 * inch))\n",
    "\n",
    "# existing charts (unchanged)\n",
    "story.append(Image(fee_chart_path, width=9.0 * inch, height=2.6 * inch))\n",
    "story.append(Spacer(1, 0.10 * inch))\n",
    "story.append(Image(slip_chart_path, width=9.0 * inch, height=2.6 * inch))\n",
    "story.append(Spacer(1, 0.10 * inch))\n",
    "story.append(Image(count_chart_path, width=9.0 * inch, height=2.6 * inch))\n",
    "story.append(PageBreak())\n",
    "\n",
    "\n",
    "\n",
    "story.append(Spacer(1, 0.10 * inch))\n",
    "story.append(Image(slip_chart_path, width=9.0 * inch, height=2.6 * inch))\n",
    "story.append(Spacer(1, 0.10 * inch))\n",
    "story.append(Image(count_chart_path, width=9.0 * inch, height=2.6 * inch))\n",
    "story.append(PageBreak())\n",
    "\n",
    "# ---------- Global Diagnostics ----------\n",
    "story.append(Paragraph(\"Global Diagnostics\", styles[\"Header\"]))\n",
    "\n",
    "make_table(\n",
    "    story,\n",
    "    \"Duplicate broker_order_id rows (should be NONE)\",\n",
    "    dup_df,\n",
    "    cols=[\"exec_ts\",\"exec_date\",\"ticker\",\"side\",\"shares\",\"signal_price\",\"order_price\",\"fill_price\",\"broker_fee\", SIG_SLIP_COL, ORD_SLIP_COL, \"broker_order_id\"],\n",
    "    font_size=7\n",
    ")\n",
    "\n",
    "make_table(\n",
    "    story,\n",
    "    \"Pending broker fills (in broker_fills_manual.csv but NOT in executed_trades.csv)\",\n",
    "    pending_df,\n",
    "    cols=[\"plan_id\",\"signal_date\",\"exec_date\",\"ticker\",\"side\",\"shares_filled\",\"order_type\",\"order_price\",\"fill_price\",\"broker_fee\",\"broker_order_id\"],\n",
    "    font_size=7\n",
    ")\n",
    "\n",
    "make_table(\n",
    "    story,\n",
    "    \"Portfolio Drift: live_portfolio.csv vs positions reconstructed from executed_trades.csv\",\n",
    "    drift_df,\n",
    "    cols=[\"ticker\",\"ledger_shares\",\"live_shares\",\"diff_shares\",\"weight_pct\"],\n",
    "    font_size=8\n",
    ")\n",
    "\n",
    "story.append(PageBreak())\n",
    "\n",
    "# ---------- Weekly Loop ----------\n",
    "for _, wk in weekly_summary.iterrows():\n",
    "    exec_dt = pd.Timestamp(wk[\"exec_date\"]).normalize()\n",
    "    \n",
    "    if exec_dt.year < MIN_YEAR_FOR_REPORT or exec_dt.year > MAX_YEAR_FOR_REPORT:\n",
    "        continue\n",
    "\n",
    "    day = weekly[weekly[\"exec_date_only\"] == exec_dt].copy()\n",
    "    # Keep audited columns, but rename so tables don't imply they're sequential after sorting\n",
    "    day = day.rename(columns={\n",
    "        \"cash_before\": \"cash_before_audited\",\n",
    "        \"cash_after\": \"cash_after_audited\",\n",
    "    })\n",
    "\n",
    "    # NEW: Get rankings for this week\n",
    "    # signal_date is typically the day before exec_date\n",
    "    signal_dt = None\n",
    "    if not day.empty and \"signal_date\" in day.columns and day[\"signal_date\"].notna().any():\n",
    "        signal_dt = pd.Timestamp(day[\"signal_date\"].dropna().iloc[0]).normalize()\n",
    "    \n",
    "    day_rankings = pd.DataFrame()\n",
    "    if signal_dt is not None and signal_dt in rankings_by_signal_date:\n",
    "        day_rankings = rankings_by_signal_date[signal_dt].copy()\n",
    "\n",
    "    ytd = ytd_slice(exec_dt)\n",
    "\n",
    "    day_sig = slippage_breakdown(day, slip_col=SIG_SLIP_COL, notional_col=\"gross_notional\")\n",
    "    ytd_sig_day = slippage_breakdown(ytd, slip_col=SIG_SLIP_COL, notional_col=\"gross_notional\")\n",
    "\n",
    "    day_ord = slippage_breakdown(day, slip_col=ORD_SLIP_COL, notional_col=\"gross_notional\")\n",
    "    ytd_ord = slippage_breakdown(ytd, slip_col=ORD_SLIP_COL, notional_col=\"gross_notional\")\n",
    "\n",
    "    ytd_trades = len(ytd)\n",
    "    day_trades = int(wk[\"trades\"]) if pd.notna(wk.get(\"trades\")) else len(day)\n",
    "\n",
    "    day_sig_per_trade = (day_sig[\"net_dollars\"] / day_trades) if day_trades > 0 and pd.notna(day_sig[\"net_dollars\"]) else np.nan\n",
    "    ytd_sig_per_trade = (ytd_sig_day[\"net_dollars\"] / ytd_trades) if ytd_trades > 0 and pd.notna(ytd_sig_day[\"net_dollars\"]) else np.nan\n",
    "\n",
    "    day_ord_per_trade = (day_ord[\"net_dollars\"] / day_trades) if day_trades > 0 and pd.notna(day_ord[\"net_dollars\"]) else np.nan\n",
    "    ytd_ord_per_trade = (ytd_ord[\"net_dollars\"] / ytd_trades) if ytd_trades > 0 and pd.notna(ytd_ord[\"net_dollars\"]) else np.nan\n",
    "\n",
    "    ytd_fees = float(ytd[\"broker_fee\"].sum(skipna=True)) if \"broker_fee\" in ytd.columns else np.nan\n",
    "\n",
    "    # -----------------------------\n",
    "    # NEW: AUDITED CASH (EXECUTION DAY)\n",
    "    # -----------------------------\n",
    "    day_sorted_for_cash = day.copy()\n",
    "\n",
    "    # Use ledger order first (best). Fallback to timestamp+numeric id if needed.\n",
    "    if \"_ledger_seq\" in day_sorted_for_cash.columns:\n",
    "        day_sorted_for_cash = day_sorted_for_cash.sort_values(\"_ledger_seq\", kind=\"mergesort\")\n",
    "    else:\n",
    "        day_sorted_for_cash[\"_boid_num\"] = pd.to_numeric(day_sorted_for_cash.get(\"broker_order_id\"), errors=\"coerce\")\n",
    "        if \"exec_ts\" in day_sorted_for_cash.columns and day_sorted_for_cash[\"exec_ts\"].notna().any():\n",
    "            day_sorted_for_cash = day_sorted_for_cash.sort_values([\"exec_ts\", \"_boid_num\", \"broker_order_id\"], kind=\"mergesort\")\n",
    "        else:\n",
    "            day_sorted_for_cash = day_sorted_for_cash.sort_values([\"exec_date\", \"_boid_num\", \"broker_order_id\"], kind=\"mergesort\")\n",
    "        day_sorted_for_cash = day_sorted_for_cash.drop(columns=[\"_boid_num\"])\n",
    "\n",
    "\n",
    "    cash_before_first = np.nan\n",
    "    if (not day_sorted_for_cash.empty) and (\"cash_before_audited\" in day_sorted_for_cash.columns) and day_sorted_for_cash[\"cash_before_audited\"].notna().any():\n",
    "        cash_before_first = float(pd.to_numeric(day_sorted_for_cash[\"cash_before_audited\"], errors=\"coerce\").dropna().iloc[0])\n",
    "\n",
    "    cash_after_last = np.nan\n",
    "    if (not day_sorted_for_cash.empty) and (\"cash_after_audited\" in day_sorted_for_cash.columns) and day_sorted_for_cash[\"cash_after_audited\"].notna().any():\n",
    "        cash_after_last = float(pd.to_numeric(day_sorted_for_cash[\"cash_after_audited\"], errors=\"coerce\").dropna().iloc[-1])\n",
    "\n",
    "    cash_change = (cash_after_last - cash_before_first) if (pd.notna(cash_after_last) and pd.notna(cash_before_first)) else np.nan\n",
    "\n",
    "    # -----------------------------\n",
    "    # NEW: PORTFOLIO (AFTER TRADES, END OF DAY)\n",
    "    # -----------------------------\n",
    "    # -----------------------------\n",
    "    # PORTFOLIO VALUATION LOGIC (TWO SNAPSHOTS)\n",
    "    #   (1) Pre-trade valuation = prior trading day close\n",
    "    #   (2) Post-trade valuation = execution day close (or best available if prices not present)\n",
    "    # -----------------------------\n",
    "\n",
    "    # ---- (1) PRE-TRADE snapshot date = prior trading day in state_curves ----\n",
    "    prev_mkt_dt = pd.NaT\n",
    "    if state_curves is not None and (not state_curves.empty):\n",
    "        # previous trading day available in the curve (handles weekends/holidays)\n",
    "        prev_candidates = state_curves.index[state_curves.index < exec_dt]\n",
    "        if len(prev_candidates) > 0:\n",
    "            prev_mkt_dt = pd.Timestamp(prev_candidates.max()).normalize()\n",
    "\n",
    "    # defaults\n",
    "    pre_portfolio_value = np.nan\n",
    "    pre_securities_value = np.nan\n",
    "    pre_cash_value = np.nan\n",
    "\n",
    "    if pd.notna(prev_mkt_dt) and (state_curves is not None) and (not state_curves.empty) and (prev_mkt_dt in state_curves.index):\n",
    "        pre_cash_value = float(state_curves.loc[prev_mkt_dt, \"cash\"])\n",
    "        pre_securities_value = float(state_curves.loc[prev_mkt_dt, \"securities_value\"])\n",
    "        if pd.notna(pre_cash_value) and pd.notna(pre_securities_value):\n",
    "            pre_portfolio_value = pre_cash_value + pre_securities_value\n",
    "        else:\n",
    "            pre_portfolio_value = float(state_curves.loc[prev_mkt_dt, \"total_value\"])\n",
    "\n",
    "    # ---- (2) POST-TRADE (end-of-day) snapshot = exec_dt close ----\n",
    "    post_portfolio_value = np.nan\n",
    "    post_securities_value = np.nan\n",
    "\n",
    "    # prefer audited cash from the ledger day block\n",
    "    post_cash_value = cash_after_last\n",
    "\n",
    "    if (state_curves is not None) and (not state_curves.empty) and (exec_dt in state_curves.index):\n",
    "        post_securities_value = float(state_curves.loc[exec_dt, \"securities_value\"])\n",
    "\n",
    "        # if audited cash is missing, fall back to curve cash\n",
    "        if pd.isna(post_cash_value):\n",
    "            post_cash_value = float(state_curves.loc[exec_dt, \"cash\"])\n",
    "\n",
    "        # total should match the cash we actually display (audited preferred)\n",
    "        if pd.notna(post_cash_value) and pd.notna(post_securities_value):\n",
    "            post_portfolio_value = post_cash_value + post_securities_value\n",
    "        else:\n",
    "            post_portfolio_value = float(state_curves.loc[exec_dt, \"total_value\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # WEEKLY PORTFOLIO RETURN (prev execution day close -> this execution day close)\n",
    "    # -----------------------------\n",
    "    prev_dt = prev_exec_map.get(exec_dt, pd.NaT)\n",
    "    weekly_portfolio_return = np.nan\n",
    "\n",
    "    if pd.notna(prev_dt) and (state_curves is not None) and (not state_curves.empty) and (prev_dt in state_curves.index):\n",
    "        prev_total = float(state_curves.loc[prev_dt, \"total_value\"])\n",
    "        cur_total = post_portfolio_value\n",
    "\n",
    "        if pd.notna(prev_total) and prev_total > 0 and pd.notna(cur_total):\n",
    "            weekly_portfolio_return = (cur_total / prev_total - 1.0)\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # Render tables + summary\n",
    "    # -----------------------------\n",
    "    story.append(Paragraph(f\"Execution Day: {exec_dt.date()} — Reconciliation Summary\", styles[\"Header\"]))\n",
    "    story.append(Spacer(1, 0.08 * inch))\n",
    "\n",
    "    audited_cash_rows = [\n",
    "        [\"Item\", \"Amount\"],\n",
    "        [\"Cash BEFORE first trade\", _fmt(cash_before_first, \"cash\")],\n",
    "        [\"Cash AFTER last trade\", _fmt(cash_after_last, \"cash\")],\n",
    "        [\"Cash Δ (After - Before)\", _fmt(cash_change, \"cash\")],\n",
    "    ]\n",
    "    t_cash = Table(audited_cash_rows, hAlign=\"LEFT\")\n",
    "    t_cash.setStyle(TableStyle([\n",
    "        (\"BACKGROUND\",(0,0),(-1,0),colors.lightgrey),\n",
    "        (\"GRID\",(0,0),(-1,-1),0.25,colors.grey),\n",
    "        (\"FONTNAME\",(0,0),(-1,-1),\"Helvetica\"),\n",
    "        (\"FONTSIZE\",(0,0),(-1,-1),9),\n",
    "        (\"ALIGN\",(1,1),(-1,-1),\"RIGHT\"),\n",
    "    ]))\n",
    "    story.append(t_cash)\n",
    "    story.append(Spacer(1, 0.10 * inch))\n",
    "\n",
    "    portfolio_rows = [\n",
    "        [\"Item\", \"Amount\"],\n",
    "        [\"Portfolio Value @ prior close (pre-trade)\", _fmt(pre_portfolio_value, \"market_value\")],\n",
    "        [\"Securities Value @ prior close\", _fmt(pre_securities_value, \"market_value\")],\n",
    "        [\"Cash Value @ prior close\", _fmt(pre_cash_value, \"cash\")],\n",
    "        [\"\", \"\"],\n",
    "        [\"Portfolio Value @ execution-day close (post-trade)\", _fmt(post_portfolio_value, \"market_value\")],\n",
    "        [\"Securities Value @ execution-day close\", _fmt(post_securities_value, \"market_value\")],\n",
    "        [\"Cash Value @ execution-day close (audited)\", _fmt(post_cash_value, \"cash\")],\n",
    "        [\"Weekly Portfolio Return\", _fmt_pct(weekly_portfolio_return)],\n",
    "    ]\n",
    "\n",
    "\n",
    "    t_port = Table(portfolio_rows, hAlign=\"LEFT\")\n",
    "    t_port.setStyle(TableStyle([\n",
    "        (\"BACKGROUND\",(0,0),(-1,0),colors.lightgrey),\n",
    "        (\"GRID\",(0,0),(-1,-1),0.25,colors.grey),\n",
    "        (\"FONTNAME\",(0,0),(-1,-1),\"Helvetica\"),\n",
    "        (\"FONTSIZE\",(0,0),(-1,-1),9),\n",
    "        (\"ALIGN\",(1,1),(-1,-1),\"RIGHT\"),\n",
    "    ]))\n",
    "    story.append(t_port)\n",
    "    story.append(Spacer(1, 0.15 * inch))\n",
    "\n",
    "    summary = [\n",
    "        [\"Item\", \"Value\"],\n",
    "        [\"Trades (count)\", _fmt(wk[\"trades\"])],\n",
    "        [\"Buys / Sells\", f\"{int(wk['buys'])} / {int(wk['sells'])}\"],\n",
    "        [\"Gross notional ($)\", _fmt(wk[\"gross_notional\"], \"gross_notional\")],\n",
    "        [\"Broker fees ($)\", _fmt(wk[\"fees\"], \"fees\")],\n",
    "\n",
    "        [\"\", \"\"],\n",
    "        [f\"--- SLIPPAGE — {SIG_LABEL} — THIS EXECUTION DAY ---\", \"\"],\n",
    "        [\"SigFill slip (net $)\", _fmt(day_sig[\"net_dollars\"], \"sig_slip_net_dollars\")],\n",
    "        [\"SigFill slip (gross cost $)\", _fmt(day_sig[\"gross_cost_dollars\"], \"sig_slip_gross_cost_dollars\")],\n",
    "        [\"SigFill slip (gross improve $)\", _fmt(day_sig[\"gross_improve_dollars\"], \"sig_slip_gross_improve_dollars\")],\n",
    "        [\"SigFill slip (net bps)\", _fmt(day_sig[\"net_bps\"], \"sig_slip_net_bps\")],\n",
    "        [\"SigFill slip per trade ($)\", _fmt(day_sig_per_trade, \"sig_slip_per_trade_dollars\")],\n",
    "\n",
    "        [\"\", \"\"],\n",
    "        [f\"--- SLIPPAGE — {ORD_LABEL} — THIS EXECUTION DAY ---\", \"\"],\n",
    "        [\"OrdFill slip (net $)\", _fmt(day_ord[\"net_dollars\"], \"ord_slip_net_dollars\")],\n",
    "        [\"OrdFill slip (gross cost $)\", _fmt(day_ord[\"gross_cost_dollars\"], \"ord_slip_gross_cost_dollars\")],\n",
    "        [\"OrdFill slip (gross improve $)\", _fmt(day_ord[\"gross_improve_dollars\"], \"ord_slip_gross_improve_dollars\")],\n",
    "        [\"OrdFill slip (net bps)\", _fmt(day_ord[\"net_bps\"], \"ord_slip_net_bps\")],\n",
    "        [\"OrdFill slip per trade ($)\", _fmt(day_ord_per_trade, \"ord_slip_per_trade_dollars\")],\n",
    "\n",
    "        [\"Missing signal_price rows\", str(int(wk.get(\"missing_signal_price\", 0)))],\n",
    "        [\"Missing order_price rows\", str(int(wk.get(\"missing_order_price\", 0)))],\n",
    "\n",
    "        [\"\", \"\"],\n",
    "        [\"--- YTD (through this execution day) ---\", \"\"],\n",
    "        [\"YTD trades\", f\"{ytd_trades:,}\"],\n",
    "        [\"YTD fees ($)\", \"N/A\" if np.isnan(ytd_fees) else f\"{ytd_fees:,.2f}\"],\n",
    "\n",
    "        [\"YTD SigFill slip (net $)\", _fmt(ytd_sig_day[\"net_dollars\"], \"ytd_sig_slip_net_dollars\")],\n",
    "        [\"YTD SigFill slip (gross cost $)\", _fmt(ytd_sig_day[\"gross_cost_dollars\"], \"ytd_sig_slip_gross_cost_dollars\")],\n",
    "        [\"YTD SigFill slip (gross improve $)\", _fmt(ytd_sig_day[\"gross_improve_dollars\"], \"ytd_sig_slip_gross_improve_dollars\")],\n",
    "        [\"YTD SigFill slip (net bps)\", _fmt(ytd_sig_day[\"net_bps\"], \"ytd_sig_slip_net_bps\")],\n",
    "        [\"YTD SigFill slip per trade ($)\", _fmt(ytd_sig_per_trade, \"ytd_sig_slip_per_trade_dollars\")],\n",
    "\n",
    "        [\"\", \"\"],\n",
    "        [\"YTD OrdFill slip (net $)\", _fmt(ytd_ord[\"net_dollars\"], \"ytd_ord_slip_net_dollars\")],\n",
    "        [\"YTD OrdFill slip (gross cost $)\", _fmt(ytd_ord[\"gross_cost_dollars\"], \"ytd_ord_slip_gross_cost_dollars\")],\n",
    "        [\"YTD OrdFill slip (gross improve $)\", _fmt(ytd_ord[\"gross_improve_dollars\"], \"ytd_ord_slip_gross_improve_dollars\")],\n",
    "        [\"YTD OrdFill slip (net bps)\", _fmt(ytd_ord[\"net_bps\"], \"ytd_ord_slip_net_bps\")],\n",
    "        [\"YTD OrdFill slip per trade ($)\", _fmt(ytd_ord_per_trade, \"ytd_ord_slip_per_trade_dollars\")],\n",
    "    ]\n",
    "\n",
    "    # ============================================================\n",
    "    # STEP 6: Add ranked stocks table display (around line 1900)\n",
    "    # ============================================================\n",
    "    # ADD this section AFTER the summary table and BEFORE the trades tables:\n",
    "\n",
    "    # -----------------------------\n",
    "    # NEW: RANKED STOCKS TABLE (BEFORE FILTERS)\n",
    "    # -----------------------------\n",
    "    if not day_rankings.empty:\n",
    "        story.append(Paragraph(\"All Ranked Stocks (Top Percentile) — Pre-Filter\", styles[\"Header\"]))\n",
    "        \n",
    "        # Sort by slope rank\n",
    "        day_rankings_display = day_rankings.sort_values(\"slope_rank\", na_position=\"last\")\n",
    "        \n",
    "        # Select columns to display\n",
    "        rank_cols = [\n",
    "            \"slope_rank\",\n",
    "            \"ticker\",\n",
    "            \"traded_flag\",\n",
    "            \"no_trade_reason\",\n",
    "            \"slope_adj\",\n",
    "            \"target_weight\",\n",
    "            \"current_weight\",\n",
    "            \"target_shares\",\n",
    "            \"current_shares\",\n",
    "            \"close_adj\",\n",
    "        ]\n",
    "        rank_cols = [c for c in rank_cols if c in day_rankings_display.columns]\n",
    "        \n",
    "        make_table(\n",
    "            story,\n",
    "            \"Ranked Stocks (shows which traded and reasons for not trading)\",\n",
    "            day_rankings_display,\n",
    "            cols=rank_cols,\n",
    "            font_size=6\n",
    "        )\n",
    "    tbl = Table(summary, hAlign=\"LEFT\")\n",
    "    tbl.setStyle(TableStyle([\n",
    "        (\"BACKGROUND\",(0,0),(-1,0),colors.lightgrey),\n",
    "        (\"GRID\",(0,0),(-1,-1),0.25,colors.grey),\n",
    "        (\"FONTNAME\",(0,0),(-1,-1),\"Helvetica\"),\n",
    "        (\"FONTSIZE\",(0,0),(-1,-1),9),\n",
    "        (\"ALIGN\",(1,1),(-1,-1),\"RIGHT\"),\n",
    "    ]))\n",
    "    story.append(tbl)\n",
    "    story.append(Spacer(1, 0.15 * inch))\n",
    "\n",
    "    # Sort trades for the table: BUYS then SELLS, each by best slope rank -> worst\n",
    "    # ------------------------------------------------------------\n",
    "    # Executed Trades tables: SELLS first, then BUYS (each w/ totals)\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    # Identify rank column (same logic as sort_trades_by_rank, but we split tables)\n",
    "    # ------------------------------------------------------------\n",
    "    # Executed Trades tables: SELLS first, then BUYS (each w/ totals)\n",
    "    # FIX: cash_before/cash_after shown in the tables must be sequential\n",
    "    #      in DISPLAY ORDER (SELLS table order, then BUYS table order).\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    rank_col_used = next((c for c in RANK_COL_CANDIDATES if c in day.columns), None)\n",
    "    sum_cols = [\"gross_notional\", \"broker_fee\", SIG_SLIP_COL, ORD_SLIP_COL, \"net_cash_impact\"]\n",
    "\n",
    "    # We will show recomputed sequential cash in these SAME column names:\n",
    "    #   cash_before, cash_after\n",
    "    # The audited originals remain available as:\n",
    "    #   cash_before_audited, cash_after_audited\n",
    "    base_cols = [\n",
    "        \"signal_date\",\"exec_date\",\"ticker\",\"side\",\"shares\",\n",
    "        \"signal_price\",\"order_price\",\"fill_price\",\n",
    "        \"gross_notional\",\"broker_fee\",\n",
    "        rank_col_used,\n",
    "        SIG_SLIP_COL, ORD_SLIP_COL,\n",
    "        \"net_cash_impact\",\n",
    "        \"cash_before\",\"cash_after\",            # <-- sequential in display order\n",
    "        \"pos_before\",\"pos_after\",\n",
    "        \"broker_order_id\"\n",
    "    ]\n",
    "    base_cols = [c for c in base_cols if c]  # drop None if no rank col\n",
    "\n",
    "    # starting cash for DISPLAY sequencing = audited cash BEFORE first trade (ledger order)\n",
    "    start_cash_display = float(cash_before_first) if pd.notna(cash_before_first) else 0.0\n",
    "\n",
    "    # ---- SELLS table (first) ----\n",
    "    day_sells = day[_norm_side(day[\"side\"]) == \"SELL\"].copy()\n",
    "    if rank_col_used is not None and rank_col_used in day_sells.columns:\n",
    "        day_sells[rank_col_used] = pd.to_numeric(day_sells[rank_col_used], errors=\"coerce\")\n",
    "        day_sells = day_sells.sort_values(rank_col_used, ascending=True, na_position=\"last\", kind=\"mergesort\")\n",
    "\n",
    "    day_sells_disp, end_cash_after_sells = add_sequential_display_cash(day_sells, start_cash_display)\n",
    "\n",
    "    # rename display cash into the standard names the report already uses\n",
    "    if day_sells_disp is not None and not day_sells_disp.empty:\n",
    "        day_sells_disp = day_sells_disp.rename(columns={\n",
    "            \"cash_before_display\": \"cash_before\",\n",
    "            \"cash_after_display\": \"cash_after\",\n",
    "            \"cash_delta_display\": \"cash_delta\",\n",
    "        })\n",
    "\n",
    "    day_sells_tbl = append_totals_row(day_sells_disp, sum_cols=sum_cols, label_col=\"ticker\", label=\"TOTAL\")\n",
    "\n",
    "    # put start/end cash on totals row (so totals row is meaningful, not summed)\n",
    "    if day_sells_tbl is not None and not day_sells_tbl.empty:\n",
    "        last_idx = day_sells_tbl.index[-1]\n",
    "        if \"cash_before\" in day_sells_tbl.columns:\n",
    "            day_sells_tbl.loc[last_idx, \"cash_before\"] = start_cash_display\n",
    "        if \"cash_after\" in day_sells_tbl.columns:\n",
    "            day_sells_tbl.loc[last_idx, \"cash_after\"] = end_cash_after_sells\n",
    "\n",
    "    sell_cols = [c for c in base_cols if c in day_sells_tbl.columns]\n",
    "\n",
    "    make_table(\n",
    "        story,\n",
    "        \"Executed SELLS (Sorted by slope rank best→worst) — cash sequential in display order\",\n",
    "        day_sells_tbl,\n",
    "        cols=sell_cols,\n",
    "        font_size=6.5,\n",
    "        bold_last_row=True\n",
    "    )\n",
    "\n",
    "    # ---- BUYS table (second) ----\n",
    "    day_buys = day[_norm_side(day[\"side\"]) == \"BUY\"].copy()\n",
    "    if rank_col_used is not None and rank_col_used in day_buys.columns:\n",
    "        day_buys[rank_col_used] = pd.to_numeric(day_buys[rank_col_used], errors=\"coerce\")\n",
    "        day_buys = day_buys.sort_values(rank_col_used, ascending=True, na_position=\"last\", kind=\"mergesort\")\n",
    "\n",
    "    # BUYS start where SELLS ended (so cash is sequential across both tables)\n",
    "    day_buys_disp, end_cash_after_buys = add_sequential_display_cash(day_buys, end_cash_after_sells)\n",
    "\n",
    "    if day_buys_disp is not None and not day_buys_disp.empty:\n",
    "        day_buys_disp = day_buys_disp.rename(columns={\n",
    "            \"cash_before_display\": \"cash_before\",\n",
    "            \"cash_after_display\": \"cash_after\",\n",
    "            \"cash_delta_display\": \"cash_delta\",\n",
    "        })\n",
    "\n",
    "    day_buys_tbl = append_totals_row(day_buys_disp, sum_cols=sum_cols, label_col=\"ticker\", label=\"TOTAL\")\n",
    "\n",
    "    if day_buys_tbl is not None and not day_buys_tbl.empty:\n",
    "        last_idx = day_buys_tbl.index[-1]\n",
    "        if \"cash_before\" in day_buys_tbl.columns:\n",
    "            day_buys_tbl.loc[last_idx, \"cash_before\"] = end_cash_after_sells\n",
    "        if \"cash_after\" in day_buys_tbl.columns:\n",
    "            day_buys_tbl.loc[last_idx, \"cash_after\"] = end_cash_after_buys\n",
    "\n",
    "    buy_cols = [c for c in base_cols if c in day_buys_tbl.columns]\n",
    "\n",
    "    make_table(\n",
    "        story,\n",
    "        \"Executed BUYS (Sorted by slope rank best→worst) — cash sequential in display order\",\n",
    "        day_buys_tbl,\n",
    "        cols=buy_cols,\n",
    "        font_size=6.5,\n",
    "        bold_last_row=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # --- Portfolio Holdings as-of this execution day (NOT global asof) ---\n",
    "    exec_slice = exec_df[exec_df[\"exec_date\"].dt.normalize() <= exec_dt].copy()\n",
    "    port_close_df = build_portfolio_snapshot_from_ledger(exec_slice, px_by_ticker, exec_dt)\n",
    "\n",
    "    story.append(Paragraph(\"Portfolio Holdings (as of execution-day close)\", styles[\"Header\"]))\n",
    "    story.append(Paragraph(\n",
    "        \"Pricing rule: use the last available close on or before the execution day. \"\n",
    "        \"See price_date_used per ticker (will never show a future date like Dec 24 for Dec 18).\",\n",
    "        styles[\"Small\"]\n",
    "    ))\n",
    "\n",
    "    make_table(\n",
    "        story,\n",
    "        \"Holdings at Close\",\n",
    "        port_close_df,\n",
    "        cols=[\"ticker\",\"shares\",\"avg_cost\",\"current_price\",\"price_date_used\",\"position_return_pct\",\"first_acquired\",\"market_value\",\"weight_pct\"],\n",
    "        font_size=8\n",
    "    )\n",
    "\n",
    "    story.append(PageBreak())\n",
    "\n",
    "\n",
    "doc.build(story)\n",
    "\n",
    "print(\"=== COMPLETE ===\")\n",
    "print(f\"Reconciliation report saved → {pdf_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
