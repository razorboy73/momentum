{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8397708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SYSTEM SUMMARY (top rows) ===\n",
      "              system  n_days  mean_daily_ret  vol_daily   sharpe     maxdd      skew  excess_kurt  tail_ratio_5pct  beta_to_spy  gamma_to_spy2   ols_r2  mean_ret_high_abs_spy  mean_ret_other_days  delta_high_move  corr_with_spy  mean_ret_on_worst_spy_1pct_days  mean_ret_on_worst_spy_5pct_days  sharpe_bull  maxdd_bull  corr_with_spy_bull  gamma_bull  beta_bull  n_days_bull  sharpe_bear  maxdd_bear  corr_with_spy_bear  gamma_bear  beta_bear  n_days_bear\n",
      "regression_spyfilter    6786        0.000636   0.008253 1.223018 -0.207327 -0.217674     6.250288         1.047984     0.341807      -1.243601 0.259336              -0.001492             0.000872        -0.002364       0.502466                        -0.012307                        -0.008831     1.863452   -0.151862            0.650149   -1.416947   0.669641         5053    -1.252573   -0.609674            0.439103   -0.963583   0.150477         1733\n",
      "\n",
      "Wrote: ./25c-convexity_tests_output\\25c-system_summary_convexity_skew.csv\n",
      "Wrote: ./25c-convexity_tests_output\\25c-convexity_tables_by_abs_spy_decile.csv\n",
      "\n",
      "=== DONE ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farty\\AppData\\Local\\Temp\\ipykernel_31816\\977565485.py:190: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df.groupby(\"abs_spy_bucket\", as_index=False)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Convexity / Positive Skew / Correlation Test Harness\n",
    "\n",
    "What this does:\n",
    "1) Loads one or more strategy equity curves -> daily returns\n",
    "2) Loads SPY regime file -> daily SPY returns + regime (0/1)\n",
    "3) Tests \"convexity\" via conditional returns vs |SPY move| buckets (and downside-only buckets)\n",
    "4) Computes skewness / tail metrics\n",
    "5) Computes correlation between systems (overall + by regime + on high-|SPY| days)\n",
    "6) Writes CSV outputs to OUT_DIR\n",
    "\n",
    "Run:\n",
    "  python 17-convexity_correlation_tests.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "OUT_DIR = \"./25c-convexity_tests_output\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Your files (add more systems by adding more entries) ---\n",
    "SYSTEMS = {\n",
    "    \"regression_spyfilter\": {\n",
    "        \"equity_file\": \"./13-trading_output_regression_insp500_spyfilter_cap15/13-equity_curve_regression_insp500_spyfilter_cap15.parquet\",\n",
    "        # optional: for trade-level skew (not needed for convexity tests)\n",
    "        \"trades_file\": \"./13-trading_output_regression_insp500_spyfilter_cap15/13-trades_regression_insp500_spyfilter_cap15.parquet\",\n",
    "    },\n",
    "    # \"system2\": {\"equity_file\": \"...\"},\n",
    "    # \"system3\": {\"equity_file\": \"...\"},\n",
    "}\n",
    "\n",
    "# SPY regime parquet (your example looks indexed by \"Date\")\n",
    "SPY_FILE = r\"C:\\TWS API\\source\\pythonclient\\TradingIdeas\\MomentumSystem\\8-SPY_200DMA_market_regime\\8-SPY_200DMA_regime.parquet\"\n",
    "\n",
    "# Convexity buckets\n",
    "N_BUCKETS = 10                # deciles of |SPY return|\n",
    "HIGH_MOVE_BUCKET = 10         # \"top decile\" is bucket 10 if using 1..10 labels\n",
    "HIGH_MOVE_Q = 0.90            # threshold quantile for \"high move days\"\n",
    "\n",
    "# Tail buckets\n",
    "WORST_SPY_QS = [0.01, 0.05]   # worst 1% and 5% SPY days\n",
    "\n",
    "# ============================================================\n",
    "# LOADERS\n",
    "# ============================================================\n",
    "\n",
    "def _pick_first(cols, available):\n",
    "    for c in cols:\n",
    "        if c in available:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def load_equity_curve(path: str) -> pd.DataFrame:\n",
    "    eq = pd.read_parquet(path)\n",
    "\n",
    "    date_col = _pick_first([\"date\", \"Date\", \"datetime\", \"timestamp\"], list(eq.columns))\n",
    "    if date_col is None:\n",
    "        # sometimes date might be index\n",
    "        if eq.index.name in [\"date\", \"Date\"]:\n",
    "            eq = eq.reset_index()\n",
    "            date_col = eq.columns[0]\n",
    "        else:\n",
    "            raise KeyError(f\"Cannot find date column in equity curve: {path} columns={list(eq.columns)}\")\n",
    "\n",
    "    eq[date_col] = pd.to_datetime(eq[date_col], errors=\"coerce\")\n",
    "    eq = eq.dropna(subset=[date_col]).copy()\n",
    "    eq.rename(columns={date_col: \"date\"}, inplace=True)\n",
    "    eq[\"date\"] = eq[\"date\"].dt.normalize()\n",
    "\n",
    "    if \"portfolio_value\" not in eq.columns:\n",
    "        pv_col = _pick_first([\"portfolio\", \"equity\", \"total\", \"account_value\"], list(eq.columns))\n",
    "        if pv_col is None:\n",
    "            raise KeyError(f\"Cannot find portfolio value column in equity curve: {path}\")\n",
    "        eq.rename(columns={pv_col: \"portfolio_value\"}, inplace=True)\n",
    "\n",
    "    eq[\"portfolio_value\"] = pd.to_numeric(eq[\"portfolio_value\"], errors=\"coerce\")\n",
    "    eq = eq.dropna(subset=[\"portfolio_value\"]).sort_values(\"date\").drop_duplicates(\"date\")\n",
    "\n",
    "    eq[\"ret\"] = eq[\"portfolio_value\"].pct_change()\n",
    "    eq = eq.dropna(subset=[\"ret\"]).reset_index(drop=True)\n",
    "    return eq[[\"date\", \"portfolio_value\", \"ret\"]]\n",
    "\n",
    "def load_spy_regime(path: str) -> pd.DataFrame:\n",
    "    spy = pd.read_parquet(path)\n",
    "\n",
    "    # your file appears to be indexed by \"Date\"\n",
    "    if \"Date\" in spy.columns:\n",
    "        spy[\"date\"] = pd.to_datetime(spy[\"Date\"], errors=\"coerce\")\n",
    "    elif \"date\" in spy.columns:\n",
    "        spy[\"date\"] = pd.to_datetime(spy[\"date\"], errors=\"coerce\")\n",
    "    elif spy.index.name in [\"Date\", \"date\"]:\n",
    "        spy = spy.reset_index().rename(columns={spy.index.name: \"date\"})\n",
    "        spy[\"date\"] = pd.to_datetime(spy[\"date\"], errors=\"coerce\")\n",
    "    else:\n",
    "        # last resort: assume first col is date\n",
    "        spy = spy.reset_index().rename(columns={\"index\": \"date\"})\n",
    "        spy[\"date\"] = pd.to_datetime(spy[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    spy = spy.dropna(subset=[\"date\"]).copy()\n",
    "    spy[\"date\"] = spy[\"date\"].dt.normalize()\n",
    "\n",
    "    if \"spy_close\" not in spy.columns:\n",
    "        raise KeyError(\"SPY file missing 'spy_close' column\")\n",
    "\n",
    "    spy[\"spy_close\"] = pd.to_numeric(spy[\"spy_close\"], errors=\"coerce\")\n",
    "    spy = spy.dropna(subset=[\"spy_close\"]).sort_values(\"date\").drop_duplicates(\"date\")\n",
    "\n",
    "    spy[\"spy_ret\"] = spy[\"spy_close\"].pct_change()\n",
    "    spy = spy.dropna(subset=[\"spy_ret\"]).reset_index(drop=True)\n",
    "\n",
    "    if \"market_regime\" in spy.columns:\n",
    "        spy[\"market_regime\"] = pd.to_numeric(spy[\"market_regime\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    else:\n",
    "        spy[\"market_regime\"] = 1  # if absent, treat all as \"bull\"\n",
    "\n",
    "    return spy[[\"date\", \"spy_close\", \"spy_ret\", \"market_regime\"]]\n",
    "\n",
    "# ============================================================\n",
    "# METRICS\n",
    "# ============================================================\n",
    "\n",
    "def max_drawdown_from_rets(rets: np.ndarray) -> float:\n",
    "    eq = np.cumprod(1.0 + rets)\n",
    "    peak = np.maximum.accumulate(eq)\n",
    "    dd = eq / peak - 1.0\n",
    "    return float(dd.min())\n",
    "\n",
    "def sharpe(rets: np.ndarray, ann=252) -> float:\n",
    "    rets = np.asarray(rets, dtype=float)\n",
    "    if rets.size < 2:\n",
    "        return np.nan\n",
    "    s = rets.std(ddof=1)\n",
    "    if s == 0:\n",
    "        return np.nan\n",
    "    return float(np.sqrt(ann) * rets.mean() / s)\n",
    "\n",
    "def tail_ratio(rets: np.ndarray, q=0.05) -> float:\n",
    "    \"\"\"(average of top q tail) / abs(average of bottom q tail)\"\"\"\n",
    "    r = np.asarray(rets, dtype=float)\n",
    "    if r.size < 50:\n",
    "        return np.nan\n",
    "    hi = np.mean(np.sort(r)[-max(1, int(q * len(r))):])\n",
    "    lo = np.mean(np.sort(r)[:max(1, int(q * len(r)))])\n",
    "    if lo == 0:\n",
    "        return np.nan\n",
    "    return float(hi / abs(lo))\n",
    "\n",
    "def ols_beta_gamma(y: np.ndarray, x: np.ndarray):\n",
    "    \"\"\"\n",
    "    Simple OLS: y = a + b*x + g*x^2\n",
    "    Returns a,b,g and R^2. (No p-values; good enough for directional convexity check.)\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    X = np.column_stack([np.ones_like(x), x, x**2])\n",
    "    coef, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
    "    yhat = X @ coef\n",
    "    ss_res = np.sum((y - yhat) ** 2)\n",
    "    ss_tot = np.sum((y - y.mean()) ** 2)\n",
    "    r2 = 1.0 - (ss_res / ss_tot) if ss_tot > 0 else np.nan\n",
    "    a, b, g = coef.tolist()\n",
    "    return a, b, g, float(r2)\n",
    "\n",
    "# ============================================================\n",
    "# ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "def convexity_tables(merged: pd.DataFrame, sys_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    merged columns required: date, ret (strategy), spy_ret, market_regime\n",
    "    Creates deciles of |spy_ret| and computes mean strat ret overall + downside-only.\n",
    "    \"\"\"\n",
    "    df = merged.copy()\n",
    "    df[\"abs_spy\"] = df[\"spy_ret\"].abs()\n",
    "\n",
    "    # deciles\n",
    "    df[\"abs_spy_bucket\"] = pd.qcut(df[\"abs_spy\"], q=N_BUCKETS, labels=list(range(1, N_BUCKETS + 1)))\n",
    "\n",
    "    tbl_all = (\n",
    "        df.groupby(\"abs_spy_bucket\", as_index=False)\n",
    "          .agg(\n",
    "              n=(\"ret\", \"size\"),\n",
    "              mean_strat_ret=(\"ret\", \"mean\"),\n",
    "              mean_spy_ret=(\"spy_ret\", \"mean\"),\n",
    "              mean_abs_spy=(\"abs_spy\", \"mean\"),\n",
    "              hit_rate=(\"ret\", lambda x: float((x > 0).mean())),\n",
    "          )\n",
    "    )\n",
    "    tbl_all[\"system\"] = sys_name\n",
    "    tbl_all[\"subset\"] = \"ALL\"\n",
    "\n",
    "    df_down = df[df[\"spy_ret\"] < 0].copy()\n",
    "    if len(df_down) > 0:\n",
    "        df_down[\"abs_spy_bucket\"] = pd.qcut(df_down[\"abs_spy\"], q=min(N_BUCKETS, df_down[\"abs_spy\"].nunique()),\n",
    "                                            labels=None, duplicates=\"drop\")\n",
    "        # relabel to 1..k\n",
    "        if df_down[\"abs_spy_bucket\"].dtype.name == \"category\":\n",
    "            df_down[\"abs_spy_bucket\"] = df_down[\"abs_spy_bucket\"].cat.codes + 1\n",
    "\n",
    "        tbl_down = (\n",
    "            df_down.groupby(\"abs_spy_bucket\", as_index=False)\n",
    "                   .agg(\n",
    "                       n=(\"ret\", \"size\"),\n",
    "                       mean_strat_ret=(\"ret\", \"mean\"),\n",
    "                       mean_spy_ret=(\"spy_ret\", \"mean\"),\n",
    "                       mean_abs_spy=(\"abs_spy\", \"mean\"),\n",
    "                       hit_rate=(\"ret\", lambda x: float((x > 0).mean())),\n",
    "                   )\n",
    "        )\n",
    "        tbl_down[\"system\"] = sys_name\n",
    "        tbl_down[\"subset\"] = \"SPY_DOWN\"\n",
    "        out = pd.concat([tbl_all, tbl_down], ignore_index=True)\n",
    "    else:\n",
    "        out = tbl_all\n",
    "\n",
    "    return out\n",
    "\n",
    "def summarize_system(merged: pd.DataFrame, sys_name: str) -> dict:\n",
    "    r = merged[\"ret\"].to_numpy()\n",
    "    spy = merged[\"spy_ret\"].to_numpy()\n",
    "\n",
    "    a, b, g, r2 = ols_beta_gamma(r, spy)\n",
    "\n",
    "    # High-move day behavior (top decile of |SPY|)\n",
    "    abs_spy = np.abs(spy)\n",
    "    thr = np.quantile(abs_spy, HIGH_MOVE_Q)\n",
    "    hi = merged[abs_spy >= thr]\n",
    "    mid = merged[abs_spy < thr]\n",
    "\n",
    "    # Worst SPY day buckets\n",
    "    worst_stats = {}\n",
    "    for q in WORST_SPY_QS:\n",
    "        cut = np.quantile(spy, q)\n",
    "        bucket = merged[merged[\"spy_ret\"] <= cut]\n",
    "        worst_stats[f\"mean_ret_on_worst_spy_{int(q*100)}pct_days\"] = float(bucket[\"ret\"].mean()) if len(bucket) else np.nan\n",
    "\n",
    "    out = {\n",
    "        \"system\": sys_name,\n",
    "        \"n_days\": int(len(merged)),\n",
    "        \"mean_daily_ret\": float(np.mean(r)),\n",
    "        \"vol_daily\": float(np.std(r, ddof=1)),\n",
    "        \"sharpe\": sharpe(r),\n",
    "        \"maxdd\": max_drawdown_from_rets(r),\n",
    "        \"skew\": float(pd.Series(r).skew()),\n",
    "        \"excess_kurt\": float(pd.Series(r).kurt()),  # pandas kurt() is excess kurtosis by default\n",
    "        \"tail_ratio_5pct\": tail_ratio(r, q=0.05),\n",
    "\n",
    "        # Convexity proxies\n",
    "        \"beta_to_spy\": float(b),\n",
    "        \"gamma_to_spy2\": float(g),    # >0 suggests convexity vs SPY\n",
    "        \"ols_r2\": float(r2),\n",
    "\n",
    "        \"mean_ret_high_abs_spy\": float(hi[\"ret\"].mean()) if len(hi) else np.nan,\n",
    "        \"mean_ret_other_days\": float(mid[\"ret\"].mean()) if len(mid) else np.nan,\n",
    "        \"delta_high_move\": (float(hi[\"ret\"].mean()) - float(mid[\"ret\"].mean())) if (len(hi) and len(mid)) else np.nan,\n",
    "\n",
    "        \"corr_with_spy\": float(np.corrcoef(r, spy)[0, 1]),\n",
    "    }\n",
    "    out.update(worst_stats)\n",
    "\n",
    "    # Regime splits\n",
    "    for regime_val, regime_name in [(1, \"bull\"), (0, \"bear\")]:\n",
    "        sub = merged[merged[\"market_regime\"] == regime_val]\n",
    "        if len(sub) >= 50:\n",
    "            rr = sub[\"ret\"].to_numpy()\n",
    "            ss = sub[\"spy_ret\"].to_numpy()\n",
    "            _, bb, gg, rr2 = ols_beta_gamma(rr, ss)\n",
    "            out[f\"sharpe_{regime_name}\"] = sharpe(rr)\n",
    "            out[f\"maxdd_{regime_name}\"] = max_drawdown_from_rets(rr)\n",
    "            out[f\"corr_with_spy_{regime_name}\"] = float(np.corrcoef(rr, ss)[0, 1])\n",
    "            out[f\"gamma_{regime_name}\"] = float(gg)\n",
    "            out[f\"beta_{regime_name}\"] = float(bb)\n",
    "            out[f\"n_days_{regime_name}\"] = int(len(sub))\n",
    "        else:\n",
    "            out[f\"sharpe_{regime_name}\"] = np.nan\n",
    "            out[f\"maxdd_{regime_name}\"] = np.nan\n",
    "            out[f\"corr_with_spy_{regime_name}\"] = np.nan\n",
    "            out[f\"gamma_{regime_name}\"] = np.nan\n",
    "            out[f\"beta_{regime_name}\"] = np.nan\n",
    "            out[f\"n_days_{regime_name}\"] = int(len(sub))\n",
    "\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    spy = load_spy_regime(SPY_FILE)\n",
    "\n",
    "    # Load systems and merge to SPY\n",
    "    rets_by_system = {}\n",
    "    summaries = []\n",
    "    convex_tables = []\n",
    "\n",
    "    for name, cfg in SYSTEMS.items():\n",
    "        eq = load_equity_curve(cfg[\"equity_file\"])\n",
    "\n",
    "        merged = eq.merge(spy, on=\"date\", how=\"inner\")\n",
    "        if len(merged) < 200:\n",
    "            print(f\"[WARN] {name}: only {len(merged)} merged days with SPY. Check dates/files.\")\n",
    "        merged = merged.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "        # store returns series\n",
    "        s = merged.set_index(\"date\")[\"ret\"].rename(name)\n",
    "        rets_by_system[name] = s\n",
    "\n",
    "        summaries.append(summarize_system(merged, name))\n",
    "        convex_tables.append(convexity_tables(merged, name))\n",
    "\n",
    "        # quick plot: strat ret vs abs spy bucket (ALL)\n",
    "        tbl_plot = convex_tables[-1].query(\"subset == 'ALL'\").copy()\n",
    "        if len(tbl_plot):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(tbl_plot[\"abs_spy_bucket\"], tbl_plot[\"mean_strat_ret\"], marker=\"o\")\n",
    "            plt.title(f\"{name}: Mean strategy return by |SPY return| decile (ALL)\")\n",
    "            plt.xlabel(\"|SPY return| decile (1=low, 10=high)\")\n",
    "            plt.ylabel(\"Mean strategy daily return\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(OUT_DIR, f\"{name}_mean_ret_by_abs_spy_decile.png\"))\n",
    "            plt.close()\n",
    "\n",
    "    # Save per-system summary\n",
    "    summary_df = pd.DataFrame(summaries).sort_values([\"gamma_to_spy2\", \"skew\"], ascending=[False, False])\n",
    "    summary_path = os.path.join(OUT_DIR, \"25c-system_summary_convexity_skew.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "    # Save convexity bucket tables\n",
    "    convex_df = pd.concat(convex_tables, ignore_index=True)\n",
    "    convex_path = os.path.join(OUT_DIR, \"25c-convexity_tables_by_abs_spy_decile.csv\")\n",
    "    convex_df.to_csv(convex_path, index=False)\n",
    "\n",
    "    print(\"\\n=== SYSTEM SUMMARY (top rows) ===\")\n",
    "    print(summary_df.head(20).to_string(index=False))\n",
    "    print(f\"\\nWrote: {summary_path}\")\n",
    "    print(f\"Wrote: {convex_path}\")\n",
    "\n",
    "    # If multiple systems, compute correlation matrices\n",
    "    if len(rets_by_system) >= 2:\n",
    "        R = pd.concat(rets_by_system.values(), axis=1).dropna()\n",
    "        corr_all = R.corr()\n",
    "\n",
    "        corr_path = os.path.join(OUT_DIR, \"25c-system_return_correlation_ALL.csv\")\n",
    "        corr_all.to_csv(corr_path)\n",
    "        print(f\"\\nWrote: {corr_path}\")\n",
    "\n",
    "        # Also correlation on high-|SPY| days\n",
    "        spy2 = spy.set_index(\"date\").loc[R.index]\n",
    "        abs_spy = spy2[\"spy_ret\"].abs()\n",
    "        thr = abs_spy.quantile(HIGH_MOVE_Q)\n",
    "        R_hi = R.loc[abs_spy >= thr]\n",
    "        if len(R_hi) >= 50:\n",
    "            corr_hi = R_hi.corr()\n",
    "            corr_hi_path = os.path.join(OUT_DIR, \"25c-system_return_correlation_high_abs_spy.csv\")\n",
    "            corr_hi.to_csv(corr_hi_path)\n",
    "            print(f\"Wrote: {corr_hi_path}\")\n",
    "\n",
    "        # By regime\n",
    "        for regime_val, regime_name in [(1, \"bull\"), (0, \"bear\")]:\n",
    "            mask = (spy2[\"market_regime\"] == regime_val)\n",
    "            R_sub = R.loc[mask]\n",
    "            if len(R_sub) >= 50:\n",
    "                corr_sub = R_sub.corr()\n",
    "                p = os.path.join(OUT_DIR, f\"system_return_correlation_{regime_name}.csv\")\n",
    "                corr_sub.to_csv(p)\n",
    "                print(f\"Wrote: {p}\")\n",
    "\n",
    "    print(\"\\n=== DONE ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
