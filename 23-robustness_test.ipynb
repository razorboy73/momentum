{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3224d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tradable universe...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\TWS API\\\\source\\\\pythonclient\\\\TradingIdeas\\\\Momentum System\\\\12-tradable_sp500_universe\\\\12-tradable_sp500_universe.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# LOAD DATA\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading tradable universe...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m trad = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUNIVERSE_TRADABLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m trad[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(trad[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading full universe...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\farty\\anaconda3\\envs\\my_quant_lab\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\farty\\anaconda3\\envs\\my_quant_lab\\Lib\\site-packages\\pandas\\io\\parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\farty\\anaconda3\\envs\\my_quant_lab\\Lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\farty\\anaconda3\\envs\\my_quant_lab\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\TWS API\\\\source\\\\pythonclient\\\\TradingIdeas\\\\Momentum System\\\\12-tradable_sp500_universe\\\\12-tradable_sp500_universe.parquet'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG: FILE PATHS (ADJUST TO YOUR MACHINE)\n",
    "# ============================================================\n",
    "\n",
    "UNIVERSE_TRADABLE = r\"C:\\TWS API\\source\\pythonclient\\TradingIdeas\\Momentum System\\12-tradable_sp500_universe\\12-tradable_sp500_universe.parquet\"\n",
    "UNIVERSE_FULL     = r\"C:\\TWS API\\source\\pythonclient\\TradingIdeas\\Momentum System\\11-universe_with_jump90\\11-universe_with_jump90.parquet\"\n",
    "SPY_REGIME_FILE   = r\"C:\\TWS API\\source\\pythonclient\\TradingIdeas\\Momentum System\\8-SPY_200DMA_market_regime\\8-SPY_200DMA_regime.parquet\"\n",
    "\n",
    "OUTPUT_DIR = \"./deep_robustness_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "START_TRADING = pd.Timestamp(\"1999-01-01\")   # same as your main system\n",
    "INITIAL_CAPITAL = 500_000\n",
    "REBALANCE_DAY = \"Wednesday\"\n",
    "\n",
    "TRADING_DAYS_PER_YEAR = 252\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# METRIC HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def cagr_from_curve(eq: pd.Series):\n",
    "    eq = eq.dropna()\n",
    "    if len(eq) < 2:\n",
    "        return np.nan\n",
    "    years = len(eq) / TRADING_DAYS_PER_YEAR\n",
    "    if years <= 0 or eq.iloc[0] <= 0:\n",
    "        return np.nan\n",
    "    return (eq.iloc[-1] / eq.iloc[0]) ** (1 / years) - 1\n",
    "\n",
    "\n",
    "def max_drawdown(eq: pd.Series):\n",
    "    eq = eq.dropna()\n",
    "    if len(eq) == 0:\n",
    "        return np.nan\n",
    "    peak = eq.cummax()\n",
    "    dd = eq / peak - 1\n",
    "    return dd.min()\n",
    "\n",
    "\n",
    "def sharpe_ratio(returns: pd.Series):\n",
    "    r = returns.dropna()\n",
    "    if len(r) < 2 or r.std() == 0:\n",
    "        return np.nan\n",
    "    return r.mean() / r.std() * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "\n",
    "\n",
    "def sortino_ratio(returns: pd.Series):\n",
    "    r = returns.dropna()\n",
    "    downside = r[r < 0]\n",
    "    if len(downside) == 0 or downside.std() == 0:\n",
    "        return np.nan\n",
    "    return r.mean() / downside.std() * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "\n",
    "\n",
    "def summarize_equity(name, eq: pd.Series):\n",
    "    eq = eq.dropna()\n",
    "    ret = eq.pct_change()\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"CAGR_%\": cagr_from_curve(eq) * 100,\n",
    "        \"MaxDD_%\": max_drawdown(eq) * 100,\n",
    "        \"Sharpe\": sharpe_ratio(ret),\n",
    "        \"Sortino\": sortino_ratio(ret),\n",
    "        \"FinalValue\": eq.iloc[-1],\n",
    "        \"StartDate\": eq.index[0].date() if len(eq) else None,\n",
    "        \"EndDate\": eq.index[-1].date() if len(eq) else None,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading tradable universe...\")\n",
    "trad = pd.read_parquet(UNIVERSE_TRADABLE)\n",
    "trad[\"date\"] = pd.to_datetime(trad[\"date\"])\n",
    "\n",
    "print(\"Loading full universe...\")\n",
    "full = pd.read_parquet(UNIVERSE_FULL)\n",
    "full[\"date\"] = pd.to_datetime(full[\"date\"])\n",
    "\n",
    "print(\"Loading SPY regime...\")\n",
    "spy_regime = (\n",
    "    pd.read_parquet(SPY_REGIME_FILE)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Date\": \"date\"})\n",
    ")\n",
    "spy_regime[\"date\"] = pd.to_datetime(spy_regime[\"date\"])\n",
    "\n",
    "# Merge SPY regime into both universes\n",
    "merge_cols = [\"date\", \"spy_close\", \"spy_ma200\", \"market_regime\"]\n",
    "\n",
    "trad = trad.merge(spy_regime[merge_cols], on=\"date\", how=\"left\")\n",
    "full = full.merge(spy_regime[merge_cols], on=\"date\", how=\"left\")\n",
    "\n",
    "print(\"Rows tradable:\", len(trad))\n",
    "print(\"Rows full    :\", len(full))\n",
    "\n",
    "# Make sure numeric columns are numeric\n",
    "for df in (trad, full):\n",
    "    for col in [\"close_adj\", \"slope_adj\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# HELPER: BUILD DAILY PRICE PANEL (date x ticker)\n",
    "# ============================================================\n",
    "\n",
    "def build_price_panel(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Pivot to date x ticker price panel for close_adj.\n",
    "    \"\"\"\n",
    "    pivot = (\n",
    "        df\n",
    "        .pivot_table(index=\"date\", columns=\"ticker\", values=\"close_adj\")\n",
    "        .sort_index()\n",
    "    )\n",
    "    return pivot\n",
    "\n",
    "\n",
    "price_panel_trad = build_price_panel(trad)\n",
    "price_panel_full = build_price_panel(full)\n",
    "\n",
    "# daily returns (no forward fill to avoid look-ahead)\n",
    "daily_returns_trad = price_panel_trad.pct_change(fill_method=None)\n",
    "daily_returns_full = price_panel_full.pct_change(fill_method=None)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CORE SIMPLE MOMENTUM ENGINE (ROBUSTNESS HARNESS)\n",
    "# ============================================================\n",
    "\n",
    "def run_simple_momo(\n",
    "    name: str,\n",
    "    universe: str = \"tradable\",     # \"tradable\" or \"full\"\n",
    "    top_percentile: float = 0.9,    # e.g. 0.8, 0.9, 0.95\n",
    "    top_k: int | None = None,       # e.g. 20, or None to ignore\n",
    "    slope_lag_days: int = 0,        # 0 = use same-day slope_adj; 1 = lag by 1 day; etc.\n",
    "    lag_filters_by_days: int = 0,   # lag in_sp500, no_big_jump_90, above_ma100, market_regime\n",
    "    exec_delay_days: int = 0,       # 0 = trade at close today, 1 = trade at next day's close\n",
    "    min_history_days: int = 0,      # require at least this many trading days of price history\n",
    "    require_spy_bull: bool = True,  # True = only hold when SPY regime is bull\n",
    "    rebalance_weekday: str = \"Wednesday\",  # rebalance weekday name\n",
    "):\n",
    "    \"\"\"\n",
    "    Simplified weekly momentum system used *only* for robustness tests.\n",
    "\n",
    "    Logic (per rebalance day):\n",
    "      1. Pick universe: 'tradable' = 12-tradable_sp500_universe, 'full' = 11-universe_with_jump90.\n",
    "      2. Build cross-section for that date (possibly lagging slopes & filters).\n",
    "      3. Apply filters:\n",
    "           - in_sp500 (if present and True)\n",
    "           - no_big_jump_90 (if present and True)\n",
    "           - above_ma100 (if present and True)\n",
    "           - min_history_days of price data (if > 0)\n",
    "           - SPY regime (if require_spy_bull == True)\n",
    "      4. Rank remaining stocks by slope_adj (momentum).\n",
    "      5. Selection:\n",
    "           - If top_k is provided: choose min(top_k, number of candidates).\n",
    "           - Else: choose all with slope_adj >= given top_percentile.\n",
    "      6. Equal-weight each selected name, fully invested (no ATR sizing here).\n",
    "      7. Execute trades either at same-day close (exec_delay_days=0) or next-day close (T+1).\n",
    "      8. Track daily equity curve and stats from START_TRADING onward.\n",
    "    \"\"\"\n",
    "    if universe == \"tradable\":\n",
    "        df = trad.copy()\n",
    "        price_panel = price_panel_trad\n",
    "        daily_returns = daily_returns_trad\n",
    "    elif universe == \"full\":\n",
    "        df = full.copy()\n",
    "        price_panel = price_panel_full\n",
    "        daily_returns = daily_returns_full\n",
    "    else:\n",
    "        raise ValueError(\"universe must be 'tradable' or 'full'\")\n",
    "\n",
    "    # Restrict to dates with price info\n",
    "    all_dates = price_panel.index\n",
    "    all_dates = all_dates[all_dates >= START_TRADING]\n",
    "\n",
    "    # For fast access: group by date\n",
    "    df_by_date = {d: sub for d, sub in df.groupby(\"date\")}\n",
    "\n",
    "    # Precompute SPY regime series (bull/bear) and lag if needed\n",
    "    spy_reg = (\n",
    "        df[[\"date\", \"market_regime\"]]\n",
    "        .drop_duplicates(\"date\")\n",
    "        .set_index(\"date\")\n",
    "        [\"market_regime\"]\n",
    "        .sort_index()\n",
    "    )\n",
    "    if lag_filters_by_days > 0:\n",
    "        spy_reg = spy_reg.shift(lag_filters_by_days)\n",
    "\n",
    "    # Precompute price history length per ticker/date (trading days)\n",
    "    # We'll use price_panel to compute \"days since first non-null close\"\n",
    "    history_len = (~price_panel.isna()).cumsum()\n",
    "\n",
    "    # Rebalance dates = all dates with given weekday\n",
    "    rebalance_dates = [d for d in all_dates if d.day_name() == rebalance_weekday]\n",
    "\n",
    "    equity = INITIAL_CAPITAL\n",
    "    positions = {}  # ticker -> weight\n",
    "    equity_curve = []\n",
    "\n",
    "    prev_date = None\n",
    "\n",
    "    for current_date in all_dates:\n",
    "\n",
    "        # First apply yesterday's positions to today's returns to evolve equity\n",
    "        if prev_date is not None:\n",
    "            if positions:\n",
    "                day_ret = 0.0\n",
    "                for t, w in positions.items():\n",
    "                    if t not in daily_returns.columns:\n",
    "                        continue\n",
    "                    r = daily_returns.loc[current_date, t]\n",
    "                    if pd.isna(r):\n",
    "                        continue\n",
    "                    day_ret += w * r\n",
    "                equity *= (1.0 + day_ret)\n",
    "            # else all cash → equity unchanged\n",
    "\n",
    "        # Record equity\n",
    "        equity_curve.append({\"date\": current_date, \"equity\": equity})\n",
    "\n",
    "        # Rebalance on chosen weekday\n",
    "        if current_date in rebalance_dates:\n",
    "\n",
    "            # Determine decision date for signals (lagging slope & filters)\n",
    "            decision_date = current_date\n",
    "            if slope_lag_days > 0 or lag_filters_by_days > 0:\n",
    "                # shift back by n trading days, not calendar days\n",
    "                idx = np.searchsorted(all_dates, current_date) - max(slope_lag_days, lag_filters_by_days)\n",
    "                if idx < 0:\n",
    "                    # not enough history yet\n",
    "                    pass\n",
    "                else:\n",
    "                    decision_date = all_dates[idx]\n",
    "\n",
    "            day_df = df_by_date.get(decision_date)\n",
    "            if day_df is None:\n",
    "                # no universe data for that day\n",
    "                positions = {}\n",
    "            else:\n",
    "                # Filter universe\n",
    "                sub = day_df.copy()\n",
    "\n",
    "                # Only allow stocks actually in the price panel on *execution* date\n",
    "                # Execution date may be T or T+1\n",
    "                exec_idx = np.searchsorted(all_dates, current_date) + exec_delay_days\n",
    "                if exec_idx >= len(all_dates):\n",
    "                    # past data end, no more trades\n",
    "                    pass\n",
    "                else:\n",
    "                    exec_date = all_dates[exec_idx]\n",
    "\n",
    "                    if min_history_days > 0:\n",
    "                        # require at least 'min_history_days' trading days of price history by exec_date\n",
    "                        for_hist = history_len.loc[:exec_date]\n",
    "                        # map: ticker -> history length at exec_date\n",
    "                        valid_hist = for_hist.iloc[-1]\n",
    "                        sub = sub[sub[\"ticker\"].map(lambda t: valid_hist.get(t, 0) >= min_history_days)]\n",
    "\n",
    "                    # Basic filters (if present)\n",
    "                    if \"in_sp500\" in sub.columns:\n",
    "                        sub = sub[sub[\"in_sp500\"] == True]\n",
    "\n",
    "                    if \"no_big_jump_90\" in sub.columns:\n",
    "                        sub = sub[sub[\"no_big_jump_90\"] == True]\n",
    "\n",
    "                    if \"above_ma100\" in sub.columns:\n",
    "                        sub = sub[sub[\"above_ma100\"] == True]\n",
    "\n",
    "                    # SPY regime filter\n",
    "                    if require_spy_bull:\n",
    "                        spy_flag = spy_reg.loc[decision_date] if decision_date in spy_reg.index else np.nan\n",
    "                        if not (pd.notna(spy_flag) and spy_flag > 0):\n",
    "                            # regime not bull -> no positions\n",
    "                            positions = {}\n",
    "                            prev_date = current_date\n",
    "                            continue\n",
    "\n",
    "                    # Rank by slope_adj\n",
    "                    sub = sub[sub[\"slope_adj\"].notna()]\n",
    "                    if len(sub) < 5:\n",
    "                        positions = {}\n",
    "                    else:\n",
    "                        sub = sub.sort_values(\"slope_adj\", ascending=False)\n",
    "\n",
    "                        # Selection: either hard top_k or percentile\n",
    "                        if top_k is not None:\n",
    "                            sub_sel = sub.head(top_k)\n",
    "                        else:\n",
    "                            cutoff = sub[\"slope_adj\"].quantile(top_percentile)\n",
    "                            sub_sel = sub[sub[\"slope_adj\"] >= cutoff]\n",
    "\n",
    "                        if len(sub_sel) == 0:\n",
    "                            positions = {}\n",
    "                        else:\n",
    "                            # Equal-weight among selected\n",
    "                            tickers = list(sub_sel[\"ticker\"].unique())\n",
    "                            w = 1.0 / len(tickers)\n",
    "                            positions = {t: w for t in tickers}\n",
    "\n",
    "        prev_date = current_date\n",
    "\n",
    "    eq = pd.DataFrame(equity_curve).set_index(\"date\")[\"equity\"]\n",
    "    eq = eq[eq.index >= START_TRADING]\n",
    "    stats = summarize_equity(name, eq)\n",
    "    return eq, stats\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DEFINE ROBUSTNESS SCENARIOS\n",
    "# ============================================================\n",
    "\n",
    "scenarios = []\n",
    "\n",
    "# 1) Baseline-like tradable universe\n",
    "scenarios.append(dict(\n",
    "    name=\"baseline_tradable\",\n",
    "    universe=\"tradable\",\n",
    "    top_percentile=0.9,\n",
    "    top_k=None,\n",
    "    slope_lag_days=0,\n",
    "    lag_filters_by_days=0,\n",
    "    exec_delay_days=0,\n",
    "    min_history_days=0,\n",
    "    require_spy_bull=True,\n",
    "))\n",
    "\n",
    "# 2) Full universe, no in_sp500 filter (stress survivors / delistings)\n",
    "scenarios.append(dict(\n",
    "    name=\"full_universe_no_sp500_filter\",\n",
    "    universe=\"full\",\n",
    "    top_percentile=0.9,\n",
    "    top_k=None,\n",
    "    slope_lag_days=0,\n",
    "    lag_filters_by_days=0,\n",
    "    exec_delay_days=0,\n",
    "    min_history_days=0,\n",
    "    require_spy_bull=True,\n",
    "))\n",
    "\n",
    "# 3) Tradable, slope lagged by 1 and 5 days\n",
    "scenarios.append(dict(\n",
    "    name=\"tradable_slope_lag1\",\n",
    "    universe=\"tradable\",\n",
    "    top_percentile=0.9,\n",
    "    top_k=None,\n",
    "    slope_lag_days=1,\n",
    "    lag_filters_by_days=0,\n",
    "    exec_delay_days=0,\n",
    "    min_history_days=0,\n",
    "    require_spy_bull=True,\n",
    "))\n",
    "scenarios.append(dict(\n",
    "    name=\"tradable_slope_lag5\",\n",
    "    universe=\"tradable\",\n",
    "    top_percentile=0.9,\n",
    "    top_k=None,\n",
    "    slope_lag_days=5,\n",
    "    lag_filters_by_days=0,\n",
    "    exec_delay_days=0,\n",
    "    min_history_days=0,\n",
    "    require_spy_bull=True,\n",
    "))\n",
    "\n",
    "# 4) Lag filters (in_sp500, no_big_jump_90, above_ma100, market_regime) by 1 day\n",
    "scenarios.append(dict(\n",
    "    name=\"tradable_filters_lag1\",\n",
    "    universe=\"tradable\",\n",
    "    top_percentile=0.9,\n",
    "    top_k=None,\n",
    "    slope_lag_days=0,\n",
    "    lag_filters_by_days=1,\n",
    "    exec_delay_days=0,\n",
    "    min_history_days=0,\n",
    "    require_spy_bull=True,\n",
    "))\n",
    "\n",
    "# 5) T+1 execution\n",
    "scenarios.append(dict(\n",
    "    name=\"tradable_exec_Tplus1\",\n",
    "    universe=\"tradable\",\n",
    "    top_percentile=0.9,\n",
    "    top_k=None,\n",
    "    slope_lag_days=0,\n",
    "    lag_filters_by_days=0,\n",
    "    exec_delay_days=1,\n",
    "    min_history_days=0,\n",
    "    require_spy_bull=True,\n",
    "))\n",
    "\n",
    "# 6) Require at least 200 days of history before trading a stock\n",
    "scenarios.append(dict(\n",
    "    name=\"tradable_minHistory200\",\n",
    "    universe=\"tradable\",\n",
    "    top_percentile=0.9,\n",
    "    top_k=None,\n",
    "    slope_lag_days=0,\n",
    "    lag_filters_by_days=0,\n",
    "    exec_delay_days=0,\n",
    "    min_history_days=200,\n",
    "    require_spy_bull=True,\n",
    "))\n",
    "\n",
    "# 7) Top 20 hard cap (equal-weight), tradable universe\n",
    "scenarios.append(dict(\n",
    "    name=\"tradable_top20\",\n",
    "    universe=\"tradable\",\n",
    "    top_percentile=0.0,   # ignored because top_k is set\n",
    "    top_k=20,\n",
    "    slope_lag_days=0,\n",
    "    lag_filters_by_days=0,\n",
    "    exec_delay_days=0,\n",
    "    min_history_days=0,\n",
    "    require_spy_bull=True,\n",
    "))\n",
    "\n",
    "# 8) Percentile variations: 0.8 and 0.95\n",
    "scenarios.append(dict(\n",
    "    name=\"tradable_p80\",\n",
    "    universe=\"tradable\",\n",
    "    top_percentile=0.8,\n",
    "    top_k=None,\n",
    "    slope_lag_days=0,\n",
    "    lag_filters_by_days=0,\n",
    "    exec_delay_days=0,\n",
    "    min_history_days=0,\n",
    "    require_spy_bull=True,\n",
    "))\n",
    "scenarios.append(dict(\n",
    "    name=\"tradable_p95\",\n",
    "    universe=\"tradable\",\n",
    "    top_percentile=0.95,\n",
    "    top_k=None,\n",
    "    slope_lag_days=0,\n",
    "    lag_filters_by_days=0,\n",
    "    exec_delay_days=0,\n",
    "    min_history_days=0,\n",
    "    require_spy_bull=True,\n",
    "))\n",
    "\n",
    "# 9) No SPY regime filter (always on)\n",
    "scenarios.append(dict(\n",
    "    name=\"tradable_no_spy_filter\",\n",
    "    universe=\"tradable\",\n",
    "    top_percentile=0.9,\n",
    "    top_k=None,\n",
    "    slope_lag_days=0,\n",
    "    lag_filters_by_days=0,\n",
    "    exec_delay_days=0,\n",
    "    min_history_days=0,\n",
    "    require_spy_bull=False,\n",
    "))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN ALL SCENARIOS\n",
    "# ============================================================\n",
    "\n",
    "all_stats = []\n",
    "all_eq_curves = {}\n",
    "\n",
    "for cfg in scenarios:\n",
    "    name = cfg[\"name\"]\n",
    "    print(f\"\\n=== Running scenario: {name} ===\")\n",
    "    eq, stats = run_simple_momo(**cfg)\n",
    "    all_eq_curves[name] = eq\n",
    "    all_stats.append(stats)\n",
    "\n",
    "    # Save equity curve for later inspection\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"equity_{name}.parquet\")\n",
    "    eq.reset_index().rename(columns={\"equity\": \"portfolio_value\"}).to_parquet(out_path, index=False)\n",
    "    print(f\"Saved equity curve → {out_path}\")\n",
    "\n",
    "# Build summary DataFrame\n",
    "stats_df = pd.DataFrame(all_stats)\n",
    "stats_df = stats_df[[\"name\", \"CAGR_%\", \"MaxDD_%\", \"Sharpe\", \"Sortino\", \"FinalValue\", \"StartDate\", \"EndDate\"]]\n",
    "stats_df = stats_df.sort_values(\"CAGR_%\", ascending=False)\n",
    "\n",
    "print(\"\\n=== DEEP ROBUSTNESS SUMMARY ===\")\n",
    "print(stats_df.to_string(index=False, float_format=lambda x: f\"{x:8.3f}\" if isinstance(x, float) else str(x)))\n",
    "\n",
    "summary_path = os.path.join(OUTPUT_DIR, \"deep_robustness_summary.csv\")\n",
    "stats_df.to_csv(summary_path, index=False)\n",
    "print(f\"\\nSummary saved → {summary_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
