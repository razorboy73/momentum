{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95278489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input data...\n",
      "Loading rankings data...\n",
      "Loaded 78 ranking records\n",
      "Trades:       9\n",
      "Equity curve: 11\n",
      "Universe:     3,591,967\n",
      "SPY rows:     7,042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farty\\AppData\\Local\\Temp\\ipykernel_136456\\1398720428.py:373: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily[\"spy_ret\"]   = daily[\"spy_close\"].pct_change().fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPLETE ===\n",
      "Weekly trading log PDF saved → ./15a-match_weekly_trading_logs\\weekly_trading_logs_20260105-115659.pdf\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Weekly trading log PDF generator WITH full reporting (AUDIT-BASED) -\n",
    "used to make sure the trade generator generates the same trades as the backtest engine\n",
    "— UPDATED to handle NEW TRADES SCHEMA with signal + execution fields.\n",
    "— UPDATED to include ranked stocks table showing which were traded and why others weren't\n",
    "\n",
    "NEW trades schema expected (example):\n",
    "    signal_date, exec_date, signal_close_adj, exec_open_adj,\n",
    "    ticker, type, shares, price, value, reason, slope_rank_within_top,\n",
    "    spy_above_200dma, cash_before, cash_after, equity_after, portfolio_after, num_positions_after\n",
    "(plus any extra columns your engine writes)\n",
    "\n",
    "Core reporting logic:\n",
    "- Weekly report date is the EXECUTION day (e.g. Thursday), grouped by exec_date.\n",
    "- Signal-day analytics (rank map, slope_adj, filters, SPY regime message) use signal_date (e.g. Wednesday).\n",
    "- Audited cash BEFORE/AFTER comes from per-trade cash_before/cash_after on exec_date.\n",
    "- Post-trade holdings are reconstructed by applying trades on exec_date and then valuing\n",
    "  holdings at exec_date close_adj (Q3=A backfill with last-known close).\n",
    "- Trade tables show BOTH:\n",
    "    * signal price (signal_close_adj on Wednesday)\n",
    "    * execution price (exec_open_adj on Thursday)\n",
    "  and still keep the original 'price'/'trade_value' columns if present.\n",
    "\n",
    "NEW SECTION:\n",
    "- All Ranked Stocks table showing every stock that ranked in the top percentile,\n",
    "  with indicators for whether it was traded and reasons if not traded.\n",
    "\n",
    "Outputs:\n",
    "- PDF in ./15-match_weekly_trading_logs/\n",
    "- Overview equity/drawdown chart on front page\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from reportlab.lib.pagesizes import letter, landscape\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import (\n",
    "    SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle,\n",
    "    PageBreak, Image\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    " \n",
    "TRADES_FILE = \"./13-match_trade_generator_output_regression_insp500_spyfilter_cap15/13-match_trades_regression_insp500_spyfilter_cap15.parquet\"\n",
    "EQUITY_FILE = \"./13-match_trade_generator_output_regression_insp500_spyfilter_cap15/13-match_equity_curve_regression_insp500_spyfilter_cap15.parquet\"\n",
    "UNIVERSE_FILE = \"./12-tradable_sp500_universe/12-tradable_sp500_universe.parquet\"\n",
    "SPY_FILE      = \"./8-SPY_200DMA_market_regime/8-SPY_200DMA_regime.parquet\"\n",
    "RANKINGS_FILE = \"./13-match_trade_generator_output_regression_insp500_spyfilter_cap15/13-match_weekly_rankings_pre_filter_cap15.parquet\"\n",
    "\n",
    "OUTPUT_DIR = \"./15a-match_weekly_trading_logs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Report cadence:\n",
    "# - trades execute on Thursday in your system (exec_date = Thursday)\n",
    "# - signals are generated Wednesday (signal_date = Wednesday)\n",
    "TRADING_DAY_NAME      = \"Thursday\"     # execution day (weekly report day)\n",
    "SIGNAL_DAY_NAME       = \"Wednesday\"    # signal day (rank/spy regime day)\n",
    "\n",
    "START_TRADING         = pd.Timestamp(\"2025-12-17\")\n",
    "TOP_PERCENTILE        = 0.90\n",
    "MIN_YEAR_FOR_REPORT   = 1999\n",
    "MAX_YEAR_FOR_REPORT   = 2026\n",
    "TRADING_DAYS_PER_YEAR = 252\n",
    "\n",
    "# cash risk flags (for the weekly summary)\n",
    "LOW_CASH_FLOOR = 2000.0\n",
    "\n",
    "# Turnover filters (for categorizing why stocks didn't trade)\n",
    "DRIFT_THRESHOLD          = 0.05\n",
    "MIN_TRADE_VALUE          = 10000\n",
    "MIN_NEW_POSITION_WEIGHT  = 0.003\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# METRICS\n",
    "# ============================================================\n",
    "\n",
    "def sharpe_ratio(r: pd.Series) -> float:\n",
    "    r = r.dropna()\n",
    "    if len(r) < 2 or r.std() == 0:\n",
    "        return np.nan\n",
    "    return np.sqrt(TRADING_DAYS_PER_YEAR) * r.mean() / r.std()\n",
    "\n",
    "def sortino_ratio(r: pd.Series) -> float:\n",
    "    r = r.dropna()\n",
    "    d = r[r < 0]\n",
    "    if len(d) == 0 or d.std() == 0:\n",
    "        return np.nan\n",
    "    return np.sqrt(TRADING_DAYS_PER_YEAR) * r.mean() / d.std()\n",
    "\n",
    "def max_drawdown(series: pd.Series) -> float:\n",
    "    s = series.dropna()\n",
    "    if s.empty:\n",
    "        return np.nan\n",
    "    peak = s.cummax()\n",
    "    dd = s / peak - 1\n",
    "    return dd.min()\n",
    "\n",
    "def calmar_ratio(total_return: float, maxdd: float) -> float:\n",
    "    # NOTE: For YTD block we treat \"total_return\" as \"return over period\" (not CAGR).\n",
    "    if np.isnan(total_return) or np.isnan(maxdd) or maxdd >= 0:\n",
    "        return np.nan\n",
    "    return total_return / abs(maxdd)\n",
    "\n",
    "def ytd_stats_for_date(dt: pd.Timestamp, daily: pd.DataFrame) -> dict:\n",
    "    year = dt.year\n",
    "    start = max(\n",
    "    pd.Timestamp(year=year, month=1, day=1),\n",
    "    FIRST_EXEC_DATE\n",
    "    )\n",
    "    ytd = daily[(daily.index >= start) & (daily.index <= dt)]\n",
    "    if ytd.empty:\n",
    "        return {k: np.nan for k in [\n",
    "            \"strat_ret\",\"strat_maxdd\",\"strat_sharpe\",\"strat_sortino\",\"strat_calmar\",\n",
    "            \"spy_ret\",\"spy_maxdd\",\"spy_sharpe\",\"spy_sortino\",\"spy_calmar\"\n",
    "        ]}\n",
    "\n",
    "    strat_ret = ytd[\"portfolio_value\"].iloc[-1] / ytd[\"portfolio_value\"].iloc[0] - 1\n",
    "    strat_dd  = max_drawdown(ytd[\"portfolio_value\"])\n",
    "    strat_sh  = sharpe_ratio(ytd[\"strat_ret\"])\n",
    "    strat_so  = sortino_ratio(ytd[\"strat_ret\"])\n",
    "    strat_ca  = calmar_ratio(strat_ret, strat_dd)\n",
    "\n",
    "    spy_ret = ytd[\"spy_close\"].iloc[-1] / ytd[\"spy_close\"].iloc[0] - 1\n",
    "    spy_dd  = max_drawdown(ytd[\"spy_close\"])\n",
    "    spy_sh  = sharpe_ratio(ytd[\"spy_ret\"])\n",
    "    spy_so  = sortino_ratio(ytd[\"spy_ret\"])\n",
    "    spy_ca  = calmar_ratio(spy_ret, spy_dd)\n",
    "\n",
    "    return {\n",
    "        \"strat_ret\": strat_ret, \"strat_maxdd\": strat_dd,\n",
    "        \"strat_sharpe\": strat_sh, \"strat_sortino\": strat_so,\n",
    "        \"strat_calmar\": strat_ca,\n",
    "        \"spy_ret\": spy_ret, \"spy_maxdd\": spy_dd,\n",
    "        \"spy_sharpe\": spy_sh, \"spy_sortino\": spy_so,\n",
    "        \"spy_calmar\": spy_ca\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PRICE LOOKUP (Q3 = A: last-known close)\n",
    "# ============================================================\n",
    "\n",
    "def fast_price_lookup(px_array, date_val):\n",
    "    \"\"\"\n",
    "    Given a structured array with fields ['date','px'] and a date,\n",
    "    return the last known price at or before that date.\n",
    "    \"\"\"\n",
    "    date_val = np.datetime64(date_val, \"ns\")\n",
    "    dates = px_array[\"date\"]\n",
    "    idx = np.searchsorted(dates, date_val, side=\"right\") - 1\n",
    "    if idx < 0:\n",
    "        return np.nan\n",
    "    return px_array[\"px\"][idx]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CATEGORIZE WHY STOCK DIDN'T TRADE\n",
    "# ============================================================\n",
    "\n",
    "def categorize_no_trade(row):\n",
    "    \"\"\"Determine why a stock didn't trade\"\"\"\n",
    "    \n",
    "    # Already at target (within drift threshold)\n",
    "    if abs(row['weight_change']) < DRIFT_THRESHOLD:\n",
    "        return 'Within drift threshold'\n",
    "    \n",
    "    # New position but too small\n",
    "    if row['current_shares'] == 0 and row['target_weight'] < MIN_NEW_POSITION_WEIGHT:\n",
    "        return 'New position too small'\n",
    "    \n",
    "    # Trade value too small\n",
    "    if abs(row['shares_change'] * row['close_adj']) < MIN_TRADE_VALUE:\n",
    "        return 'Trade value too small'\n",
    "    \n",
    "    # Would buy but SPY regime prevents it\n",
    "    if row['shares_change'] > 0 and not row['spy_above_200dma']:\n",
    "        return 'SPY regime prevented buy'\n",
    "    \n",
    "    # Insufficient cash\n",
    "    if row['shares_change'] > 0:\n",
    "        return 'Insufficient cash'\n",
    "    \n",
    "    return 'Other'\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading input data...\")\n",
    "\n",
    "trades = pd.read_parquet(TRADES_FILE)\n",
    "\n",
    "# --- normalize / validate new schema ---\n",
    "required = [\"signal_date\", \"exec_date\", \"ticker\", \"type\", \"shares\"]\n",
    "missing = [c for c in required if c not in trades.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Trades file missing required columns: {missing}\")\n",
    "\n",
    "trades[\"signal_date\"] = pd.to_datetime(trades[\"signal_date\"])\n",
    "trades[\"exec_date\"]   = pd.to_datetime(trades[\"exec_date\"])\n",
    "\n",
    "# Back-compat helper: treat exec_date as the reporting \"date\"\n",
    "# (a lot of older report logic groups by trades['date'])\n",
    "trades[\"date\"] = trades[\"exec_date\"]\n",
    "\n",
    "# IMPORTANT: preserve intra-day execution order; only stable-sort by exec_date.\n",
    "trades = trades.sort_values(\"date\", kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD RANKINGS DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading rankings data...\")\n",
    "rankings = pd.read_parquet(RANKINGS_FILE)\n",
    "rankings[\"signal_date\"] = pd.to_datetime(rankings[\"signal_date\"])\n",
    "rankings[\"exec_date\"] = pd.to_datetime(rankings[\"exec_date\"])\n",
    "\n",
    "# Group by signal_date and ticker for merging\n",
    "trades_agg = trades.groupby(['signal_date', 'ticker']).agg({\n",
    "    'type': lambda x: ', '.join(x.unique()),\n",
    "    'shares': 'sum',\n",
    "    'value': 'sum' if 'value' in trades.columns else 'count',\n",
    "}).reset_index()\n",
    "trades_agg.columns = ['signal_date', 'ticker', 'trade_type', 'trade_shares', 'trade_value']\n",
    "\n",
    "# Merge rankings with trades\n",
    "rankings = rankings.merge(\n",
    "    trades_agg,\n",
    "    on=['signal_date', 'ticker'],\n",
    "    how='left',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# Add trade status\n",
    "rankings['traded'] = rankings['_merge'] == 'both'\n",
    "rankings['traded_flag'] = rankings['traded'].map({True: 'TRADED', False: 'NOT_TRADED'})\n",
    "rankings = rankings.drop('_merge', axis=1)\n",
    "\n",
    "# Add analysis columns\n",
    "rankings['weight_change'] = rankings['target_weight'] - rankings['current_weight']\n",
    "rankings['shares_change'] = rankings['target_shares'] - rankings['current_shares']\n",
    "\n",
    "# Categorize why stocks didn't trade\n",
    "rankings['no_trade_reason'] = rankings[~rankings['traded']].apply(categorize_no_trade, axis=1)\n",
    "\n",
    "print(f\"Loaded {len(rankings):,} ranking records\")\n",
    "\n",
    "# Group rankings by signal_date for quick lookup\n",
    "rankings_by_signal_date = {d: sub for d, sub in rankings.groupby(\"signal_date\")}\n",
    "\n",
    "# ============================================================\n",
    "# SLIPPAGE (signal close -> execution open)\n",
    "# Positive slippage_dollars = worse (cost) for both BUY and SELL\n",
    "# ============================================================\n",
    "\n",
    "def add_slippage_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Choose columns robustly\n",
    "    sig_col = \"signal_close_adj\" if \"signal_close_adj\" in df.columns else None\n",
    "    exe_col = \"exec_open_adj\" if \"exec_open_adj\" in df.columns else (\"price\" if \"price\" in df.columns else None)\n",
    "\n",
    "    if sig_col is None or exe_col is None:\n",
    "        # still add columns so downstream doesn't break\n",
    "        df[\"slippage_per_share\"] = np.nan\n",
    "        df[\"slippage_dollars\"] = np.nan\n",
    "        df[\"slippage_bps\"] = np.nan\n",
    "        return df\n",
    "\n",
    "    side = df[\"type\"].astype(str).str.upper()\n",
    "    shares = pd.to_numeric(df[\"shares\"], errors=\"coerce\").fillna(0).astype(float)\n",
    "    signal_px = pd.to_numeric(df[sig_col], errors=\"coerce\")\n",
    "    exec_px   = pd.to_numeric(df[exe_col], errors=\"coerce\")\n",
    "\n",
    "    # cost-per-share: BUY -> exec - signal ; SELL -> signal - exec\n",
    "    slip_ps = np.where(\n",
    "        side == \"BUY\", exec_px - signal_px,\n",
    "        np.where(side == \"SELL\", signal_px - exec_px, np.nan)\n",
    "    )\n",
    "\n",
    "    slip_dollars = slip_ps * shares\n",
    "\n",
    "    # Bps vs executed notional (abs)\n",
    "    exec_notional = (exec_px * shares).abs()\n",
    "    slip_bps = np.where(exec_notional > 0, (slip_dollars / exec_notional) * 1e4, np.nan)\n",
    "\n",
    "    df[\"slippage_per_share\"] = slip_ps\n",
    "    df[\"slippage_dollars\"] = slip_dollars\n",
    "    df[\"slippage_bps\"] = slip_bps\n",
    "    return df\n",
    "\n",
    "\n",
    "trades = add_slippage_columns(trades)\n",
    "\n",
    "\n",
    "# Trade value normalization\n",
    "if \"trade_value\" not in trades.columns:\n",
    "    if \"value\" in trades.columns:\n",
    "        trades[\"trade_value\"] = trades[\"value\"]\n",
    "    else:\n",
    "        # fallback if you only have shares*price\n",
    "        if \"price\" in trades.columns:\n",
    "            trades[\"trade_value\"] = trades[\"shares\"].astype(float) * trades[\"price\"].astype(float)\n",
    "\n",
    "# Audit fields are still required for cash before/after reporting\n",
    "audit_cols = [\"cash_before\", \"cash_after\"]\n",
    "missing_audit = [c for c in audit_cols if c not in trades.columns]\n",
    "if missing_audit:\n",
    "    raise ValueError(\n",
    "        f\"ERROR: Trades file is missing required audit fields: {missing_audit}. \"\n",
    "        f\"Trading engine must write these per trade.\"\n",
    "    )\n",
    "\n",
    "FIRST_EXEC_DATE = trades[\"exec_date\"].min()\n",
    "\n",
    "equity_df = pd.read_parquet(EQUITY_FILE)\n",
    "equity_df[\"date\"] = pd.to_datetime(equity_df[\"date\"])\n",
    "equity_df = equity_df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# INITIAL_CAPITAL from earliest portfolio_value (same as before)\n",
    "INITIAL_CAPITAL = float(equity_df[\"portfolio_value\"].iloc[0])\n",
    "\n",
    "universe = pd.read_parquet(UNIVERSE_FILE)\n",
    "universe[\"date\"] = pd.to_datetime(universe[\"date\"])\n",
    "universe = universe.sort_values([\"date\", \"ticker\"]).reset_index(drop=True)\n",
    "\n",
    "spy = pd.read_parquet(SPY_FILE)\n",
    "# normalize date column\n",
    "if spy.index.name in [\"Date\", \"date\", None]:\n",
    "    spy = spy.reset_index().rename(columns={\"index\": \"date\", \"Date\": \"date\"})\n",
    "spy[\"date\"] = pd.to_datetime(spy[\"date\"])\n",
    "\n",
    "# Ensure spy_close exists\n",
    "if \"spy_close\" not in spy.columns:\n",
    "    if \"Close\" in spy.columns:\n",
    "        spy[\"spy_close\"] = spy[\"Close\"]\n",
    "    else:\n",
    "        raise ValueError(\"SPY file missing spy_close/Close column.\")\n",
    "\n",
    "print(f\"Trades:       {len(trades):,}\")\n",
    "print(f\"Equity curve: {len(equity_df):,}\")\n",
    "print(f\"Universe:     {len(universe):,}\")\n",
    "print(f\"SPY rows:     {len(spy):,}\")\n",
    "\n",
    "trades_by_exec_date = {d: sub for d, sub in trades.groupby(\"exec_date\")}\n",
    "universe_by_date    = {d: sub for d, sub in universe.groupby(\"date\")}\n",
    "\n",
    "# price history by ticker for Q3=A\n",
    "px_by_ticker = {}\n",
    "for t, sub in universe.groupby(\"ticker\", sort=False):\n",
    "    sub = sub.sort_values(\"date\")\n",
    "    arr = np.zeros(len(sub), dtype=[(\"date\",\"datetime64[ns]\"), (\"px\",\"float64\")])\n",
    "    arr[\"date\"] = sub[\"date\"].values.astype(\"datetime64[ns]\")\n",
    "    arr[\"px\"]   = sub[\"close_adj\"].astype(float).values\n",
    "    px_by_ticker[t] = arr\n",
    "\n",
    "# daily = equity + spy_close\n",
    "daily = equity_df.merge(spy[[\"date\", \"spy_close\"]], on=\"date\", how=\"left\")\n",
    "daily = daily.set_index(\"date\").sort_index()\n",
    "daily[\"strat_ret\"] = daily[\"portfolio_value\"].pct_change().fillna(0)\n",
    "daily[\"spy_ret\"]   = daily[\"spy_close\"].pct_change().fillna(0)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RECONSTRUCT POSITIONS (EXECUTION-DAY snapshots)\n",
    "# ============================================================\n",
    "\n",
    "positions = {}          # live positions (ticker -> shares)\n",
    "weekly_portfolios = {}  # snapshot on each exec Thursday\n",
    "\n",
    "# Build report calendar from BOTH trades AND rankings\n",
    "# This ensures we generate reports even for no-trade weeks\n",
    "trade_exec_dates = set(trades[\"exec_date\"].unique())\n",
    "ranking_exec_dates = set(rankings[\"exec_date\"].unique())\n",
    "exec_dates = sorted(trade_exec_dates | ranking_exec_dates)\n",
    "\n",
    "for exec_dt in exec_dates:\n",
    "    todays = trades_by_exec_date.get(exec_dt, pd.DataFrame())\n",
    "\n",
    "    # apply trades in recorded order\n",
    "    for _, tr in todays.iterrows():\n",
    "        ticker = tr[\"ticker\"]\n",
    "        sh = int(tr[\"shares\"])\n",
    "        side = str(tr[\"type\"]).upper()\n",
    "\n",
    "        if side == \"BUY\":\n",
    "            positions[ticker] = positions.get(ticker, 0) + sh\n",
    "        else:\n",
    "            positions[ticker] = positions.get(ticker, 0) - sh\n",
    "            if positions[ticker] <= 0:\n",
    "                positions.pop(ticker, None)\n",
    "\n",
    "    # snapshot on Thursday OR Friday (to handle holiday-shifted executions) within range\n",
    "    if exec_dt.day_name() in (\"Thursday\", \"Friday\") and MIN_YEAR_FOR_REPORT <= exec_dt.year <= MAX_YEAR_FOR_REPORT:\n",
    "        if not positions:\n",
    "            weekly_portfolios[exec_dt] = pd.DataFrame()\n",
    "            continue\n",
    "\n",
    "        day_univ = universe_by_date.get(exec_dt, pd.DataFrame())\n",
    "        pos_df = pd.DataFrame([{\"ticker\": t, \"shares\": sh} for t, sh in positions.items()])\n",
    "        merged = pos_df.merge(day_univ, on=\"ticker\", how=\"left\")\n",
    "\n",
    "        # Q3=A: backfill missing close_adj with last-known price\n",
    "        def _fill_price(row):\n",
    "            v = row.get(\"close_adj\")\n",
    "            if not pd.isna(v):\n",
    "                return float(v)\n",
    "            t = row[\"ticker\"]\n",
    "            if t in px_by_ticker:\n",
    "                return float(fast_price_lookup(px_by_ticker[t], exec_dt))\n",
    "            return np.nan\n",
    "\n",
    "        merged[\"close_adj\"] = merged.apply(_fill_price, axis=1)\n",
    "        merged[\"market_value\"] = merged[\"shares\"] * merged[\"close_adj\"]\n",
    "        total_mv = merged[\"market_value\"].sum()\n",
    "        merged[\"weight\"] = merged[\"market_value\"] / total_mv if total_mv > 0 else 0.0\n",
    "\n",
    "        weekly_portfolios[exec_dt] = merged\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FRONT PAGE CHART\n",
    "# ============================================================\n",
    "\n",
    "chart_path = os.path.join(OUTPUT_DIR, \"equity_drawdown_overview.png\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 7), sharex=True)\n",
    "\n",
    "norm_strat = daily[\"portfolio_value\"] / daily[\"portfolio_value\"].iloc[0]\n",
    "norm_spy   = daily[\"spy_close\"] / daily[\"spy_close\"].iloc[0]\n",
    "\n",
    "ax1.plot(daily.index, norm_strat, label=\"Strategy\", color=\"blue\")\n",
    "ax1.plot(daily.index, norm_spy,   label=\"SPY\",      color=\"orange\")\n",
    "ax1.set_title(\"Strategy vs SPY – Full Period\")\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "dd = daily[\"portfolio_value\"] / daily[\"portfolio_value\"].cummax() - 1\n",
    "ax2.plot(daily.index, dd, color=\"red\")\n",
    "ax2.set_ylabel(\"Drawdown\")\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(chart_path, dpi=150)\n",
    "plt.close(fig)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PDF STYLES\n",
    "# ============================================================\n",
    "\n",
    "styles = getSampleStyleSheet()\n",
    "styles.add(ParagraphStyle(name=\"Small\",  fontSize=8, leading=9))\n",
    "styles.add(ParagraphStyle(name=\"Tiny\",   fontSize=7, leading=8))\n",
    "styles.add(ParagraphStyle(name=\"Header\", fontSize=12, leading=14, spaceAfter=6, spaceBefore=6))\n",
    "styles.add(ParagraphStyle(name=\"TitleBig\", fontSize=16, leading=20, spaceAfter=10))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PDF SETUP\n",
    "# ============================================================\n",
    "\n",
    "pdf_name = f\"weekly_trading_logs_{datetime.now():%Y%m%d-%H%M%S}.pdf\"\n",
    "pdf_path = os.path.join(OUTPUT_DIR, pdf_name)\n",
    "\n",
    "doc = SimpleDocTemplate(\n",
    "    pdf_path,\n",
    "    pagesize=landscape(letter),\n",
    "    rightMargin=36, leftMargin=36,\n",
    "    topMargin=36,  bottomMargin=36,\n",
    ")\n",
    "\n",
    "story = []\n",
    "\n",
    "# Front page\n",
    "story.append(Paragraph(\"Weekly Trading Logs\", styles[\"TitleBig\"]))\n",
    "story.append(Paragraph(f\"Generated: {datetime.now().isoformat(timespec='seconds')}\", styles[\"Small\"]))\n",
    "story.append(Spacer(1, 0.2 * inch))\n",
    "story.append(Image(chart_path, width=9.0 * inch, height=4.5 * inch))\n",
    "story.append(PageBreak())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TABLE HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def _fmt(v, col=None):\n",
    "    if v is None:\n",
    "        return \"N/A\"\n",
    "\n",
    "    # Handle NaN / NaT cleanly\n",
    "    if isinstance(v, (float, np.floating)) and np.isnan(v):\n",
    "        return \"N/A\"\n",
    "    if isinstance(v, (pd.Timestamp, np.datetime64)):\n",
    "        return str(pd.Timestamp(v).date())\n",
    "\n",
    "    # Numbers\n",
    "    if isinstance(v, (int, np.integer)):\n",
    "        return str(int(v))\n",
    "\n",
    "    if isinstance(v, (float, np.floating)):\n",
    "        # bps\n",
    "        if col in (\"slippage_bps\",):\n",
    "            return f\"{float(v):,.1f}\"\n",
    "\n",
    "        # money-ish\n",
    "        if col in (\n",
    "            \"trade_value\", \"value\",\n",
    "            \"cash_before\", \"cash_after\",\n",
    "            \"portfolio_after\", \"equity_after\",\n",
    "            \"market_value\", \"slippage_dollars\",\n",
    "            \"target_value\",\n",
    "        ):\n",
    "            return f\"{float(v):,.2f}\"\n",
    "\n",
    "        # prices\n",
    "        if col in (\n",
    "            \"price\", \"signal_close_adj\", \"exec_open_adj\",\n",
    "            \"close_adj\", \"spy_close\", \"spy_ma200\", \"ma100\",\n",
    "        ):\n",
    "            return f\"{float(v):,.2f}\"\n",
    "\n",
    "        # per-share slippage or other small-ish metrics\n",
    "        if col in (\"slippage_per_share\",):\n",
    "            return f\"{float(v):0.4f}\"\n",
    "\n",
    "        # weights (as percentages)\n",
    "        if col in (\"target_weight\", \"current_weight\", \"weight\", \"raw_weight\", \"capped_weight\"):\n",
    "            return f\"{float(v)*100:0.2f}%\"\n",
    "\n",
    "        # default float formatting\n",
    "        return f\"{float(v):0.4f}\"\n",
    "\n",
    "    return str(v)\n",
    "\n",
    "\n",
    "\n",
    "def make_trade_table(df: pd.DataFrame, title: str):\n",
    "    story.append(Paragraph(title, styles[\"Small\"]))\n",
    "\n",
    "    if df.empty:\n",
    "        story.append(Paragraph(\"None\", styles[\"Tiny\"]))\n",
    "        story.append(Spacer(1, 0.1 * inch))\n",
    "        return\n",
    "\n",
    "    # Prefer showing both signal and execution prices when present\n",
    "    cols_pref = [\n",
    "        \"ticker\",\n",
    "        \"type\",\n",
    "        \"shares\",\n",
    "        \"signal_date\",\n",
    "        \"exec_date\",\n",
    "        \"signal_close_adj\",\n",
    "        \"exec_open_adj\",\n",
    "        \"slippage_per_share\",\n",
    "        \"slippage_dollars\",\n",
    "        \"slippage_bps\",\n",
    "        \"price\",         # (your engine may set this = exec_open_adj)\n",
    "        \"trade_value\",\n",
    "        \"reason\",\n",
    "        \"slope_rank_within_top\",\n",
    "        \"spy_above_200dma\",\n",
    "        \"cash_before\",\n",
    "        \"cash_after\",\n",
    "    ]\n",
    "\n",
    "    cols = [c for c in cols_pref if c in df.columns]\n",
    "\n",
    "    # sort by rank if available\n",
    "    if \"slope_rank_within_top\" in df.columns:\n",
    "        df = df.sort_values(\"slope_rank_within_top\", na_position=\"last\")\n",
    "\n",
    "    data = [cols]\n",
    "    for _, row in df[cols].iterrows():\n",
    "        data.append([_fmt(row[c], c) for c in cols])\n",
    "\n",
    "    tbl = Table(data, hAlign=\"LEFT\", repeatRows=1)\n",
    "    tbl.setStyle(TableStyle([\n",
    "        (\"BACKGROUND\",(0,0),(-1,0),colors.lightgrey),\n",
    "        (\"GRID\",(0,0),(-1,-1),0.25,colors.grey),\n",
    "        (\"FONTNAME\",(0,0),(-1,-1),\"Helvetica\"),\n",
    "        (\"FONTSIZE\",(0,0),(-1,-1),6.5),\n",
    "        (\"ALIGN\",(1,1),(-1,-1),\"RIGHT\"),\n",
    "    ]))\n",
    "    story.append(tbl)\n",
    "    story.append(Spacer(1, 0.12 * inch))\n",
    "\n",
    "\n",
    "def make_ranked_stocks_table(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    NEW: Create table showing all ranked stocks with trade status and reasons\n",
    "    \"\"\"\n",
    "    story.append(Paragraph(\"All Ranked Stocks (Top Percentile)\", styles[\"Small\"]))\n",
    "\n",
    "    if df.empty:\n",
    "        story.append(Paragraph(\"No stocks ranked this week.\", styles[\"Tiny\"]))\n",
    "        story.append(Spacer(1, 0.1 * inch))\n",
    "        return\n",
    "\n",
    "    # Select columns to display\n",
    "    cols_pref = [\n",
    "        \"slope_rank\",\n",
    "        \"ticker\",\n",
    "        \"traded_flag\",\n",
    "        \"no_trade_reason\",\n",
    "        \"slope_adj\",\n",
    "        \"target_weight\",\n",
    "        \"current_weight\",\n",
    "        \"target_shares\",\n",
    "        \"current_shares\",\n",
    "        \"close_adj\",\n",
    "    ]\n",
    "\n",
    "    cols = [c for c in cols_pref if c in df.columns]\n",
    "\n",
    "    # Sort by rank\n",
    "    df = df.sort_values(\"slope_rank\", na_position=\"last\")\n",
    "\n",
    "    data = [cols]\n",
    "    for _, row in df[cols].iterrows():\n",
    "        row_out = []\n",
    "        for c in cols:\n",
    "            v = row[c]\n",
    "            # Special handling for no_trade_reason - show empty for traded stocks\n",
    "            if c == \"no_trade_reason\" and row.get(\"traded_flag\") == \"TRADED\":\n",
    "                row_out.append(\"\")\n",
    "            else:\n",
    "                row_out.append(_fmt(v, c))\n",
    "        data.append(row_out)\n",
    "\n",
    "    tbl = Table(data, hAlign=\"LEFT\", repeatRows=1)\n",
    "    tbl.setStyle(TableStyle([\n",
    "        (\"BACKGROUND\",(0,0),(-1,0),colors.lightgrey),\n",
    "        (\"GRID\",(0,0),(-1,-1),0.25,colors.grey),\n",
    "        (\"FONTNAME\",(0,0),(-1,-1),\"Helvetica\"),\n",
    "        (\"FONTSIZE\",(0,0),(-1,-1),6),\n",
    "        (\"ALIGN\",(1,1),(-1,-1),\"RIGHT\"),\n",
    "        # Highlight traded vs not traded rows\n",
    "        (\"TEXTCOLOR\",(0,1),(-1,-1),colors.black),\n",
    "    ]))\n",
    "    \n",
    "    story.append(tbl)\n",
    "    story.append(Spacer(1, 0.12 * inch))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SLIPPAGE BREAKDOWN HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def slippage_breakdown(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dict with:\n",
    "      net_dollars: sum(slippage_dollars) (positive = cost, negative = improvement)\n",
    "      gross_cost_dollars: sum of positive slippage_dollars (cost only)\n",
    "      gross_improve_dollars: -sum of negative slippage_dollars (positive magnitude)\n",
    "      net_bps: notional-weighted bps using exec price (exec_open_adj else price)\n",
    "    \"\"\"\n",
    "    if df is None or df.empty or \"slippage_dollars\" not in df.columns:\n",
    "        return {\n",
    "            \"net_dollars\": np.nan,\n",
    "            \"gross_cost_dollars\": np.nan,\n",
    "            \"gross_improve_dollars\": np.nan,\n",
    "            \"net_bps\": np.nan,\n",
    "        }\n",
    "\n",
    "    slip = pd.to_numeric(df[\"slippage_dollars\"], errors=\"coerce\")\n",
    "\n",
    "    # Notional proxy for bps\n",
    "    if \"exec_open_adj\" in df.columns:\n",
    "        px = pd.to_numeric(df[\"exec_open_adj\"], errors=\"coerce\")\n",
    "    elif \"price\" in df.columns:\n",
    "        px = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "    else:\n",
    "        px = pd.Series(np.nan, index=df.index)\n",
    "\n",
    "    sh = pd.to_numeric(df[\"shares\"], errors=\"coerce\")\n",
    "    notional = (px * sh).abs()\n",
    "\n",
    "    net = float(slip.sum(skipna=True))\n",
    "\n",
    "    pos = slip.where(slip > 0, 0.0)\n",
    "    neg = slip.where(slip < 0, 0.0)\n",
    "\n",
    "    gross_cost = float(pos.sum(skipna=True))\n",
    "    gross_improve = float((-neg).sum(skipna=True))  # positive magnitude\n",
    "\n",
    "    tot_notional = float(notional.sum(skipna=True))\n",
    "    net_bps = (net / tot_notional) * 1e4 if tot_notional > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"net_dollars\": net,\n",
    "        \"gross_cost_dollars\": gross_cost,\n",
    "        \"gross_improve_dollars\": gross_improve,\n",
    "        \"net_bps\": net_bps,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# WEEKLY REPORT LOOP (EXECUTION DAY)\n",
    "# ============================================================\n",
    "\n",
    "weekly_dates = sorted(weekly_portfolios.keys())\n",
    "prev_week_port_value = None\n",
    "\n",
    "for exec_dt in weekly_dates:\n",
    "    if exec_dt < START_TRADING or not (MIN_YEAR_FOR_REPORT <= exec_dt.year <= MAX_YEAR_FOR_REPORT):\n",
    "        continue\n",
    "\n",
    "    # trades executed this day\n",
    "    day_trades = trades_by_exec_date.get(exec_dt, pd.DataFrame()).copy()\n",
    "\n",
    "    # infer the signal date for this execution day (normally one unique)\n",
    "    # infer the signal date for this execution day\n",
    "    signal_dt = None\n",
    "    \n",
    "    # First try from trades\n",
    "    if not day_trades.empty and \"signal_date\" in day_trades.columns:\n",
    "        uniq = sorted(pd.to_datetime(day_trades[\"signal_date\"]).unique())\n",
    "        if len(uniq) >= 1:\n",
    "            signal_dt = pd.Timestamp(uniq[0])\n",
    "    \n",
    "    # Fallback: look up signal_date from rankings if no trades\n",
    "    if signal_dt is None:\n",
    "        ranking_match = rankings[rankings[\"exec_date\"] == exec_dt]\n",
    "        if not ranking_match.empty and \"signal_date\" in ranking_match.columns:\n",
    "            signal_dt = pd.Timestamp(ranking_match[\"signal_date\"].iloc[0])\n",
    "\n",
    "    # Get rankings for this signal date\n",
    "    day_rankings = pd.DataFrame()\n",
    "    if signal_dt is not None and signal_dt in rankings_by_signal_date:\n",
    "        day_rankings = rankings_by_signal_date[signal_dt].copy()\n",
    "\n",
    "    # Header\n",
    "    # Header - indicate if no trades this week\n",
    "    has_trades = not day_trades.empty\n",
    "    header_suffix = \"\" if has_trades else \" (NO TRADES)\"\n",
    "    story.append(Paragraph(f\"Week of {exec_dt.date()} (EXECUTION DAY){header_suffix} — Year {exec_dt.year}\", styles[\"Header\"]))\n",
    "    if signal_dt is not None:\n",
    "        story.append(Paragraph(f\"Signal day: {signal_dt.date()} ({signal_dt.day_name()})\", styles[\"Small\"]))\n",
    "    story.append(Spacer(1, 0.08 * inch))\n",
    "\n",
    "    # ======================================================\n",
    "    # SPY regime message (use SIGNAL DAY if available, else EXEC DAY)\n",
    "    # ======================================================\n",
    "    spy_day = signal_dt if signal_dt is not None else exec_dt\n",
    "\n",
    "    spy_row = spy[spy[\"date\"] == spy_day]\n",
    "    if not spy_row.empty:\n",
    "        spy_close = float(spy_row[\"spy_close\"].iloc[0])\n",
    "        # many regime files store 200DMA under a specific name; try common options\n",
    "        spy_ma200 = None\n",
    "        for c in (\"spy_ma200\", \"ma200\", \"MA200\", \"spy_200dma\", \"spy_ma_200\"):\n",
    "            if c in spy_row.columns:\n",
    "                v = spy_row[c].iloc[0]\n",
    "                if not pd.isna(v):\n",
    "                    spy_ma200 = float(v)\n",
    "                    break\n",
    "    else:\n",
    "        spy_close, spy_ma200 = np.nan, None\n",
    "\n",
    "    if spy_ma200 is not None and not np.isnan(spy_ma200) and not np.isnan(spy_close):\n",
    "        if spy_close >= spy_ma200:\n",
    "            spy_status_msg = \"<b>SPY is ABOVE 200-Day Moving Average — Buying Allowed (signal day)</b>\"\n",
    "        else:\n",
    "            spy_status_msg = \"<b>SPY is BELOW 200-Day Moving Average — No Buying Stock (signal day)</b>\"\n",
    "    else:\n",
    "        spy_status_msg = \"<b>SPY 200DMA unavailable for this date</b>\"\n",
    "\n",
    "    story.append(Paragraph(spy_status_msg, styles[\"Small\"]))\n",
    "    story.append(Spacer(1, 0.05 * inch))\n",
    "    story.append(Paragraph(\n",
    "        f\"SPY Date: {spy_day.date()} | SPY Close: {_fmt(spy_close, 'spy_close')} | SPY 200DMA: \"\n",
    "        f\"{'N/A' if spy_ma200 is None else _fmt(spy_ma200, 'spy_ma200')}\",\n",
    "        styles[\"Small\"]\n",
    "    ))\n",
    "    story.append(Spacer(1, 0.12 * inch))\n",
    "\n",
    "    # ======================================================\n",
    "    # YTD PERFORMANCE (use execution date vs daily equity curve)\n",
    "    # ======================================================\n",
    "    stats = ytd_stats_for_date(exec_dt, daily)\n",
    "    \n",
    "    # ======================================================\n",
    "    # YTD SLIPPAGE (net + gross cost + gross improvement), through exec_dt\n",
    "    # ======================================================\n",
    "    ystart = max(pd.Timestamp(exec_dt.year, 1, 1), FIRST_EXEC_DATE)\n",
    "    ytd_tr = trades[(trades[\"exec_date\"] >= ystart) & (trades[\"exec_date\"] <= exec_dt)]\n",
    "\n",
    "    yb = slippage_breakdown(ytd_tr)\n",
    "    ytd_slip_net_dollars = yb[\"net_dollars\"]\n",
    "    ytd_slip_gross_cost_dollars = yb[\"gross_cost_dollars\"]\n",
    "    ytd_slip_gross_improve_dollars = yb[\"gross_improve_dollars\"]\n",
    "    ytd_slip_net_bps = yb[\"net_bps\"]\n",
    "\n",
    "    perf_data = [\n",
    "        [\"Metric\", \"Strategy\", \"SPY\"],\n",
    "        [\"YTD Return\",\n",
    "         f\"{stats['strat_ret']*100:5.2f}%\"  if not np.isnan(stats['strat_ret']) else \"N/A\",\n",
    "         f\"{stats['spy_ret']*100:5.2f}%\"    if not np.isnan(stats['spy_ret'])   else \"N/A\"],\n",
    "        [\"YTD Max Drawdown\",\n",
    "         f\"{stats['strat_maxdd']*100:5.2f}%\" if not np.isnan(stats['strat_maxdd']) else \"N/A\",\n",
    "         f\"{stats['spy_maxdd']*100:5.2f}%\"   if not np.isnan(stats['spy_maxdd'])   else \"N/A\"],\n",
    "        [\"YTD Sharpe\",\n",
    "         f\"{stats['strat_sharpe']:5.2f}\" if not np.isnan(stats['strat_sharpe']) else \"N/A\",\n",
    "         f\"{stats['spy_sharpe']:5.2f}\"   if not np.isnan(stats['spy_sharpe'])   else \"N/A\"],\n",
    "        [\"YTD Sortino\",\n",
    "         f\"{stats['strat_sortino']:5.2f}\" if not np.isnan(stats['strat_sortino']) else \"N/A\",\n",
    "         f\"{stats['spy_sortino']:5.2f}\"   if not np.isnan(stats['spy_sortino'])   else \"N/A\"],\n",
    "        [\"YTD Calmar\",\n",
    "         f\"{stats['strat_calmar']:5.2f}\" if not np.isnan(stats['strat_calmar']) else \"N/A\",\n",
    "         f\"{stats['spy_calmar']:5.2f}\"   if not np.isnan(stats['spy_calmar'])   else \"N/A\"],\n",
    "        [\"YTD Slippage (net $)\",\n",
    "         \"N/A\" if np.isnan(ytd_slip_net_dollars) else f\"{ytd_slip_net_dollars:,.2f}\",\n",
    "         \"N/A\"],\n",
    "        [\"YTD Slippage (gross cost $)\",\n",
    "         \"N/A\" if np.isnan(ytd_slip_gross_cost_dollars) else f\"{ytd_slip_gross_cost_dollars:,.2f}\",\n",
    "         \"N/A\"],\n",
    "        [\"YTD Slippage (gross improve $)\",\n",
    "         \"N/A\" if np.isnan(ytd_slip_gross_improve_dollars) else f\"{ytd_slip_gross_improve_dollars:,.2f}\",\n",
    "         \"N/A\"],\n",
    "        [\"YTD Slippage (net bps)\",\n",
    "         \"N/A\" if np.isnan(ytd_slip_net_bps) else f\"{ytd_slip_net_bps:,.1f}\",\n",
    "         \"N/A\"],\n",
    "    ]\n",
    "\n",
    "    t = Table(perf_data, hAlign=\"LEFT\")\n",
    "    t.setStyle(TableStyle([\n",
    "        (\"BACKGROUND\",(0,0),(-1,0),colors.lightgrey),\n",
    "        (\"GRID\",(0,0),(-1,-1),0.25,colors.grey),\n",
    "        (\"FONTNAME\",(0,0),(-1,-1),\"Helvetica\"),\n",
    "        (\"FONTSIZE\",(0,0),(-1,-1),8),\n",
    "        (\"ALIGN\",(1,1),(-1,-1),\"RIGHT\"),\n",
    "    ]))\n",
    "    story.append(t)\n",
    "    story.append(Spacer(1, 0.15 * inch))\n",
    "\n",
    "    # ======================================================\n",
    "    # BUILD RANK MAP (use SIGNAL DAY universe if available)\n",
    "    # ======================================================\n",
    "    rank_map = {}\n",
    "    day_univ_signal = universe_by_date.get(signal_dt, pd.DataFrame()) if signal_dt is not None else pd.DataFrame()\n",
    "\n",
    "    if not day_univ_signal.empty and \"slope_adj\" in day_univ_signal.columns:\n",
    "        rankable = day_univ_signal[day_univ_signal[\"slope_adj\"].notna()].copy()\n",
    "        if not rankable.empty:\n",
    "            rankable = rankable.sort_values(\"slope_adj\", ascending=False)\n",
    "            cutoff = rankable[\"slope_adj\"].quantile(TOP_PERCENTILE)\n",
    "            top_slice = rankable[rankable[\"slope_adj\"] >= cutoff].copy()\n",
    "            if not top_slice.empty:\n",
    "                top_slice = top_slice.sort_values(\"slope_adj\", ascending=False)\n",
    "                top_slice[\"slope_rank_within_top\"] = np.arange(1, len(top_slice) + 1)\n",
    "                rank_map = dict(zip(top_slice[\"ticker\"], top_slice[\"slope_rank_within_top\"]))\n",
    "\n",
    "    # ======================================================\n",
    "    # ENRICH DAY TRADES WITH SIGNAL-DAY UNIVERSE FIELDS (optional)\n",
    "    # ======================================================\n",
    "    if not day_trades.empty and not day_univ_signal.empty:\n",
    "        # merge on ticker only (dates differ: signal_date vs exec_date)\n",
    "        enrich_cols = [c for c in day_univ_signal.columns if c not in (\"date\",)]\n",
    "        day_trades = day_trades.merge(\n",
    "            day_univ_signal[[\"ticker\"] + [c for c in enrich_cols if c != \"ticker\"]],\n",
    "            on=\"ticker\",\n",
    "            how=\"left\",\n",
    "            suffixes=(\"\", \"_u\")\n",
    "        )\n",
    "\n",
    "    # If file doesn't already carry rank, map it\n",
    "    if not day_trades.empty and \"slope_rank_within_top\" in day_trades.columns:\n",
    "        # keep existing if present; fill missing from rank_map\n",
    "        day_trades[\"slope_rank_within_top\"] = day_trades[\"slope_rank_within_top\"].fillna(\n",
    "            day_trades[\"ticker\"].map(rank_map)\n",
    "        )\n",
    "    elif not day_trades.empty:\n",
    "        day_trades[\"slope_rank_within_top\"] = day_trades[\"ticker\"].map(rank_map)\n",
    "\n",
    "    buys  = day_trades[day_trades[\"type\"].str.upper() == \"BUY\"]  if (not day_trades.empty and \"type\" in day_trades) else pd.DataFrame()\n",
    "    sells = day_trades[day_trades[\"type\"].str.upper() == \"SELL\"] if (not day_trades.empty and \"type\" in day_trades) else pd.DataFrame()\n",
    "    \n",
    "    # ======================================================\n",
    "    # WEEKLY SLIPPAGE (net + gross cost + gross improvement)\n",
    "    # ======================================================\n",
    "    wb = slippage_breakdown(day_trades)\n",
    "    weekly_slip_net_dollars = wb[\"net_dollars\"]\n",
    "    weekly_slip_gross_cost_dollars = wb[\"gross_cost_dollars\"]\n",
    "    weekly_slip_gross_improve_dollars = wb[\"gross_improve_dollars\"]\n",
    "    weekly_slip_net_bps = wb[\"net_bps\"]\n",
    "\n",
    "    # ======================================================\n",
    "    # AUDITED CASH (from this EXECUTION DAY trades)\n",
    "    # ======================================================\n",
    "    # ======================================================\n",
    "    # AUDITED CASH (from this EXECUTION DAY trades)\n",
    "    # ======================================================\n",
    "    if not day_trades.empty:\n",
    "        cash_before = float(day_trades[\"cash_before\"].iloc[0])\n",
    "        cash_after  = float(day_trades[\"cash_after\"].iloc[-1])\n",
    "\n",
    "        # On the first execution day, enforce initial capital as \"cash_before\"\n",
    "        if exec_dt == FIRST_EXEC_DATE:\n",
    "            cash_before = INITIAL_CAPITAL\n",
    "\n",
    "        cash_delta = cash_after - cash_before\n",
    "    else:\n",
    "        # No trades executed this day; use equity_df cash if available\n",
    "        if exec_dt in daily.index and \"cash\" in daily.columns:\n",
    "            cash_before = cash_after = float(daily.loc[exec_dt, \"cash\"])\n",
    "            cash_delta = 0.0\n",
    "        else:\n",
    "            # Fallback: use previous week's cash or look in equity curve\n",
    "            equity_row = equity_df[equity_df[\"date\"] == exec_dt]\n",
    "            if not equity_row.empty and \"cash\" in equity_row.columns:\n",
    "                cash_before = cash_after = float(equity_row[\"cash\"].iloc[0])\n",
    "                cash_delta = 0.0\n",
    "            else:\n",
    "                cash_before = cash_after = cash_delta = np.nan\n",
    "\n",
    "    # ======================================================\n",
    "    # WEEKLY SUMMARY (execution-day end-of-day values)\n",
    "    # ======================================================\n",
    "    week_port_value = float(daily.loc[exec_dt, \"portfolio_value\"]) if exec_dt in daily.index else np.nan\n",
    "\n",
    "    # securities AFTER trades from reconstructed holdings\n",
    "    week_port = weekly_portfolios.get(exec_dt, pd.DataFrame())\n",
    "    sec_after = float(week_port[\"market_value\"].sum()) if (not week_port.empty and \"market_value\" in week_port.columns) else np.nan\n",
    "\n",
    "    cash_status = \"OK\"\n",
    "    if not np.isnan(cash_after):\n",
    "        if cash_after < 0:\n",
    "            cash_status = \"NEGATIVE (AUDIT)\"\n",
    "        elif cash_after < LOW_CASH_FLOOR:\n",
    "            cash_status = f\"LOW CASH ( < {LOW_CASH_FLOOR:,.0f} )\"\n",
    "\n",
    "    if (prev_week_port_value is not None and\n",
    "        not np.isnan(prev_week_port_value) and\n",
    "        not np.isnan(week_port_value) and\n",
    "        prev_week_port_value != 0):\n",
    "        weekly_ret    = week_port_value / prev_week_port_value - 1.0\n",
    "        weekly_change = week_port_value - prev_week_port_value\n",
    "    else:\n",
    "        weekly_ret = weekly_change = np.nan\n",
    "\n",
    "    week_buys_value  = float(buys[\"trade_value\"].sum())  if (\"trade_value\" in buys.columns and not buys.empty) else 0.0\n",
    "    week_sells_value = float(sells[\"trade_value\"].sum()) if (\"trade_value\" in sells.columns and not sells.empty) else 0.0\n",
    "\n",
    "    summary_data = [\n",
    "        [\"Item\",                          \"Amount\"],\n",
    "        [\"--- AUDITED CASH (EXECUTION DAY) ---\", \"\"],\n",
    "        [\"Cash BEFORE first trade\",       \"N/A\" if np.isnan(cash_before) else f\"{cash_before:,.2f}\"],\n",
    "        [\"Cash AFTER last trade\",         \"N/A\" if np.isnan(cash_after)  else f\"{cash_after:,.2f}\"],\n",
    "        [\"Cash Δ (After - Before)\",       \"N/A\" if np.isnan(cash_delta)  else f\"{cash_delta:,.2f}\"],\n",
    "\n",
    "        [\"\", \"\"],\n",
    "        [\"--- PORTFOLIO (AFTER TRADES, END OF DAY) ---\", \"\"],\n",
    "        [\"Total Portfolio Value\",         \"N/A\" if np.isnan(week_port_value) else f\"{week_port_value:,.2f}\"],\n",
    "        [\"Total Securities Value\",        \"N/A\" if np.isnan(sec_after)       else f\"{sec_after:,.2f}\"],\n",
    "        [\"Total Cash Value (Audited)\",    \"N/A\" if np.isnan(cash_after)      else f\"{cash_after:,.2f}\"],\n",
    "        [\"Cash Status\",                   cash_status],\n",
    "\n",
    "        [\"\", \"\"],\n",
    "        [\"Weekly Portfolio Return\",       \"N/A\" if np.isnan(weekly_ret)     else f\"{weekly_ret*100:,.2f}%\"],\n",
    "        [\"Weekly P&L (Δ value)\",          \"N/A\" if np.isnan(weekly_change)  else f\"{weekly_change:,.2f}\"],\n",
    "        \n",
    "        [\"Weekly Slippage (net $)\",            \"N/A\" if np.isnan(weekly_slip_net_dollars) else f\"{weekly_slip_net_dollars:,.2f}\"],\n",
    "        [\"Weekly Slippage (gross cost $)\",     \"N/A\" if np.isnan(weekly_slip_gross_cost_dollars) else f\"{weekly_slip_gross_cost_dollars:,.2f}\"],\n",
    "        [\"Weekly Slippage (gross improve $)\",  \"N/A\" if np.isnan(weekly_slip_gross_improve_dollars) else f\"{weekly_slip_gross_improve_dollars:,.2f}\"],\n",
    "        [\"Weekly Slippage (net bps)\",          \"N/A\" if np.isnan(weekly_slip_net_bps) else f\"{weekly_slip_net_bps:,.1f}\"],\n",
    "\n",
    "        [\"Total Dollar Buys\",             f\"{week_buys_value:,.2f}\"],\n",
    "        [\"Total Dollar Sells\",            f\"{week_sells_value:,.2f}\"],\n",
    "        [\"Net Buy/Sell Flow\",             f\"{(week_buys_value - week_sells_value):,.2f}\"],\n",
    "    ]\n",
    "\n",
    "    summary_tbl = Table(summary_data, hAlign=\"LEFT\")\n",
    "    summary_tbl.setStyle(TableStyle([\n",
    "        (\"BACKGROUND\",(0,0),(-1,0),colors.lightgrey),\n",
    "        (\"GRID\",(0,0),(-1,-1),0.25,colors.grey),\n",
    "        (\"FONTNAME\",(0,0),(-1,-1),\"Helvetica\"),\n",
    "        (\"FONTSIZE\",(0,0),(-1,-1),8),\n",
    "        (\"ALIGN\",(1,1),(-1,-1),\"RIGHT\"),\n",
    "    ]))\n",
    "    story.append(summary_tbl)\n",
    "    story.append(Spacer(1, 0.15 * inch))\n",
    "\n",
    "    prev_week_port_value = week_port_value\n",
    "\n",
    "    # ======================================================\n",
    "    # NEW: ALL RANKED STOCKS TABLE (BEFORE FILTERS)\n",
    "    # ======================================================\n",
    "    make_ranked_stocks_table(day_rankings)\n",
    "\n",
    "    # ======================================================\n",
    "    # TRADES TABLES (NOW INCLUDE signal/execution prices)\n",
    "    # ======================================================\n",
    "    make_trade_table(sells, \"Sells (shows signal_close_adj + exec_open_adj)\")\n",
    "    make_trade_table(buys,  \"Buys (shows signal_close_adj + exec_open_adj)\")\n",
    "\n",
    "    # ======================================================\n",
    "    # PORTFOLIO TABLE (post-trade holdings valued at EXEC CLOSE)\n",
    "    # ======================================================\n",
    "    story.append(Paragraph(\"Current Portfolio (post-trade, valued at execution-day close)\", styles[\"Small\"]))\n",
    "\n",
    "    port = weekly_portfolios.get(exec_dt, pd.DataFrame()).copy()\n",
    "    if port.empty:\n",
    "        story.append(Paragraph(\"No open positions.\", styles[\"Tiny\"]))\n",
    "        story.append(PageBreak())\n",
    "        continue\n",
    "\n",
    "    # Add slope ranks using SIGNAL rank_map (more meaningful for that week's selection)\n",
    "    if rank_map:\n",
    "        port[\"slope_rank_within_top\"] = port[\"ticker\"].map(rank_map)\n",
    "\n",
    "    port_cols = [\n",
    "        \"ticker\",\"shares\",\"close_adj\",\"market_value\",\"weight\",\n",
    "        \"slope_rank_within_top\",\"slope_adj\",\"above_ma100\",\n",
    "        \"no_big_jump_90\",\"ma100\"\n",
    "    ]\n",
    "    port_cols = [c for c in port_cols if c in port.columns]\n",
    "\n",
    "    if \"slope_rank_within_top\" in port.columns:\n",
    "        port = port.sort_values(\"slope_rank_within_top\", na_position=\"last\")\n",
    "\n",
    "    data = [port_cols]\n",
    "    for _, row in port[port_cols].iterrows():\n",
    "        row_out = []\n",
    "        for c in port_cols:\n",
    "            v = row[c]\n",
    "            if c == \"weight\" and not pd.isna(v):\n",
    "                row_out.append(f\"{float(v)*100:0.2f}%\")\n",
    "            else:\n",
    "                row_out.append(_fmt(v, c))\n",
    "        data.append(row_out)\n",
    "\n",
    "    tbl = Table(data, hAlign=\"LEFT\", repeatRows=1)\n",
    "    tbl.setStyle(TableStyle([\n",
    "        (\"BACKGROUND\",(0,0),(-1,0),colors.lightgrey),\n",
    "        (\"GRID\",(0,0),(-1,-1),0.25,colors.grey),\n",
    "        (\"FONTNAME\",(0,0),(-1,-1),\"Helvetica\"),\n",
    "        (\"FONTSIZE\",(0,0),(-1,-1),6.5),\n",
    "        (\"ALIGN\",(1,1),(-1,-1),\"RIGHT\"),\n",
    "    ]))\n",
    "\n",
    "    story.append(tbl)\n",
    "    story.append(PageBreak())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# BUILD & SAVE PDF\n",
    "# ============================================================\n",
    "\n",
    "doc.build(story)\n",
    "\n",
    "print(\"=== COMPLETE ===\")\n",
    "print(f\"Weekly trading log PDF saved → {pdf_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
