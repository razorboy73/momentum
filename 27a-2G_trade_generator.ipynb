{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37662d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LIVE TRADE GENERATOR (BACKTEST-CONGRUENT, OPTION A) ===\n",
      "Execution pricing: WED_CLOSE_AS_THU_OPEN_ESTIMATE\n",
      "Start trading:     2025-12-17\n",
      "Rebalance day:     Wednesday\n",
      "Max position cap:  12%\n",
      "SPY Regime Confirmation Period: 1 day(s)\n",
      "===============================================================\n",
      "\n",
      "Loaded universe: 3,591,462 rows\n",
      "Universe with ATR20 merged: 3,591,462 rows\n",
      "\n",
      "SPY Regime Statistics:\n",
      "  Raw regime:       5,107 bull days, 1,935 bear days\n",
      "  Confirmed regime: 5,107 bull days, 1,935 bear days\n",
      "\n",
      "Loaded AUTHORITATIVE live portfolio state (from ./27a-2G_live_trading/live_portfolio.csv):\n",
      "  Cash:      345,000.00\n",
      "  Positions: 0\n",
      "\n",
      "Eligible rebalance weeks: 3\n",
      "\n",
      "\n",
      "=== DONE ===\n",
      "Weeks processed:   3\n",
      "Weeks written:     3\n",
      "Orders generated:  9\n",
      "Drops:\n",
      "  missing_price:        0\n",
      "  min_trade:            0\n",
      "  cash_floor_buy:       0\n",
      "  spy_regime_buy:       0\n",
      "  min_new_weight:       0\n",
      "  deduped_existing:     0\n",
      "\n",
      "Outputs:\n",
      "  Master planned trades:        ./27a-2G_live_trading/master_trades.csv\n",
      "  Master rankings:              ./27a-2G_live_trading/master_rankings.csv\n",
      "  Weekly planned trades dir:    ./27a-2G_live_trading/weekly_trades\n",
      "  Weekly rankings dir:          ./27a-2G_live_trading/weekly_rankings\n",
      "  Weekly planned portfolios:    ./27a-2G_live_trading/weekly_portfolios\n",
      "  Planned live portfolio file:  ./27a-2G_live_trading/planned_live_portfolio.csv\n",
      "\n",
      "NOTE: ./27a-2G_live_trading/live_portfolio.csv was NOT modified (authoritative executions only).\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# LIVE TRADE GENERATOR (OPTION A)\n",
    "# ============================================================\n",
    "# PURPOSE\n",
    "# - Generate \"planned trades\" off Wednesday close (signal day)\n",
    "# - WITHOUT requiring Thursday open data\n",
    "# - WITHOUT mutating the authoritative live portfolio file\n",
    "#\n",
    "# KEY RULES\n",
    "# 1) live_portfolio.csv is AUTHORITATIVE and should ONLY be updated by reconciliation (executed fills)\n",
    "# 2) This script writes PLANNED outputs only:\n",
    "#    - weekly_trades_signal_YYYYMMDD.csv\n",
    "#    - planned_portfolio_signal_YYYYMMDD.csv\n",
    "#    - master_trades.csv (planned trades ledger; de-duped)\n",
    "#    - weekly_rankings_signal_YYYYMMDD.csv (NEW)\n",
    "#    - master_rankings.csv (NEW)\n",
    "#\n",
    "# OPTION A EXECUTION PRICING\n",
    "# - We do NOT know Thursday open on Wednesday night.\n",
    "# - So we estimate execution price = Wednesday close_adj (signal-day close).\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG (from your live settings)\n",
    "# ============================================================\n",
    "\n",
    "UNIVERSE_FILE   = \"./12-tradable_sp500_universe/12-tradable_sp500_universe.parquet\"\n",
    "ATR20_DIR       = \"./4-ATR20_adjusted_All_Prices\"\n",
    "SPY_FILE        = \"./8-SPY_200DMA_market_regime/8-SPY_200DMA_regime.parquet\"\n",
    "\n",
    "# LIVE OUTPUT ROOT\n",
    "LIVE_ROOT = \"./27a-2G_live_trading\"\n",
    "MASTER_TRADES_FILE        = f\"{LIVE_ROOT}/master_trades.csv\"\n",
    "MASTER_RANKINGS_FILE      = f\"{LIVE_ROOT}/master_rankings.csv\"\n",
    "WEEKLY_TRADES_DIR         = f\"{LIVE_ROOT}/weekly_trades\"\n",
    "WEEKLY_RANKINGS_DIR       = f\"{LIVE_ROOT}/weekly_rankings\"\n",
    "WEEKLY_PORT_DIR           = f\"{LIVE_ROOT}/weekly_portfolios\"\n",
    "LIVE_PORTFOLIO_FILE       = f\"{LIVE_ROOT}/live_portfolio.csv\"              # AUTHORITATIVE (DO NOT WRITE)\n",
    "PLANNED_LIVE_PORT_FILE    = f\"{LIVE_ROOT}/planned_live_portfolio.csv\"       # PLANNING STATE (OK TO WRITE)\n",
    "\n",
    "for d in [LIVE_ROOT, WEEKLY_TRADES_DIR, WEEKLY_RANKINGS_DIR, WEEKLY_PORT_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Start from this signal date onward\n",
    "START_TRADING         = pd.Timestamp(\"2025-12-17\")\n",
    "\n",
    "INITIAL_CAPITAL       = 345000.0\n",
    "TOP_PERCENTILE        = 0.95\n",
    "REBALANCE_DAY         = \"Wednesday\"\n",
    "TRADING_DAYS_PER_YEAR = 252\n",
    "\n",
    "MIN_CASH_RESERVE = 20000.0\n",
    "\n",
    "# --- SPY REGIME CONFIRMATION PERIOD (MATCH BACKTEST ENGINE) ---\n",
    "# Number of consecutive days SPY must stay above/below 200 DMA before confirming regime change\n",
    "# Set to 1 for original behavior (immediate flip on crossover)\n",
    "# Set to 5, 10, etc. to filter out whipsaw signals around the 200 DMA\n",
    "SPY_REGIME_CONFIRM_DAYS = 1\n",
    "\n",
    "# --- Position cap (MATCH BACKTEST ENGINE) ---\n",
    "MAX_POSITION_WEIGHT = 0.12   # 12% max position weight at exec-proxy (Wednesday close)\n",
    "\n",
    "# Turnover / trade filters (kept consistent with your engine)\n",
    "DRIFT_THRESHOLD          = 0.05\n",
    "MIN_TRADE_VALUE          = 10000.0\n",
    "MIN_NEW_POSITION_WEIGHT  = 0.005\n",
    "\n",
    "# ============================================================\n",
    "# EXECUTION PRICING MODE (Option A)\n",
    "# ============================================================\n",
    "EXECUTION_PRICE_MODE = \"WED_CLOSE_AS_THU_OPEN_ESTIMATE\"\n",
    "\n",
    "print(\"=== LIVE TRADE GENERATOR (BACKTEST-CONGRUENT, OPTION A) ===\")\n",
    "print(f\"Execution pricing: {EXECUTION_PRICE_MODE}\")\n",
    "print(f\"Start trading:     {START_TRADING.date()}\")\n",
    "print(f\"Rebalance day:     {REBALANCE_DAY}\")\n",
    "print(f\"Max position cap:  {MAX_POSITION_WEIGHT:.0%}\")\n",
    "print(f\"SPY Regime Confirmation Period: {SPY_REGIME_CONFIRM_DAYS} day(s)\")\n",
    "print(\"===============================================================\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# EXECUTION DIAGNOSTICS (kept; lightweight)\n",
    "# ============================================================\n",
    "\n",
    "exec_diag = {\n",
    "    \"weeks_seen\": 0,\n",
    "    \"weeks_written\": 0,\n",
    "    \"orders_generated\": 0,\n",
    "    \"dropped_missing_price\": 0,\n",
    "    \"dropped_min_trade\": 0,\n",
    "    \"dropped_cash_floor_buy\": 0,\n",
    "    \"dropped_spy_regime_buy\": 0,\n",
    "    \"dropped_min_new_weight\": 0,\n",
    "    \"dropped_deduped_existing\": 0,\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# SPY REGIME CONFIRMATION FUNCTION (MATCH BACKTEST ENGINE)\n",
    "# ============================================================\n",
    "\n",
    "def create_confirmed_regime(raw_regime: np.ndarray, confirm_days: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a confirmed regime signal that requires N consecutive days\n",
    "    above/below the 200 DMA before flipping the regime.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    raw_regime : array of 0/1 (0 = below 200 DMA, 1 = above 200 DMA)\n",
    "    confirm_days : number of consecutive days required to confirm regime change\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    confirmed_regime : array of 0/1 with smoothed regime signal\n",
    "    \"\"\"\n",
    "    if confirm_days <= 1:\n",
    "        return raw_regime.copy()\n",
    "    \n",
    "    n = len(raw_regime)\n",
    "    confirmed = np.zeros(n, dtype=int)\n",
    "    \n",
    "    # Start with the initial regime (use first value)\n",
    "    current_regime = raw_regime[0]\n",
    "    consecutive_count = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        if raw_regime[i] == current_regime:\n",
    "            # Same as current confirmed regime\n",
    "            consecutive_count = 0  # Reset counter for opposite regime\n",
    "            confirmed[i] = current_regime\n",
    "        else:\n",
    "            # Different from current confirmed regime\n",
    "            consecutive_count += 1\n",
    "            \n",
    "            if consecutive_count >= confirm_days:\n",
    "                # Confirm the regime change\n",
    "                current_regime = raw_regime[i]\n",
    "                consecutive_count = 0\n",
    "            \n",
    "            confirmed[i] = current_regime\n",
    "    \n",
    "    return confirmed\n",
    "\n",
    "# ============================================================\n",
    "# FAST PRICE LOOKUP (close fallback)\n",
    "# ============================================================\n",
    "\n",
    "def fast_price_lookup(px_array, date_val):\n",
    "    date_val = np.datetime64(date_val, \"ns\")\n",
    "    dates = px_array[\"date\"]\n",
    "    idx = np.searchsorted(dates, date_val, side=\"right\") - 1\n",
    "    if idx < 0:\n",
    "        return np.nan\n",
    "    return px_array[\"px\"][idx]\n",
    "\n",
    "# ============================================================\n",
    "# SNAPSHOT PORTFOLIO (close + \"exec proxy\")\n",
    "# ============================================================\n",
    "\n",
    "def snapshot_portfolio_close(date, cash, positions, px_by_ticker_close):\n",
    "    \"\"\"\n",
    "    Snapshot using close prices (fallback).\n",
    "    NOTE: In Option A we primarily size using exec proxy prices from signal-day close.\n",
    "    \"\"\"\n",
    "    equity = 0.0\n",
    "    for t, pos in positions.items():\n",
    "        if t not in px_by_ticker_close:\n",
    "            continue\n",
    "        px = fast_price_lookup(px_by_ticker_close[t], date)\n",
    "        if not np.isnan(px):\n",
    "            equity += int(pos[\"shares\"]) * float(px)\n",
    "    return equity, cash + equity, len(positions)\n",
    "\n",
    "def snapshot_portfolio_exec_proxy(exec_date, cash, positions, exec_px_map, px_by_ticker_close_fallback):\n",
    "    \"\"\"\n",
    "    Snapshot using \"execution proxy\" prices.\n",
    "    Option A: exec_px_map is built from signal-day close_adj (Wednesday close).\n",
    "    \"\"\"\n",
    "    equity = 0.0\n",
    "    for t, pos in positions.items():\n",
    "        px = exec_px_map.get(t, np.nan)\n",
    "        if pd.isna(px) or px <= 0:\n",
    "            if t in px_by_ticker_close_fallback:\n",
    "                px = fast_price_lookup(px_by_ticker_close_fallback[t], exec_date)\n",
    "        if pd.notna(px) and px > 0:\n",
    "            equity += int(pos[\"shares\"]) * float(px)\n",
    "    return equity, cash + equity, len(positions)\n",
    "\n",
    "# ============================================================\n",
    "# LIVE PORTFOLIO LOAD / SAVE\n",
    "# ============================================================\n",
    "\n",
    "def load_live_portfolio(path: str):\n",
    "    \"\"\"\n",
    "    Loads AUTHORITATIVE live portfolio state (from executed trades).\n",
    "    This file should NOT be mutated by this generator.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return float(INITIAL_CAPITAL), {}  # cash, positions\n",
    "\n",
    "    p = pd.read_csv(path)\n",
    "    if p.empty:\n",
    "        return float(INITIAL_CAPITAL), {}\n",
    "\n",
    "    cash = float(p[\"cash\"].iloc[0]) if \"cash\" in p.columns else float(INITIAL_CAPITAL)\n",
    "\n",
    "    positions = {}\n",
    "    if \"ticker\" in p.columns and \"shares\" in p.columns:\n",
    "        for _, r in p.iterrows():\n",
    "            t = str(r[\"ticker\"]).strip()\n",
    "            if not t or t == \"nan\":\n",
    "                continue\n",
    "            sh = int(r[\"shares\"])\n",
    "            if sh <= 0:\n",
    "                continue\n",
    "            entry = float(r[\"entry_price\"]) if \"entry_price\" in p.columns and pd.notna(r[\"entry_price\"]) else np.nan\n",
    "            positions[t] = {\"shares\": sh, \"entry\": entry}\n",
    "\n",
    "    return cash, positions\n",
    "\n",
    "def save_portfolio_snapshot(path: str, asof_date: pd.Timestamp, cash: float, positions: dict):\n",
    "    \"\"\"\n",
    "    Writes a portfolio snapshot (planning or reporting).\n",
    "    Safe to use for planned portfolios and weekly snapshots.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    if positions:\n",
    "        for t, pos in sorted(positions.items()):\n",
    "            rows.append({\n",
    "                \"asof_date\": asof_date.date().isoformat(),\n",
    "                \"cash\": float(cash),\n",
    "                \"ticker\": t,\n",
    "                \"shares\": int(pos[\"shares\"]),\n",
    "                \"entry_price\": float(pos.get(\"entry\", np.nan)) if pd.notna(pos.get(\"entry\", np.nan)) else np.nan,\n",
    "            })\n",
    "    else:\n",
    "        rows.append({\n",
    "            \"asof_date\": asof_date.date().isoformat(),\n",
    "            \"cash\": float(cash),\n",
    "            \"ticker\": \"\",\n",
    "            \"shares\": 0,\n",
    "            \"entry_price\": np.nan,\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(path, index=False)\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def is_rebalance_day(date: pd.Timestamp) -> bool:\n",
    "    return date.day_name() == REBALANCE_DAY\n",
    "\n",
    "def get_signal_close(day_df, ticker):\n",
    "    row = day_df.loc[day_df[\"ticker\"] == ticker, \"close_adj\"]\n",
    "    if row.empty:\n",
    "        return np.nan\n",
    "    return float(row.iloc[0])\n",
    "\n",
    "def make_plan_id(signal_date: pd.Timestamp, exec_date: pd.Timestamp, ticker: str, side: str) -> str:\n",
    "    \"\"\"\n",
    "    Deterministic ID so repeated runs don't duplicate planned trades in master_trades.csv\n",
    "    \"\"\"\n",
    "    return f\"PLAN-{signal_date.strftime('%Y%m%d')}-{exec_date.strftime('%Y%m%d')}-{ticker}-{side}\"\n",
    "\n",
    "def categorize_no_trade(row):\n",
    "    \"\"\"Determine why a stock didn't trade\"\"\"\n",
    "    \n",
    "    # Already at target (within drift threshold)\n",
    "    if abs(row['weight_change']) < DRIFT_THRESHOLD:\n",
    "        return 'Within drift threshold'\n",
    "    \n",
    "    # New position but too small\n",
    "    if row['current_shares'] == 0 and row['target_weight'] < MIN_NEW_POSITION_WEIGHT:\n",
    "        return 'New position too small'\n",
    "    \n",
    "    # Trade value too small\n",
    "    if abs(row['shares_change'] * row['close_adj']) < MIN_TRADE_VALUE:\n",
    "        return 'Trade value too small'\n",
    "    \n",
    "    # Would buy but SPY regime prevents it\n",
    "    if row['shares_change'] > 0 and not row['spy_above_200dma']:\n",
    "        return 'SPY regime prevented buy'\n",
    "    \n",
    "    # Insufficient cash\n",
    "    if row['shares_change'] > 0:\n",
    "        return 'Insufficient cash'\n",
    "    \n",
    "    return 'Other'\n",
    "\n",
    "def cap_and_redistribute_weights(w: np.ndarray, cap: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    MATCH BACKTEST ENGINE:\n",
    "    Caps weights at `cap` and redistributes any excess proportionally\n",
    "    to the remaining (uncapped) names.\n",
    "\n",
    "    If not feasible to fully invest under cap (N*cap < 1), it will cap\n",
    "    and leave leftover unallocated (cash drag).\n",
    "    \"\"\"\n",
    "    w = np.asarray(w, dtype=float).copy()\n",
    "    if w.size == 0:\n",
    "        return w\n",
    "\n",
    "    s = w.sum()\n",
    "    if s > 0:\n",
    "        w /= s\n",
    "\n",
    "    # Not feasible to be fully invested under cap\n",
    "    if w.size * cap < 1.0:\n",
    "        return np.minimum(w, cap)\n",
    "\n",
    "    # Iteratively cap and redistribute\n",
    "    for _ in range(10_000):\n",
    "        over = w > cap\n",
    "        if not over.any():\n",
    "            break\n",
    "        excess = (w[over] - cap).sum()\n",
    "        w[over] = cap\n",
    "        under = ~over\n",
    "        under_sum = w[under].sum()\n",
    "        if under_sum <= 0:\n",
    "            break\n",
    "        w[under] += excess * (w[under] / under_sum)\n",
    "\n",
    "    return w\n",
    "\n",
    "# ============================================================\n",
    "# TRADING CALENDAR HELPER\n",
    "# ============================================================\n",
    "\n",
    "def get_next_trading_day(signal_date: pd.Timestamp, trading_calendar: list) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    Returns the next actual trading day after signal_date.\n",
    "    Uses the trading calendar (from universe or SPY data) to skip weekends and holidays.\n",
    "    \n",
    "    If signal_date is beyond the calendar, falls back to finding the next weekday\n",
    "    that isn't a known US market holiday.\n",
    "    \"\"\"\n",
    "    signal_date = pd.Timestamp(signal_date).normalize()\n",
    "    \n",
    "    # First, try to find it in the trading calendar\n",
    "    for d in trading_calendar:\n",
    "        if d > signal_date:\n",
    "            return d\n",
    "    \n",
    "    # Fallback: iterate forward until we find a likely trading day\n",
    "    # (Skip weekends and known US market holidays)\n",
    "    US_MARKET_HOLIDAYS = {\n",
    "        # 2025\n",
    "        (2025, 1, 1),   # New Year's Day\n",
    "        (2025, 1, 20),  # MLK Day\n",
    "        (2025, 2, 17),  # Presidents Day\n",
    "        (2025, 4, 18),  # Good Friday\n",
    "        (2025, 5, 26),  # Memorial Day\n",
    "        (2025, 6, 19),  # Juneteenth\n",
    "        (2025, 7, 4),   # Independence Day\n",
    "        (2025, 9, 1),   # Labor Day\n",
    "        (2025, 11, 27), # Thanksgiving\n",
    "        (2025, 12, 25), # Christmas\n",
    "        # 2026\n",
    "        (2026, 1, 1),   # New Year's Day\n",
    "        (2026, 1, 19),  # MLK Day\n",
    "        (2026, 2, 16),  # Presidents Day\n",
    "        (2026, 4, 3),   # Good Friday\n",
    "        (2026, 5, 25),  # Memorial Day\n",
    "        (2026, 6, 19),  # Juneteenth\n",
    "        (2026, 7, 3),   # Independence Day (observed)\n",
    "        (2026, 9, 7),   # Labor Day\n",
    "        (2026, 11, 26), # Thanksgiving\n",
    "        (2026, 12, 25), # Christmas\n",
    "    }\n",
    "    \n",
    "    candidate = signal_date + pd.Timedelta(days=1)\n",
    "    for _ in range(10):  # Max 10 days forward (handles long weekends)\n",
    "        # Skip weekends\n",
    "        if candidate.weekday() >= 5:  # Saturday=5, Sunday=6\n",
    "            candidate += pd.Timedelta(days=1)\n",
    "            continue\n",
    "        # Skip known holidays\n",
    "        if (candidate.year, candidate.month, candidate.day) in US_MARKET_HOLIDAYS:\n",
    "            candidate += pd.Timedelta(days=1)\n",
    "            continue\n",
    "        # Found a likely trading day\n",
    "        return candidate\n",
    "    \n",
    "    # Ultimate fallback (shouldn't reach here)\n",
    "    return signal_date + pd.Timedelta(days=1)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD UNIVERSE\n",
    "# ============================================================\n",
    "\n",
    "df = pd.read_parquet(UNIVERSE_FILE)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "df[\"slope_adj\"] = pd.to_numeric(df[\"slope_adj\"], errors=\"coerce\")\n",
    "df[\"close_adj\"] = pd.to_numeric(df[\"close_adj\"], errors=\"coerce\")\n",
    "df[\"open_adj\"]  = pd.to_numeric(df.get(\"open_adj\", np.nan), errors=\"coerce\")  # may exist; not used in Option A\n",
    "\n",
    "# Some universe files use in_sp500; if missing treat as True\n",
    "if \"in_sp500\" not in df.columns:\n",
    "    df[\"in_sp500\"] = True\n",
    "\n",
    "print(f\"Loaded universe: {len(df):,} rows\")\n",
    "\n",
    "# ============================================================\n",
    "# MERGE ATR20 PER-TICKER\n",
    "# ============================================================\n",
    "\n",
    "atr20_map = {}\n",
    "for f in os.listdir(ATR20_DIR):\n",
    "    if not f.endswith(\".parquet\"):\n",
    "        continue\n",
    "    t = f.replace(\".parquet\", \"\")\n",
    "    tmp = pd.read_parquet(os.path.join(ATR20_DIR, f))\n",
    "    if \"atr20\" not in tmp.columns:\n",
    "        continue\n",
    "    tmp[\"date\"] = pd.to_datetime(tmp[\"date\"])\n",
    "    atr20_map[t] = tmp[[\"date\", \"atr20\"]]\n",
    "\n",
    "rows = []\n",
    "for t, sub in df.groupby(\"ticker\", sort=False):\n",
    "    if t in atr20_map:\n",
    "        rows.append(sub.merge(atr20_map[t], on=\"date\", how=\"left\"))\n",
    "    else:\n",
    "        sub = sub.copy()\n",
    "        sub[\"atr20\"] = np.nan\n",
    "        rows.append(sub)\n",
    "\n",
    "df = pd.concat(rows, ignore_index=True)\n",
    "print(f\"Universe with ATR20 merged: {len(df):,} rows\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD SPY REGIME (WITH CONFIRMATION FILTER)\n",
    "# ============================================================\n",
    "\n",
    "spy = pd.read_parquet(SPY_FILE)\n",
    "\n",
    "if spy.index.name in [\"Date\", \"date\", None]:\n",
    "    spy = spy.reset_index().rename(columns={\"index\": \"date\", \"Date\": \"date\"})\n",
    "spy[\"date\"] = pd.to_datetime(spy[\"date\"])\n",
    "\n",
    "if \"market_regime\" not in spy.columns:\n",
    "    raise ValueError(\"SPY file missing 'market_regime' column\")\n",
    "\n",
    "# Get raw regime signal (1 = above 200 DMA, 0 = below)\n",
    "raw_regime = spy[\"market_regime\"].astype(int).values\n",
    "\n",
    "# Apply confirmation period filter (MATCH BACKTEST ENGINE)\n",
    "confirmed_regime = create_confirmed_regime(raw_regime, SPY_REGIME_CONFIRM_DAYS)\n",
    "spy[\"spy_above_200dma\"] = confirmed_regime == 1\n",
    "\n",
    "# Report regime statistics\n",
    "n_bull_raw = (raw_regime == 1).sum()\n",
    "n_bear_raw = (raw_regime == 0).sum()\n",
    "n_bull_confirmed = (confirmed_regime == 1).sum()\n",
    "n_bear_confirmed = (confirmed_regime == 0).sum()\n",
    "\n",
    "print(f\"\\nSPY Regime Statistics:\")\n",
    "print(f\"  Raw regime:       {n_bull_raw:,} bull days, {n_bear_raw:,} bear days\")\n",
    "print(f\"  Confirmed regime: {n_bull_confirmed:,} bull days, {n_bear_confirmed:,} bear days\")\n",
    "if SPY_REGIME_CONFIRM_DAYS > 1:\n",
    "    regime_changes_raw = np.sum(np.diff(raw_regime) != 0)\n",
    "    regime_changes_confirmed = np.sum(np.diff(confirmed_regime) != 0)\n",
    "    print(f\"  Regime changes:   {regime_changes_raw} raw â†’ {regime_changes_confirmed} confirmed\")\n",
    "\n",
    "spy_regime_map = spy.set_index(\"date\")[\"spy_above_200dma\"].to_dict()\n",
    "\n",
    "# ============================================================\n",
    "# PREP GROUPED DATA + DATE MAPS\n",
    "# ============================================================\n",
    "\n",
    "df_by_date = {d: sub for d, sub in df.groupby(\"date\")}\n",
    "\n",
    "# close price history (fallback)\n",
    "px_by_ticker_close = {}\n",
    "for t, sub in df.groupby(\"ticker\", sort=False):\n",
    "    sub = sub.sort_values(\"date\")\n",
    "    arr = np.zeros(len(sub), dtype=[(\"date\", \"datetime64[ns]\"), (\"px\", \"float64\")])\n",
    "    arr[\"date\"] = sub[\"date\"].values.astype(\"datetime64[ns]\")\n",
    "    arr[\"px\"]   = sub[\"close_adj\"].astype(float).values\n",
    "    px_by_ticker_close[t] = arr\n",
    "\n",
    "dates = sorted(df_by_date.keys())\n",
    "\n",
    "# Build trading calendar for next-day lookups (combine universe dates + SPY dates for completeness)\n",
    "trading_calendar = sorted(set(dates) | set(spy[\"date\"].dropna()))\n",
    "\n",
    "# Next-trading-day map from available universe dates\n",
    "next_date_map = {d: dates[i + 1] if i + 1 < len(dates) else None for i, d in enumerate(dates)}\n",
    "\n",
    "# Next-trading-day map from available universe dates\n",
    "next_date_map = {d: dates[i + 1] if i + 1 < len(dates) else None for i, d in enumerate(dates)}\n",
    "\n",
    "# ============================================================\n",
    "# LOAD AUTHORITATIVE LIVE PORTFOLIO (DO NOT WRITE IT HERE)\n",
    "# ============================================================\n",
    "\n",
    "cash_real, positions_real = load_live_portfolio(LIVE_PORTFOLIO_FILE)\n",
    "\n",
    "# PLANNING STATE (we mutate this in-sim to build weekly plans)\n",
    "cash_plan = float(cash_real)\n",
    "positions_plan = {k: v.copy() for k, v in positions_real.items()}\n",
    "\n",
    "print(f\"\\nLoaded AUTHORITATIVE live portfolio state (from {LIVE_PORTFOLIO_FILE}):\")\n",
    "print(f\"  Cash:      {cash_real:,.2f}\")\n",
    "print(f\"  Positions: {len(positions_real)}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# BUILD SIGNAL DATES (DO NOT REQUIRE NEXT DAY DATA EXISTENCE)\n",
    "# ============================================================\n",
    "\n",
    "signal_dates = [\n",
    "    d for d in dates\n",
    "    if d >= START_TRADING\n",
    "    and is_rebalance_day(d)\n",
    "]\n",
    "\n",
    "if not signal_dates:\n",
    "    raise RuntimeError(\"No eligible rebalance Wednesdays found after START_TRADING.\")\n",
    "\n",
    "print(f\"Eligible rebalance weeks: {len(signal_dates)}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# MASTER TRADES & RANKINGS LOAD (append mode + DE-DUP)\n",
    "# ============================================================\n",
    "\n",
    "def load_master_trades(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "master_df = load_master_trades(MASTER_TRADES_FILE)\n",
    "\n",
    "# Ensure master has plan_id column if present from older runs\n",
    "if not master_df.empty and \"plan_id\" not in master_df.columns:\n",
    "    master_df[\"plan_id\"] = np.nan\n",
    "\n",
    "# Load master rankings (NEW)\n",
    "master_rankings = load_master_trades(MASTER_RANKINGS_FILE)\n",
    "\n",
    "# ============================================================\n",
    "# MAIN LOOP: GENERATE TRADES FOR EVERY SIGNAL WEEK (PLANNING)\n",
    "# ============================================================\n",
    "\n",
    "for signal_date in signal_dates:\n",
    "    exec_diag[\"weeks_seen\"] += 1\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Exec date is \"next trading day\" if available, else calendar +1 day.\n",
    "    # This is only a LABEL for the planned execution day.\n",
    "    # Option A pricing uses signal-day close regardless.\n",
    "    # --------------------------------------------------------\n",
    "    trade_date = next_date_map.get(signal_date)\n",
    "    if trade_date is None:\n",
    "        # Use trading calendar to find actual next trading day (handles holidays)\n",
    "        trade_date = get_next_trading_day(signal_date, trading_calendar)\n",
    "\n",
    "    # Pull signal-day universe slice\n",
    "    if signal_date not in df_by_date:\n",
    "        continue\n",
    "\n",
    "    day = df_by_date[signal_date]\n",
    "    if day is None or day.empty:\n",
    "        continue\n",
    "\n",
    "    # -------------------------\n",
    "    # SPY regime for this week (signal-day close, with confirmation filter applied)\n",
    "    # -------------------------\n",
    "    spy_above_200 = bool(spy_regime_map.get(signal_date, True))\n",
    "    can_buy_next_open = spy_above_200\n",
    "\n",
    "    # -------------------------\n",
    "    # Rank / top group\n",
    "    # -------------------------\n",
    "    rankable = day[\n",
    "        (day[\"slope_adj\"].notna()) &\n",
    "        (day[\"in_sp500\"] == True)\n",
    "    ].copy()\n",
    "\n",
    "    if rankable.empty:\n",
    "        continue\n",
    "\n",
    "    rankable = rankable.sort_values(\"slope_adj\", ascending=False)\n",
    "    cutoff = rankable[\"slope_adj\"].quantile(TOP_PERCENTILE)\n",
    "    top_group = rankable[rankable[\"slope_adj\"] >= cutoff].copy()\n",
    "\n",
    "    if top_group.empty:\n",
    "        continue\n",
    "\n",
    "    top_group = top_group.sort_values(\"slope_adj\", ascending=False)\n",
    "    top_group[\"slope_rank_within_top\"] = np.arange(1, len(top_group) + 1)\n",
    "    rank_map = dict(zip(top_group[\"ticker\"], top_group[\"slope_rank_within_top\"]))\n",
    "    top_tickers = set(top_group[\"ticker\"].values)\n",
    "\n",
    "    # -------------------------\n",
    "    # Exec price map (Option A proxy) = signal-day close_adj\n",
    "    # -------------------------\n",
    "    exec_px_map = day.set_index(\"ticker\")[\"close_adj\"].to_dict()\n",
    "\n",
    "    # =========================================================\n",
    "    # WEEKLY TRADE GENERATION (MUTATES PLANNING STATE ONLY)\n",
    "    # =========================================================\n",
    "    weekly_rows = []\n",
    "\n",
    "    # 1) EXIT SELLS FIRST\n",
    "    exit_tickers = [t for t in list(positions_plan.keys()) if t not in top_tickers]\n",
    "\n",
    "    for t in exit_tickers:\n",
    "        pos_shares = int(positions_plan[t][\"shares\"])\n",
    "\n",
    "        px = exec_px_map.get(t, np.nan)\n",
    "        if pd.isna(px) or px <= 0:\n",
    "            if t in px_by_ticker_close:\n",
    "                px = fast_price_lookup(px_by_ticker_close[t], signal_date)\n",
    "\n",
    "        if pd.isna(px) or px <= 0:\n",
    "            exec_diag[\"dropped_missing_price\"] += 1\n",
    "            continue\n",
    "\n",
    "        px = float(px)\n",
    "        est_value = pos_shares * px\n",
    "\n",
    "        cash_before = cash_plan\n",
    "        cash_plan += est_value\n",
    "\n",
    "        del positions_plan[t]\n",
    "\n",
    "        _, port_after, npos_after = snapshot_portfolio_exec_proxy(\n",
    "            trade_date, cash_plan, positions_plan, exec_px_map, px_by_ticker_close\n",
    "        )\n",
    "\n",
    "        side = \"SELL\"\n",
    "        weekly_rows.append({\n",
    "            \"plan_id\": make_plan_id(signal_date, trade_date, t, side),\n",
    "            \"created_ts\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "            \"signal_date\": signal_date.date().isoformat(),\n",
    "            \"exec_date\": trade_date.date().isoformat(),\n",
    "            \"ticker\": t,\n",
    "            \"side\": side,\n",
    "            \"shares\": pos_shares,\n",
    "            \"est_exec_px\": px,\n",
    "            \"est_value\": float(est_value),\n",
    "            \"reason\": \"not_in_top_quintile\",\n",
    "            \"slope_rank\": int(rank_map.get(t, 9999)),\n",
    "            \"spy_above_200dma\": spy_above_200,\n",
    "            \"cash_before\": float(cash_before),\n",
    "            \"cash_after\": float(cash_plan),\n",
    "            \"portfolio_after\": float(port_after),\n",
    "            \"num_positions_after\": int(npos_after),\n",
    "        })\n",
    "\n",
    "    # 2) REVALUE PORTFOLIO AT \"EXEC TIME\" (proxy) AFTER EXITS\n",
    "    equity_exec, portfolio_exec, _ = snapshot_portfolio_exec_proxy(\n",
    "        trade_date, cash_plan, positions_plan, exec_px_map, px_by_ticker_close\n",
    "    )\n",
    "    effective_equity = max(portfolio_exec - MIN_CASH_RESERVE, 0.0)\n",
    "\n",
    "    # 3) BUILD TARGETS (signal-day atr/slope) AND SIZE USING exec_px_map (proxy)\n",
    "    tg = top_group.copy()\n",
    "    tg = tg[\n",
    "        tg[\"atr20\"].notna() &\n",
    "        (tg[\"atr20\"] > 0) &\n",
    "        tg[\"close_adj\"].notna() &\n",
    "        (tg[\"close_adj\"] > 0)\n",
    "    ].copy()\n",
    "\n",
    "    # =========================================================\n",
    "    # NEW: STORE PRE-FILTER RANKINGS\n",
    "    # =========================================================\n",
    "    weekly_rankings = []\n",
    "\n",
    "    if not tg.empty:\n",
    "        inv_vol = 1.0 / tg[\"atr20\"].astype(float)\n",
    "        total_inv_vol = inv_vol.sum()\n",
    "\n",
    "        if total_inv_vol > 0:\n",
    "            tg[\"inv_vol\"] = inv_vol\n",
    "\n",
    "            # ---- MATCH BACKTEST ENGINE: compute raw weights then cap+redistribute ----\n",
    "            tg[\"raw_weight\"] = tg[\"inv_vol\"] / total_inv_vol\n",
    "            w_cap = cap_and_redistribute_weights(tg[\"raw_weight\"].to_numpy(), MAX_POSITION_WEIGHT)\n",
    "            tg[\"weight\"] = w_cap\n",
    "\n",
    "            # Size off effective equity (after cash reserve) using capped weights\n",
    "            tg[\"target_value\"] = effective_equity * tg[\"weight\"]\n",
    "\n",
    "            # Attach proxy execution price (signal-day close)\n",
    "            tg = tg.merge(\n",
    "                pd.Series(exec_px_map, name=\"exec_px_est\"),\n",
    "                left_on=\"ticker\",\n",
    "                right_index=True,\n",
    "                how=\"inner\"\n",
    "            )\n",
    "            tg = tg[tg[\"exec_px_est\"].notna() & (tg[\"exec_px_est\"] > 0)].copy()\n",
    "\n",
    "            # ---- Hard safety: no target value over 12% of proxy portfolio value ----\n",
    "            tg[\"target_value\"] = np.minimum(tg[\"target_value\"], MAX_POSITION_WEIGHT * portfolio_exec)\n",
    "\n",
    "            # target_shares uses proxy exec price (signal-day close)\n",
    "            tg[\"target_shares\"] = np.floor(tg[\"target_value\"] / tg[\"exec_px_est\"]).astype(int)\n",
    "            tg = tg[tg[\"target_shares\"] > 0].copy()\n",
    "\n",
    "            total_portfolio_value_exec = float(portfolio_exec)\n",
    "\n",
    "            # deterministic order (match backtest: slope descending)\n",
    "            tg = tg.sort_values(\"slope_adj\", ascending=False)\n",
    "\n",
    "            # -------------------------\n",
    "            # CAPTURE ALL RANKED STOCKS BEFORE FILTERING\n",
    "            # -------------------------\n",
    "            for _, row in tg.iterrows():\n",
    "                t = str(row[\"ticker\"])\n",
    "                rank = int(rank_map.get(t, 9999))\n",
    "                px = float(row[\"exec_px_est\"])\n",
    "                target_sh = int(row[\"target_shares\"])\n",
    "                current_sh = int(positions_plan.get(t, {}).get(\"shares\", 0))\n",
    "                \n",
    "                target_value = target_sh * px\n",
    "                target_weight = (target_value / total_portfolio_value_exec) if total_portfolio_value_exec > 0 else 0.0\n",
    "                \n",
    "                current_value = current_sh * px\n",
    "                current_weight = (current_value / total_portfolio_value_exec) if total_portfolio_value_exec > 0 else 0.0\n",
    "                \n",
    "                weekly_rankings.append({\n",
    "                    \"signal_date\": signal_date.date().isoformat(),\n",
    "                    \"exec_date\": trade_date.date().isoformat(),\n",
    "                    \"ticker\": t,\n",
    "                    \"slope_rank\": rank,\n",
    "                    \"slope_adj\": float(row[\"slope_adj\"]),\n",
    "                    \"atr20\": float(row[\"atr20\"]),\n",
    "                    \"close_adj\": float(row[\"close_adj\"]),\n",
    "                    \"raw_weight\": float(row[\"raw_weight\"]),\n",
    "                    \"capped_weight\": float(row[\"weight\"]),\n",
    "                    \"target_value\": target_value,\n",
    "                    \"target_weight\": target_weight,\n",
    "                    \"target_shares\": target_sh,\n",
    "                    \"current_shares\": current_sh,\n",
    "                    \"current_value\": current_value,\n",
    "                    \"current_weight\": current_weight,\n",
    "                    \"weight_change\": target_weight - current_weight,\n",
    "                    \"shares_change\": target_sh - current_sh,\n",
    "                    \"spy_above_200dma\": spy_above_200,\n",
    "                    \"portfolio_value\": total_portfolio_value_exec,\n",
    "                })\n",
    "\n",
    "            # -------------------------\n",
    "            # NOW GENERATE ACTUAL TRADES (WITH FILTERS)\n",
    "            # -------------------------\n",
    "            for _, row in tg.iterrows():\n",
    "                t = str(row[\"ticker\"])\n",
    "                px = float(row[\"exec_px_est\"])\n",
    "                if not (px > 0):\n",
    "                    exec_diag[\"dropped_missing_price\"] += 1\n",
    "                    continue\n",
    "\n",
    "                target_sh = int(row[\"target_shares\"])\n",
    "                current_sh = int(positions_plan.get(t, {}).get(\"shares\", 0))\n",
    "\n",
    "                # ---- MATCH BACKTEST ENGINE: share-level cap enforcement ----\n",
    "                max_shares_allowed = (\n",
    "                    int(np.floor((MAX_POSITION_WEIGHT * total_portfolio_value_exec) / px))\n",
    "                    if total_portfolio_value_exec > 0 else 0\n",
    "                )\n",
    "                target_sh = min(target_sh, max_shares_allowed)\n",
    "\n",
    "                target_value = target_sh * px\n",
    "                target_weight = (target_value / total_portfolio_value_exec) if total_portfolio_value_exec > 0 else 0.0\n",
    "\n",
    "                current_value = current_sh * px\n",
    "                current_weight = (current_value / total_portfolio_value_exec) if total_portfolio_value_exec > 0 else 0.0\n",
    "\n",
    "                weight_diff = abs(target_weight - current_weight)\n",
    "                is_new_position = (current_sh == 0)\n",
    "\n",
    "                # ---- MATCH BACKTEST ENGINE: force trim if currently breaching cap ----\n",
    "                cap_breach = (current_weight > MAX_POSITION_WEIGHT + 1e-9)\n",
    "\n",
    "                # Skip micro rebalances unless we need to fix a cap breach\n",
    "                if (weight_diff < DRIFT_THRESHOLD) and (not cap_breach):\n",
    "                    continue\n",
    "\n",
    "                # ---------------- SELL (rebalance down / cap trim) ----------------\n",
    "                if target_sh < current_sh:\n",
    "                    trade_shares = current_sh - target_sh\n",
    "                    est_value = trade_shares * px\n",
    "\n",
    "                    if est_value < MIN_TRADE_VALUE:\n",
    "                        exec_diag[\"dropped_min_trade\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    cash_before = cash_plan\n",
    "                    cash_plan += est_value\n",
    "\n",
    "                    new_sh = current_sh - trade_shares\n",
    "                    if new_sh <= 0:\n",
    "                        positions_plan.pop(t, None)\n",
    "                    else:\n",
    "                        positions_plan[t][\"shares\"] = int(new_sh)\n",
    "\n",
    "                    _, port_after, npos_after = snapshot_portfolio_exec_proxy(\n",
    "                        trade_date, cash_plan, positions_plan, exec_px_map, px_by_ticker_close\n",
    "                    )\n",
    "\n",
    "                    side = \"SELL\"\n",
    "                    weekly_rows.append({\n",
    "                        \"plan_id\": make_plan_id(signal_date, trade_date, t, side),\n",
    "                        \"created_ts\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "                        \"signal_date\": signal_date.date().isoformat(),\n",
    "                        \"exec_date\": trade_date.date().isoformat(),\n",
    "                        \"ticker\": t,\n",
    "                        \"side\": side,\n",
    "                        \"shares\": int(trade_shares),\n",
    "                        \"est_exec_px\": px,\n",
    "                        \"est_value\": float(est_value),\n",
    "                        \"reason\": \"rebalance_down\" if not cap_breach else \"cap_trim\",\n",
    "                        \"slope_rank\": int(rank_map.get(t, 9999)),\n",
    "                        \"spy_above_200dma\": spy_above_200,\n",
    "                        \"cash_before\": float(cash_before),\n",
    "                        \"cash_after\": float(cash_plan),\n",
    "                        \"portfolio_after\": float(port_after),\n",
    "                        \"num_positions_after\": int(npos_after),\n",
    "                    })\n",
    "\n",
    "                # ---------------- BUY (rebalance up / new entry) ----------------\n",
    "                elif target_sh > current_sh:\n",
    "                    if not can_buy_next_open:\n",
    "                        exec_diag[\"dropped_spy_regime_buy\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    trade_shares = target_sh - current_sh\n",
    "                    est_value = trade_shares * px\n",
    "\n",
    "                    if is_new_position and target_weight < MIN_NEW_POSITION_WEIGHT:\n",
    "                        exec_diag[\"dropped_min_new_weight\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    if est_value < MIN_TRADE_VALUE:\n",
    "                        exec_diag[\"dropped_min_trade\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    if est_value > cash_plan - MIN_CASH_RESERVE:\n",
    "                        exec_diag[\"dropped_cash_floor_buy\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    cash_before = cash_plan\n",
    "                    cash_plan -= est_value\n",
    "\n",
    "                    if t in positions_plan:\n",
    "                        positions_plan[t][\"shares\"] = int(current_sh + trade_shares)\n",
    "                    else:\n",
    "                        positions_plan[t] = {\"shares\": int(trade_shares), \"entry\": px}\n",
    "\n",
    "                    _, port_after, npos_after = snapshot_portfolio_exec_proxy(\n",
    "                        trade_date, cash_plan, positions_plan, exec_px_map, px_by_ticker_close\n",
    "                    )\n",
    "\n",
    "                    side = \"BUY\"\n",
    "                    weekly_rows.append({\n",
    "                        \"plan_id\": make_plan_id(signal_date, trade_date, t, side),\n",
    "                        \"created_ts\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "                        \"signal_date\": signal_date.date().isoformat(),\n",
    "                        \"exec_date\": trade_date.date().isoformat(),\n",
    "                        \"ticker\": t,\n",
    "                        \"side\": side,\n",
    "                        \"shares\": int(trade_shares),\n",
    "                        \"est_exec_px\": px,\n",
    "                        \"est_value\": float(est_value),\n",
    "                        \"reason\": \"rebalance_up\" if current_sh > 0 else \"new_entry\",\n",
    "                        \"slope_rank\": int(rank_map.get(t, 9999)),\n",
    "                        \"spy_above_200dma\": spy_above_200,\n",
    "                        \"cash_before\": float(cash_before),\n",
    "                        \"cash_after\": float(cash_plan),\n",
    "                        \"portfolio_after\": float(port_after),\n",
    "                        \"num_positions_after\": int(npos_after),\n",
    "                    })\n",
    "\n",
    "    # =========================================================\n",
    "    # MERGE RANKINGS WITH TRADES TO DETERMINE STATUS\n",
    "    # =========================================================\n",
    "    \n",
    "    rankings_df = pd.DataFrame(weekly_rankings)\n",
    "    trades_df = pd.DataFrame(weekly_rows)\n",
    "    \n",
    "    if not rankings_df.empty:\n",
    "        # Merge to determine which stocks traded\n",
    "        trades_agg = trades_df.groupby('ticker').agg({\n",
    "            'side': lambda x: ', '.join(x.unique()),\n",
    "            'shares': 'sum',\n",
    "        }).reset_index() if not trades_df.empty else pd.DataFrame(columns=['ticker', 'side', 'shares'])\n",
    "        \n",
    "        rankings_df = rankings_df.merge(\n",
    "            trades_agg,\n",
    "            on='ticker',\n",
    "            how='left',\n",
    "            indicator=True\n",
    "        )\n",
    "        \n",
    "        rankings_df['traded'] = rankings_df['_merge'] == 'both'\n",
    "        rankings_df['traded_flag'] = rankings_df['traded'].map({True: 'TRADED', False: 'NOT_TRADED'})\n",
    "        rankings_df = rankings_df.drop('_merge', axis=1)\n",
    "        \n",
    "        # Add reasons for not trading\n",
    "        rankings_df['no_trade_reason'] = ''\n",
    "        not_traded_mask = ~rankings_df['traded']\n",
    "        rankings_df.loc[not_traded_mask, 'no_trade_reason'] = rankings_df[not_traded_mask].apply(\n",
    "            categorize_no_trade, axis=1\n",
    "        )\n",
    "\n",
    "    # =========================================================\n",
    "    # WRITE WEEKLY FILES + APPEND MASTER (DE-DUP) + SAVE PLANNED PORT\n",
    "    # =========================================================\n",
    "\n",
    "    weekly_tag = signal_date.strftime(\"%Y%m%d\")\n",
    "    weekly_trades_file = os.path.join(WEEKLY_TRADES_DIR, f\"weekly_trades_signal_{weekly_tag}.csv\")\n",
    "    weekly_rankings_file = os.path.join(WEEKLY_RANKINGS_DIR, f\"weekly_rankings_signal_{weekly_tag}.csv\")\n",
    "    weekly_port_file   = os.path.join(WEEKLY_PORT_DIR,   f\"weekly_portfolio_after_{weekly_tag}.csv\")\n",
    "    planned_port_file  = os.path.join(WEEKLY_PORT_DIR,   f\"planned_portfolio_signal_{weekly_tag}.csv\")\n",
    "\n",
    "    if not trades_df.empty:\n",
    "        # SELLs first for readability\n",
    "        trades_df[\"side_order\"] = trades_df[\"side\"].map({\"SELL\": 0, \"BUY\": 1}).fillna(9).astype(int)\n",
    "\n",
    "        trades_df = trades_df.sort_values(\n",
    "            [\"side_order\", \"slope_rank\"],\n",
    "            ascending=[True, True]\n",
    "        ).drop(columns=[\"side_order\"])\n",
    "\n",
    "    # Always write the weekly files (even empty) so you can see \"no trades\" weeks\n",
    "    trades_df.to_csv(weekly_trades_file, index=False)\n",
    "    \n",
    "    # Write rankings file\n",
    "    if not rankings_df.empty:\n",
    "        rankings_df = rankings_df.sort_values(\"slope_rank\")\n",
    "        rankings_df.to_csv(weekly_rankings_file, index=False)\n",
    "\n",
    "    # Save planned portfolio snapshots (optional)\n",
    "    # save_portfolio_snapshot(planned_port_file, signal_date, cash_plan, positions_plan)\n",
    "    # save_portfolio_snapshot(weekly_port_file,  signal_date, cash_plan, positions_plan)\n",
    "\n",
    "    # Update a PLANNED live portfolio file (NOT the authoritative one)\n",
    "    # save_portfolio_snapshot(PLANNED_LIVE_PORT_FILE, signal_date, cash_plan, positions_plan)\n",
    "\n",
    "    # Append to master trades with de-dup on plan_id\n",
    "    if not trades_df.empty:\n",
    "        if master_df.empty:\n",
    "            master_df = trades_df.copy()\n",
    "        else:\n",
    "            if \"plan_id\" not in master_df.columns:\n",
    "                master_df[\"plan_id\"] = np.nan\n",
    "\n",
    "            existing = set(master_df[\"plan_id\"].dropna().astype(str).tolist())\n",
    "            keep_mask = ~trades_df[\"plan_id\"].astype(str).isin(existing)\n",
    "            dropped = int((~keep_mask).sum())\n",
    "            if dropped > 0:\n",
    "                exec_diag[\"dropped_deduped_existing\"] += dropped\n",
    "\n",
    "            to_add = trades_df.loc[keep_mask].copy()\n",
    "            if not to_add.empty:\n",
    "                master_df = pd.concat([master_df, to_add], ignore_index=True)\n",
    "\n",
    "    # Append to master rankings (NEW)\n",
    "    if not rankings_df.empty:\n",
    "        if master_rankings.empty:\n",
    "            master_rankings = rankings_df.copy()\n",
    "        else:\n",
    "            master_rankings = pd.concat([master_rankings, rankings_df], ignore_index=True)\n",
    "\n",
    "    exec_diag[\"orders_generated\"] += len(trades_df)\n",
    "    exec_diag[\"weeks_written\"] += 1\n",
    "\n",
    "# Final master writes\n",
    "master_df.to_csv(MASTER_TRADES_FILE, index=False)\n",
    "master_rankings.to_csv(MASTER_RANKINGS_FILE, index=False)\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(f\"Weeks processed:   {exec_diag['weeks_seen']}\")\n",
    "print(f\"Weeks written:     {exec_diag['weeks_written']}\")\n",
    "print(f\"Orders generated:  {exec_diag['orders_generated']}\")\n",
    "print(\"Drops:\")\n",
    "print(f\"  missing_price:        {exec_diag['dropped_missing_price']}\")\n",
    "print(f\"  min_trade:            {exec_diag['dropped_min_trade']}\")\n",
    "print(f\"  cash_floor_buy:       {exec_diag['dropped_cash_floor_buy']}\")\n",
    "print(f\"  spy_regime_buy:       {exec_diag['dropped_spy_regime_buy']}\")\n",
    "print(f\"  min_new_weight:       {exec_diag['dropped_min_new_weight']}\")\n",
    "print(f\"  deduped_existing:     {exec_diag['dropped_deduped_existing']}\")\n",
    "print(\"\\nOutputs:\")\n",
    "print(f\"  Master planned trades:        {MASTER_TRADES_FILE}\")\n",
    "print(f\"  Master rankings:              {MASTER_RANKINGS_FILE}\")\n",
    "print(f\"  Weekly planned trades dir:    {WEEKLY_TRADES_DIR}\")\n",
    "print(f\"  Weekly rankings dir:          {WEEKLY_RANKINGS_DIR}\")\n",
    "print(f\"  Weekly planned portfolios:    {WEEKLY_PORT_DIR}\")\n",
    "print(f\"  Planned live portfolio file:  {PLANNED_LIVE_PORT_FILE}\")\n",
    "print(f\"\\nNOTE: {LIVE_PORTFOLIO_FILE} was NOT modified (authoritative executions only).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
