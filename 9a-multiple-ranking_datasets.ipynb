{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2cc958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BUILDING RANKING DATASETS (OHLC + slope × r2) FOR EACH LOOKBACK ===\n",
      "Found 1167 tickers with adjusted OHLC data.\n",
      "Found 5 regression lookback folders:\n",
      "  - lookback_60D\n",
      "  - lookback_90D\n",
      "  - lookback_120D\n",
      "  - lookback_180D\n",
      "  - lookback_252D\n",
      "\n",
      "Loading adjusted OHLC into memory (one-time cache)...\n",
      "Price cache ready.\n",
      "\n",
      "\n",
      "====================================================\n",
      "Building ranking dataset for: lookback_60D\n",
      "====================================================\n",
      "⚠ lookback_60D | BBI1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_60D\\BBI1.parquet\n",
      "⚠ lookback_60D | ITT1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_60D\\ITT1.parquet\n",
      "⚠ lookback_60D | Q | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_60D\\Q.parquet\n",
      "⚠ lookback_60D | SOLS | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_60D\\SOLS.parquet\n",
      "\n",
      "=== COMPLETED: lookback_60D ===\n",
      "Saved ranking dataset → ./9a-multiple_regression_ranking_dataset\\lookback_60D\\ranking_dataset.parquet\n",
      "Saved validation log  → ./system_verification/9a-multiple_regression_ranking_dataset\\lookback_60D\\ranking_validation-20251231-084901.csv\n",
      "Rows: 5,366,253\n",
      "\n",
      "====================================================\n",
      "Building ranking dataset for: lookback_90D\n",
      "====================================================\n",
      "⚠ lookback_90D | BBI1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_90D\\BBI1.parquet\n",
      "⚠ lookback_90D | CFL1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_90D\\CFL1.parquet\n",
      "⚠ lookback_90D | FG1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_90D\\FG1.parquet\n",
      "⚠ lookback_90D | ITT1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_90D\\ITT1.parquet\n",
      "⚠ lookback_90D | Q | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_90D\\Q.parquet\n",
      "⚠ lookback_90D | SOLS | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_90D\\SOLS.parquet\n",
      "\n",
      "=== COMPLETED: lookback_90D ===\n",
      "Saved ranking dataset → ./9a-multiple_regression_ranking_dataset\\lookback_90D\\ranking_dataset.parquet\n",
      "Saved validation log  → ./system_verification/9a-multiple_regression_ranking_dataset\\lookback_90D\\ranking_validation-20251231-084924.csv\n",
      "Rows: 5,366,253\n",
      "\n",
      "====================================================\n",
      "Building ranking dataset for: lookback_120D\n",
      "====================================================\n",
      "⚠ lookback_120D | BBI1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_120D\\BBI1.parquet\n",
      "⚠ lookback_120D | CFL1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_120D\\CFL1.parquet\n",
      "⚠ lookback_120D | DEC1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_120D\\DEC1.parquet\n",
      "⚠ lookback_120D | FG1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_120D\\FG1.parquet\n",
      "⚠ lookback_120D | ITT1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_120D\\ITT1.parquet\n",
      "⚠ lookback_120D | Q | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_120D\\Q.parquet\n",
      "⚠ lookback_120D | SK1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_120D\\SK1.parquet\n",
      "⚠ lookback_120D | SOLS | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_120D\\SOLS.parquet\n",
      "\n",
      "=== COMPLETED: lookback_120D ===\n",
      "Saved ranking dataset → ./9a-multiple_regression_ranking_dataset\\lookback_120D\\ranking_dataset.parquet\n",
      "Saved validation log  → ./system_verification/9a-multiple_regression_ranking_dataset\\lookback_120D\\ranking_validation-20251231-084947.csv\n",
      "Rows: 5,366,253\n",
      "\n",
      "====================================================\n",
      "Building ranking dataset for: lookback_180D\n",
      "====================================================\n",
      "⚠ lookback_180D | BAY1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\BAY1.parquet\n",
      "⚠ lookback_180D | BBI1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\BBI1.parquet\n",
      "⚠ lookback_180D | BNL1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\BNL1.parquet\n",
      "⚠ lookback_180D | CFL1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\CFL1.parquet\n",
      "⚠ lookback_180D | DEC1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\DEC1.parquet\n",
      "⚠ lookback_180D | DIGI2 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\DIGI2.parquet\n",
      "⚠ lookback_180D | DNB2 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\DNB2.parquet\n",
      "⚠ lookback_180D | ECH1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\ECH1.parquet\n",
      "⚠ lookback_180D | FG1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\FG1.parquet\n",
      "⚠ lookback_180D | GNT1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\GNT1.parquet\n",
      "⚠ lookback_180D | ITT1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\ITT1.parquet\n",
      "⚠ lookback_180D | MCIC | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\MCIC.parquet\n",
      "⚠ lookback_180D | MST1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\MST1.parquet\n",
      "⚠ lookback_180D | PET1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\PET1.parquet\n",
      "⚠ lookback_180D | Q | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\Q.parquet\n",
      "⚠ lookback_180D | SK1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\SK1.parquet\n",
      "⚠ lookback_180D | SOLS | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\SOLS.parquet\n",
      "⚠ lookback_180D | WAI1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_180D\\WAI1.parquet\n",
      "\n",
      "=== COMPLETED: lookback_180D ===\n",
      "Saved ranking dataset → ./9a-multiple_regression_ranking_dataset\\lookback_180D\\ranking_dataset.parquet\n",
      "Saved validation log  → ./system_verification/9a-multiple_regression_ranking_dataset\\lookback_180D\\ranking_validation-20251231-085010.csv\n",
      "Rows: 5,366,253\n",
      "\n",
      "====================================================\n",
      "Building ranking dataset for: lookback_252D\n",
      "====================================================\n",
      "⚠ lookback_252D | AHM1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\AHM1.parquet\n",
      "⚠ lookback_252D | BAC1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\BAC1.parquet\n",
      "⚠ lookback_252D | BAY1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\BAY1.parquet\n",
      "⚠ lookback_252D | BBI1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\BBI1.parquet\n",
      "⚠ lookback_252D | BNL1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\BNL1.parquet\n",
      "⚠ lookback_252D | BXLT | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\BXLT.parquet\n",
      "⚠ lookback_252D | C1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\C1.parquet\n",
      "⚠ lookback_252D | CCI1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\CCI1.parquet\n",
      "⚠ lookback_252D | CFL1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\CFL1.parquet\n",
      "⚠ lookback_252D | DEC1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\DEC1.parquet\n",
      "⚠ lookback_252D | DI1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\DI1.parquet\n",
      "⚠ lookback_252D | DIGI2 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\DIGI2.parquet\n",
      "⚠ lookback_252D | DNB2 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\DNB2.parquet\n",
      "⚠ lookback_252D | ECH1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\ECH1.parquet\n",
      "⚠ lookback_252D | FCN1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\FCN1.parquet\n",
      "⚠ lookback_252D | FG1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\FG1.parquet\n",
      "⚠ lookback_252D | GFS.A | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\GFS.A.parquet\n",
      "⚠ lookback_252D | GNT1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\GNT1.parquet\n",
      "⚠ lookback_252D | GRN1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\GRN1.parquet\n",
      "⚠ lookback_252D | GSX1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\GSX1.parquet\n",
      "⚠ lookback_252D | H1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\H1.parquet\n",
      "⚠ lookback_252D | ITT1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\ITT1.parquet\n",
      "⚠ lookback_252D | MCIC | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\MCIC.parquet\n",
      "⚠ lookback_252D | MNR1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\MNR1.parquet\n",
      "⚠ lookback_252D | MST1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\MST1.parquet\n",
      "⚠ lookback_252D | PET1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\PET1.parquet\n",
      "⚠ lookback_252D | Q | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\Q.parquet\n",
      "⚠ lookback_252D | SK1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\SK1.parquet\n",
      "⚠ lookback_252D | SNDK | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\SNDK.parquet\n",
      "⚠ lookback_252D | SOLS | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\SOLS.parquet\n",
      "⚠ lookback_252D | STO1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\STO1.parquet\n",
      "⚠ lookback_252D | USS1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\USS1.parquet\n",
      "⚠ lookback_252D | WAI1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\WAI1.parquet\n",
      "⚠ lookback_252D | WFC1 | missing_regression_file | ./7a-multiple_regressions_adjusted_all_prices\\lookback_252D\\WFC1.parquet\n",
      "\n",
      "=== COMPLETED: lookback_252D ===\n",
      "Saved ranking dataset → ./9a-multiple_regression_ranking_dataset\\lookback_252D\\ranking_dataset.parquet\n",
      "Saved validation log  → ./system_verification/9a-multiple_regression_ranking_dataset\\lookback_252D\\ranking_validation-20251231-085034.csv\n",
      "Rows: 5,366,253\n",
      "\n",
      "✅ All lookbacks processed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\"\n",
    "Purpose:\n",
    "    Build ranking datasets (OHLC + volume + slope_adj = slope_annual * r2)\n",
    "    for multiple regression lookback periods.\n",
    "\n",
    "Assumptions:\n",
    "    - REGRESSION_ROOT contains subdirectories like: lookback_60D, lookback_90D, ...\n",
    "    - Each subdirectory contains per-ticker parquet files with columns:\n",
    "        date, slope_annual, r2   (others allowed)\n",
    "\n",
    "Outputs (per lookback):\n",
    "    OUTPUT_ROOT/lookback_XXD/ranking_dataset.parquet\n",
    "    VER_ROOT/lookback_XXD/ranking_validation-<timestamp>.csv\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "ADJ_PRICE_DIR    = \"./3-adjusted_All_Prices_OHLC\"\n",
    "REGRESSION_ROOT  = \"./7a-multiple_regressions_adjusted_all_prices\"\n",
    "\n",
    "OUTPUT_ROOT      = \"./9a-multiple_regression_ranking_dataset\"\n",
    "VER_ROOT         = \"./system_verification/9a-multiple_regression_ranking_dataset\"\n",
    "\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "os.makedirs(VER_ROOT, exist_ok=True)\n",
    "\n",
    "REQUIRED_PX = {\"date\", \"open_adj\", \"high_adj\", \"low_adj\", \"close_adj\"}  # volume optional\n",
    "\n",
    "print(\"=== BUILDING RANKING DATASETS (OHLC + slope × r2) FOR EACH LOOKBACK ===\")\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def discover_lookback_dirs(root: str):\n",
    "    \"\"\"\n",
    "    Find subdirectories like 'lookback_90D' and sort by numeric window.\n",
    "    \"\"\"\n",
    "    pat = re.compile(r\"^lookback_(\\d+)D$\")\n",
    "    items = []\n",
    "    for name in os.listdir(root):\n",
    "        full = os.path.join(root, name)\n",
    "        if os.path.isdir(full):\n",
    "            m = pat.match(name)\n",
    "            if m:\n",
    "                items.append((int(m.group(1)), name))\n",
    "    items.sort(key=lambda x: x[0])\n",
    "    return [name for _, name in items]\n",
    "\n",
    "def log_issue(validation_rows, ticker, lookback_dir, issue, detail=\"\"):\n",
    "    validation_rows.append({\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"lookback_dir\": lookback_dir,\n",
    "        \"ticker\": ticker,\n",
    "        \"issue\": issue,\n",
    "        \"detail\": detail,\n",
    "    })\n",
    "    print(f\"⚠ {lookback_dir} | {ticker} | {issue} | {detail}\")\n",
    "\n",
    "# ============================================================\n",
    "# DISCOVER TICKERS\n",
    "# ============================================================\n",
    "\n",
    "tickers = sorted([\n",
    "    f.replace(\".parquet\", \"\")\n",
    "    for f in os.listdir(ADJ_PRICE_DIR)\n",
    "    if f.endswith(\".parquet\")\n",
    "])\n",
    "print(f\"Found {len(tickers)} tickers with adjusted OHLC data.\")\n",
    "\n",
    "# ============================================================\n",
    "# DISCOVER LOOKBACK DIRECTORIES\n",
    "# ============================================================\n",
    "\n",
    "lookback_dirs = discover_lookback_dirs(REGRESSION_ROOT)\n",
    "if not lookback_dirs:\n",
    "    raise ValueError(\n",
    "        f\"No lookback_*D subdirectories found under {REGRESSION_ROOT}. \"\n",
    "        \"Expected e.g. lookback_60D, lookback_90D, etc.\"\n",
    "    )\n",
    "\n",
    "print(f\"Found {len(lookback_dirs)} regression lookback folders:\")\n",
    "for d in lookback_dirs:\n",
    "    print(f\"  - {d}\")\n",
    "\n",
    "# ============================================================\n",
    "# OPTIONAL: CACHE PRICE DATA ONCE (faster; reasonable for SP500)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nLoading adjusted OHLC into memory (one-time cache)...\")\n",
    "px_cache = {}\n",
    "for ticker in tickers:\n",
    "    f_px = os.path.join(ADJ_PRICE_DIR, f\"{ticker}.parquet\")\n",
    "    try:\n",
    "        px = pd.read_parquet(f_px)\n",
    "\n",
    "        if not REQUIRED_PX.issubset(px.columns):\n",
    "            # keep in cache as None; log per lookback later (so you see it everywhere)\n",
    "            px_cache[ticker] = None\n",
    "            continue\n",
    "\n",
    "        px[\"date\"] = pd.to_datetime(px[\"date\"])\n",
    "        px = px.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "        # Keep required + volume if present\n",
    "        keep_cols = [\"date\", \"open_adj\", \"high_adj\", \"low_adj\", \"close_adj\"]\n",
    "        if \"volume\" in px.columns:\n",
    "            keep_cols.append(\"volume\")\n",
    "        else:\n",
    "            # add volume as NaN to keep schema consistent downstream\n",
    "            px[\"volume\"] = np.nan\n",
    "            keep_cols.append(\"volume\")\n",
    "\n",
    "        px_cache[ticker] = px[keep_cols].copy()\n",
    "    except Exception:\n",
    "        px_cache[ticker] = None\n",
    "\n",
    "print(\"Price cache ready.\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# MAIN: BUILD ONE RANKING DATASET PER LOOKBACK\n",
    "# ============================================================\n",
    "\n",
    "for lb_dir in lookback_dirs:\n",
    "    print(\"\\n====================================================\")\n",
    "    print(f\"Building ranking dataset for: {lb_dir}\")\n",
    "    print(\"====================================================\")\n",
    "\n",
    "    regression_dir = os.path.join(REGRESSION_ROOT, lb_dir)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, lb_dir)\n",
    "    ver_dir = os.path.join(VER_ROOT, lb_dir)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(ver_dir, exist_ok=True)\n",
    "\n",
    "    validation_rows = []\n",
    "    rows = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        base_px = px_cache.get(ticker)\n",
    "\n",
    "        if base_px is None:\n",
    "            f_px = os.path.join(ADJ_PRICE_DIR, f\"{ticker}.parquet\")\n",
    "            log_issue(validation_rows, ticker, lb_dir, \"cannot_read_or_missing_required_px_columns\", f_px)\n",
    "            continue\n",
    "\n",
    "        df = base_px.copy()\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # Load regression file for this lookback\n",
    "        # --------------------------------------------------------\n",
    "        f_reg = os.path.join(regression_dir, f\"{ticker}.parquet\")\n",
    "\n",
    "        if not os.path.exists(f_reg):\n",
    "            log_issue(validation_rows, ticker, lb_dir, \"missing_regression_file\", f_reg)\n",
    "            df[\"slope_annual\"] = np.nan\n",
    "            df[\"r2\"] = np.nan\n",
    "            df[\"slope_adj\"] = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                rg = pd.read_parquet(f_reg)\n",
    "\n",
    "                # Expect at least date + slope_annual + r2\n",
    "                if not {\"date\", \"slope_annual\", \"r2\"}.issubset(rg.columns):\n",
    "                    log_issue(validation_rows, ticker, lb_dir, \"regression_missing_columns\", str(set([\"date\",\"slope_annual\",\"r2\"]) - set(rg.columns)))\n",
    "                    df[\"slope_annual\"] = np.nan\n",
    "                    df[\"r2\"] = np.nan\n",
    "                    df[\"slope_adj\"] = np.nan\n",
    "                else:\n",
    "                    rg = rg[[\"date\", \"slope_annual\", \"r2\"]].copy()\n",
    "                    rg[\"date\"] = pd.to_datetime(rg[\"date\"])\n",
    "\n",
    "                    df = df.merge(rg, on=\"date\", how=\"left\")\n",
    "                    df[\"slope_adj\"] = df[\"slope_annual\"] * df[\"r2\"]\n",
    "\n",
    "            except Exception as e:\n",
    "                log_issue(validation_rows, ticker, lb_dir, \"cannot_read_regression_file\", str(e))\n",
    "                df[\"slope_annual\"] = np.nan\n",
    "                df[\"r2\"] = np.nan\n",
    "                df[\"slope_adj\"] = np.nan\n",
    "\n",
    "        # Add ticker\n",
    "        df[\"ticker\"] = ticker\n",
    "        rows.append(df)\n",
    "\n",
    "    if not rows:\n",
    "        print(f\"❌ No rows produced for {lb_dir}. Skipping save.\")\n",
    "        continue\n",
    "\n",
    "    # ============================================================\n",
    "    # CONCAT + SORT FINAL DATA\n",
    "    # ============================================================\n",
    "\n",
    "    ranking_df = pd.concat(rows, ignore_index=True)\n",
    "    ranking_df = ranking_df.sort_values([\"date\", \"slope_adj\"], ascending=[True, False])\n",
    "\n",
    "    # ============================================================\n",
    "    # SAVE OUTPUTS\n",
    "    # ============================================================\n",
    "\n",
    "    ranking_path = os.path.join(out_dir, \"ranking_dataset.parquet\")\n",
    "    ranking_df.to_parquet(ranking_path, index=False)\n",
    "\n",
    "    val_df = pd.DataFrame(validation_rows)\n",
    "    val_path = os.path.join(ver_dir, f\"ranking_validation-{datetime.now().strftime('%Y%m%d-%H%M%S')}.csv\")\n",
    "    val_df.to_csv(val_path, index=False)\n",
    "\n",
    "    print(f\"\\n=== COMPLETED: {lb_dir} ===\")\n",
    "    print(f\"Saved ranking dataset → {ranking_path}\")\n",
    "    print(f\"Saved validation log  → {val_path}\")\n",
    "    print(f\"Rows: {len(ranking_df):,}\")\n",
    "\n",
    "print(\"\\n✅ All lookbacks processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
