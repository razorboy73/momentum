{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3306c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Building Enriched Universes (Add MA100 + Above/Below MA100) per Lookback ===\n",
      "\n",
      "Loading MA100 files...\n",
      "Loaded MA100 for 1167 tickers.\n",
      "Unreadable/invalid MA100 files: 0 tickers.\n",
      "\n",
      "Found 5 ranking datasets to process.\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_60D\n",
      "Ranking file: ./9a-multiple_regression_ranking_dataset\\lookback_60D\\ranking_dataset.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved enriched universe → ./10a-multiple_100MA_enriched_universe\\lookback_60D\\enriched_universe_ma100.parquet\n",
      "✔ Saved validation log   → ./system_verification/10a-multiple_100MA_enriched_universe\\lookback_60D\\validation_20251231-090606.csv\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_90D\n",
      "Ranking file: ./9a-multiple_regression_ranking_dataset\\lookback_90D\\ranking_dataset.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved enriched universe → ./10a-multiple_100MA_enriched_universe\\lookback_90D\\enriched_universe_ma100.parquet\n",
      "✔ Saved validation log   → ./system_verification/10a-multiple_100MA_enriched_universe\\lookback_90D\\validation_20251231-090606.csv\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_120D\n",
      "Ranking file: ./9a-multiple_regression_ranking_dataset\\lookback_120D\\ranking_dataset.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved enriched universe → ./10a-multiple_100MA_enriched_universe\\lookback_120D\\enriched_universe_ma100.parquet\n",
      "✔ Saved validation log   → ./system_verification/10a-multiple_100MA_enriched_universe\\lookback_120D\\validation_20251231-090606.csv\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_180D\n",
      "Ranking file: ./9a-multiple_regression_ranking_dataset\\lookback_180D\\ranking_dataset.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved enriched universe → ./10a-multiple_100MA_enriched_universe\\lookback_180D\\enriched_universe_ma100.parquet\n",
      "✔ Saved validation log   → ./system_verification/10a-multiple_100MA_enriched_universe\\lookback_180D\\validation_20251231-090606.csv\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_252D\n",
      "Ranking file: ./9a-multiple_regression_ranking_dataset\\lookback_252D\\ranking_dataset.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved enriched universe → ./10a-multiple_100MA_enriched_universe\\lookback_252D\\enriched_universe_ma100.parquet\n",
      "✔ Saved validation log   → ./system_verification/10a-multiple_100MA_enriched_universe\\lookback_252D\\validation_20251231-090606.csv\n",
      "\n",
      "=== COMPLETED ===\n",
      "Saved summary validation log → ./system_verification/10a-multiple_100MA_enriched_universe\\validation_summary_20251231-090606.csv\n",
      "Processed datasets: 5\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\"\n",
    "Purpose:\n",
    "    For each ranking dataset (one per regression lookback), add MA100 and above_ma100.\n",
    "\n",
    "Inputs:\n",
    "    Ranking datasets (per lookback):\n",
    "        ./9a-multiple_regression_ranking_dataset/lookback_XXD/ranking_dataset.parquet\n",
    "\n",
    "    MA100 per ticker:\n",
    "        ./5-100D_MA_adjusted_all_prices/{TICKER}.parquet  with columns ['date','ma100']\n",
    "\n",
    "Outputs (per lookback):\n",
    "    ./10a-multiple_100MA_enriched_universe/lookback_XXD/enriched_universe_ma100.parquet\n",
    "    ./system_verification/10a-multiple_100MA_enriched_universe/lookback_XXD/validation_<timestamp>.csv\n",
    "\n",
    "Also writes:\n",
    "    ./system_verification/10a-multiple_100MA_enriched_universe/validation_summary_<timestamp>.csv\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "RANKING_ROOT = \"./9a-multiple_regression_ranking_dataset\"\n",
    "MA100_DIR    = \"./5-100D_MA_adjusted_all_prices\"\n",
    "\n",
    "OUTPUT_ROOT  = \"./10a-multiple_100MA_enriched_universe\"\n",
    "VER_ROOT     = \"./system_verification/10a-multiple_100MA_enriched_universe\"\n",
    "\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "os.makedirs(VER_ROOT, exist_ok=True)\n",
    "\n",
    "RANKING_FILENAME = \"ranking_dataset.parquet\"\n",
    "OUT_FILENAME     = \"enriched_universe_ma100.parquet\"\n",
    "\n",
    "print(\"=== Building Enriched Universes (Add MA100 + Above/Below MA100) per Lookback ===\")\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def discover_lookback_dirs(root: str):\n",
    "    \"\"\"\n",
    "    Finds subdirectories like lookback_60D, lookback_90D, etc., sorted by numeric window.\n",
    "    \"\"\"\n",
    "    pat = re.compile(r\"^lookback_(\\d+)D$\")\n",
    "    found = []\n",
    "    for name in os.listdir(root):\n",
    "        full = os.path.join(root, name)\n",
    "        if os.path.isdir(full):\n",
    "            m = pat.match(name)\n",
    "            if m:\n",
    "                found.append((int(m.group(1)), name))\n",
    "    found.sort(key=lambda x: x[0])\n",
    "    return [name for _, name in found]\n",
    "\n",
    "def safe_read_parquet(path: str):\n",
    "    try:\n",
    "        return pd.read_parquet(path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ============================================================\n",
    "# 1) LOAD MA100 FILES ONCE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nLoading MA100 files...\")\n",
    "\n",
    "ma100_map = {}\n",
    "missing_ma100_files = []\n",
    "bad_ma100_files = []\n",
    "\n",
    "for fname in os.listdir(MA100_DIR):\n",
    "    if not fname.endswith(\".parquet\"):\n",
    "        continue\n",
    "\n",
    "    ticker = fname.replace(\".parquet\", \"\")\n",
    "    fpath = os.path.join(MA100_DIR, fname)\n",
    "\n",
    "    tmp = safe_read_parquet(fpath)\n",
    "    if tmp is None:\n",
    "        bad_ma100_files.append(ticker)\n",
    "        continue\n",
    "\n",
    "    if not {\"date\", \"ma100\"}.issubset(tmp.columns):\n",
    "        bad_ma100_files.append(ticker)\n",
    "        continue\n",
    "\n",
    "    tmp = tmp[[\"date\", \"ma100\"]].copy()\n",
    "    tmp[\"date\"] = pd.to_datetime(tmp[\"date\"])\n",
    "    ma100_map[ticker] = tmp\n",
    "\n",
    "print(f\"Loaded MA100 for {len(ma100_map)} tickers.\")\n",
    "print(f\"Unreadable/invalid MA100 files: {len(bad_ma100_files)} tickers.\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) DISCOVER RANKING DATASETS (PER LOOKBACK)\n",
    "# ============================================================\n",
    "\n",
    "lookback_dirs = discover_lookback_dirs(RANKING_ROOT)\n",
    "\n",
    "# If no lookback subdirs found, fall back to a single ranking file at root (optional)\n",
    "ranking_jobs = []\n",
    "if lookback_dirs:\n",
    "    for lb in lookback_dirs:\n",
    "        rfile = os.path.join(RANKING_ROOT, lb, RANKING_FILENAME)\n",
    "        if os.path.exists(rfile):\n",
    "            ranking_jobs.append((lb, rfile))\n",
    "        else:\n",
    "            print(f\"⚠ Missing ranking file for {lb}: {rfile}\")\n",
    "else:\n",
    "    root_file = os.path.join(RANKING_ROOT, RANKING_FILENAME)\n",
    "    if os.path.exists(root_file):\n",
    "        ranking_jobs.append((\"root\", root_file))\n",
    "\n",
    "if not ranking_jobs:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No ranking datasets found. Expected {RANKING_ROOT}/lookback_XXD/{RANKING_FILENAME} \"\n",
    "        f\"(or {RANKING_ROOT}/{RANKING_FILENAME}).\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nFound {len(ranking_jobs)} ranking datasets to process.\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) PROCESS EACH RANKING DATASET\n",
    "# ============================================================\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "summary_rows = []\n",
    "\n",
    "for lookback, ranking_path in ranking_jobs:\n",
    "    print(\"\\n----------------------------------------------------\")\n",
    "    print(f\"Processing lookback: {lookback}\")\n",
    "    print(f\"Ranking file: {ranking_path}\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "\n",
    "    df = safe_read_parquet(ranking_path)\n",
    "    if df is None or df.empty:\n",
    "        print(f\"❌ Could not read or empty ranking dataset: {ranking_path}\")\n",
    "        continue\n",
    "\n",
    "    # basic expectations\n",
    "    if \"date\" not in df.columns or \"ticker\" not in df.columns:\n",
    "        print(f\"❌ ranking dataset missing required columns 'date'/'ticker': {ranking_path}\")\n",
    "        continue\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    # merge MA100 per ticker (same approach as your original)\n",
    "    rows = []\n",
    "    missing_tickers = []\n",
    "\n",
    "    for ticker, sub in df.groupby(\"ticker\", sort=False):\n",
    "        if ticker in ma100_map:\n",
    "            merged = sub.merge(ma100_map[ticker], on=\"date\", how=\"left\")\n",
    "        else:\n",
    "            merged = sub.copy()\n",
    "            merged[\"ma100\"] = np.nan\n",
    "            missing_tickers.append(ticker)\n",
    "        rows.append(merged)\n",
    "\n",
    "    df2 = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # add boolean\n",
    "    # if ma100 is NaN, above_ma100 becomes False (because comparison yields False)\n",
    "    df2[\"above_ma100\"] = df2[\"close_adj\"] > df2[\"ma100\"]\n",
    "\n",
    "    # sort (optional but consistent)\n",
    "    df2 = df2.sort_values([\"date\", \"ticker\"]).reset_index(drop=True)\n",
    "\n",
    "    # output dirs\n",
    "    out_dir = OUTPUT_ROOT if lookback == \"root\" else os.path.join(OUTPUT_ROOT, lookback)\n",
    "    ver_dir = VER_ROOT if lookback == \"root\" else os.path.join(VER_ROOT, lookback)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(ver_dir, exist_ok=True)\n",
    "\n",
    "    out_path = os.path.join(out_dir, OUT_FILENAME)\n",
    "    df2.to_parquet(out_path, index=False)\n",
    "\n",
    "    # validation log per lookback\n",
    "    val = pd.DataFrame([{\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"lookback\": lookback,\n",
    "        \"ranking_file\": ranking_path,\n",
    "        \"rows_in_ranking\": int(df.shape[0]),\n",
    "        \"rows_after_merge\": int(df2.shape[0]),\n",
    "        \"unique_tickers\": int(df[\"ticker\"].nunique()),\n",
    "        \"missing_ma100_tickers\": int(len(set(missing_tickers))),\n",
    "        \"null_ma100_values\": int(df2[\"ma100\"].isna().sum()),\n",
    "        \"true_above_ma100\": int(df2[\"above_ma100\"].sum()),\n",
    "        \"false_above_ma100\": int((~df2[\"above_ma100\"]).sum()),\n",
    "        \"output_file\": out_path,\n",
    "    }])\n",
    "\n",
    "    val_path = os.path.join(ver_dir, f\"validation_{ts}.csv\")\n",
    "    val.to_csv(val_path, index=False)\n",
    "\n",
    "    print(f\"✔ Saved enriched universe → {out_path}\")\n",
    "    print(f\"✔ Saved validation log   → {val_path}\")\n",
    "\n",
    "    summary_rows.append(val.iloc[0].to_dict())\n",
    "\n",
    "# ============================================================\n",
    "# 4) SAVE OVERALL SUMMARY LOG\n",
    "# ============================================================\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_path = os.path.join(VER_ROOT, f\"validation_summary_{ts}.csv\")\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"\\n=== COMPLETED ===\")\n",
    "print(f\"Saved summary validation log → {summary_path}\")\n",
    "print(f\"Processed datasets: {len(summary_df):,}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
