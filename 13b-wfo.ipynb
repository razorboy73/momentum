{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f387bf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAILY_RETURNS_DIR resolved: C:\\TWS API\\source\\pythonclient\\TradingIdeas\\MomentumSystem\\13a-trading_output_sweep_performance_daily_returns\\daily_returns\\20260105-142026\n",
      "Exists? True\n",
      "Sample files: ['lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0050.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0100.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0200.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0300.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0500_mnw0050.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0500_mnw0100.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0500_mnw0200.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0500_mnw0300.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr1000_mnw0050.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr1000_mnw0100.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr1000_mnw0200.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr1000_mnw0300.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0200_mnw0050.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0200_mnw0100.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0200_mnw0200.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0200_mnw0300.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0500_mnw0050.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0500_mnw0100.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0500_mnw0200.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0500_mnw0300.parquet']\n",
      "Daily returns dir: 13a-trading_output_sweep_performance_daily_returns\\daily_returns\\20260105-142026\n",
      "Found 48 parquet, 0 csv files.\n",
      "\n",
      "Top 48 UNIQUE configs by full-sample CAGR (after de-dupe): 48\n",
      "\n",
      "Loaded daily returns for 48/48 configs.\n",
      "\n",
      "25 walk-forward windows (1y train / 1y trade).\n",
      "\n",
      "=== Walk-forward window results ===\n",
      "   train_start test_start   test_end  \\\n",
      "0   1999-01-01 2000-01-01 2001-01-01   \n",
      "1   2000-01-01 2001-01-01 2002-01-01   \n",
      "2   2001-01-01 2002-01-01 2003-01-01   \n",
      "3   2002-01-01 2003-01-01 2004-01-01   \n",
      "4   2003-01-01 2004-01-01 2005-01-01   \n",
      "5   2004-01-01 2005-01-01 2006-01-01   \n",
      "6   2005-01-01 2006-01-01 2007-01-01   \n",
      "7   2006-01-01 2007-01-01 2008-01-01   \n",
      "8   2007-01-01 2008-01-01 2009-01-01   \n",
      "9   2008-01-01 2009-01-01 2010-01-01   \n",
      "10  2009-01-01 2010-01-01 2011-01-01   \n",
      "11  2010-01-01 2011-01-01 2012-01-01   \n",
      "12  2011-01-01 2012-01-01 2013-01-01   \n",
      "13  2012-01-01 2013-01-01 2014-01-01   \n",
      "14  2013-01-01 2014-01-01 2015-01-01   \n",
      "15  2014-01-01 2015-01-01 2016-01-01   \n",
      "16  2015-01-01 2016-01-01 2017-01-01   \n",
      "17  2016-01-01 2017-01-01 2018-01-01   \n",
      "18  2017-01-01 2018-01-01 2019-01-01   \n",
      "19  2018-01-01 2019-01-01 2020-01-01   \n",
      "20  2019-01-01 2020-01-01 2021-01-01   \n",
      "21  2020-01-01 2021-01-01 2022-01-01   \n",
      "22  2021-01-01 2022-01-01 2023-01-01   \n",
      "23  2022-01-01 2023-01-01 2024-01-01   \n",
      "24  2023-01-01 2024-01-01 2025-01-01   \n",
      "\n",
      "                                    chosen_config_key  train_cagr  \\\n",
      "0   (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.811242   \n",
      "1    (90, 20, 0.95, 5000.0, 0.12, 5000.0, 0.02, 0.01)    0.389344   \n",
      "2    (90, 20, 0.95, 5000.0, 0.12, 10000.0, 0.1, 0.02)    0.000000   \n",
      "3   (90, 20, 0.95, 10000.0, 0.12, 10000.0, 0.02, 0...   -0.001798   \n",
      "4   (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.356737   \n",
      "5   (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.05, 0.03)    0.200853   \n",
      "6   (90, 20, 0.95, 10000.0, 0.12, 10000.0, 0.02, 0...    0.197380   \n",
      "7   (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.171672   \n",
      "8   (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.236614   \n",
      "9   (90, 20, 0.95, 5000.0, 0.12, 10000.0, 0.05, 0.03)   -0.028385   \n",
      "10  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.525453   \n",
      "11  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.205475   \n",
      "12  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.03)    0.052729   \n",
      "13  (90, 20, 0.95, 5000.0, 0.12, 10000.0, 0.02, 0.03)    0.292260   \n",
      "14  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.456951   \n",
      "15  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.03)    0.114111   \n",
      "16  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.05, 0.03)    0.151579   \n",
      "17  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.324509   \n",
      "18  (90, 20, 0.95, 10000.0, 0.12, 10000.0, 0.02, 0...    0.189634   \n",
      "19  (90, 20, 0.95, 10000.0, 0.12, 10000.0, 0.05, 0...    0.083926   \n",
      "20  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.03)    0.180399   \n",
      "21  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.05, 0.03)    0.331561   \n",
      "22   (90, 20, 0.95, 5000.0, 0.12, 5000.0, 0.02, 0.01)    0.279031   \n",
      "23  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.05, 0.03)    0.023171   \n",
      "24  (90, 20, 0.95, 5000.0, 0.12, 5000.0, 0.05, 0.005)    0.060502   \n",
      "\n",
      "    train_sharpe  train_dd  train_calmar  test_cagr  test_sharpe   test_dd  \\\n",
      "0       2.793691 -0.078651     10.314508   0.386707     1.205914 -0.246284   \n",
      "1       1.213106 -0.246254      1.581065  -0.005466    -0.956441 -0.008619   \n",
      "2       0.000000  0.000000           NaN  -0.003571    -0.141686 -0.023227   \n",
      "3       0.002411 -0.054264     -0.033134   0.341003     1.721187 -0.116802   \n",
      "4       1.620214 -0.130974      2.723721   0.157751     1.124882 -0.096065   \n",
      "5       2.120142 -0.047293      4.247039   0.146324     1.372917 -0.083927   \n",
      "6       1.384657 -0.105066      1.878630   0.169625     1.306294 -0.132054   \n",
      "7       1.245235 -0.145642      1.178719   0.236614     1.223588 -0.150344   \n",
      "8       1.223588 -0.150344      1.573815  -0.057348    -0.868279 -0.095100   \n",
      "9      -0.737837 -0.050987     -0.556698   0.372740     1.917704 -0.104995   \n",
      "10      1.764919 -0.165411      3.176646   0.205475     1.026087 -0.186381   \n",
      "11      1.026087 -0.186381      1.102446   0.052514     0.380877 -0.152338   \n",
      "12      0.402560 -0.146770      0.359261   0.292138     1.779395 -0.121658   \n",
      "13      1.782486 -0.121463      2.406169   0.421581     2.381149 -0.072740   \n",
      "14      2.377219 -0.078811      5.798095   0.101961     0.865684 -0.093887   \n",
      "15      1.049342 -0.079305      1.438891   0.130604     1.086389 -0.081937   \n",
      "16      1.519737 -0.068142      2.224457   0.253546     1.900694 -0.078791   \n",
      "17      1.730406 -0.103360      3.139605   0.188614     1.432494 -0.048909   \n",
      "18      1.445134 -0.048894      3.878470  -0.014540    -0.011486 -0.145151   \n",
      "19      0.758058 -0.085288      0.984033   0.133666     1.390823 -0.057497   \n",
      "20      1.536245 -0.076887      2.346275   0.296368     1.130931 -0.282053   \n",
      "21      1.474027 -0.197481      1.678950   0.227117     1.172232 -0.083718   \n",
      "22      1.167575 -0.129647      2.152246  -0.033096    -0.127958 -0.132365   \n",
      "23      0.241709 -0.103938      0.222934   0.060460     0.558561 -0.129540   \n",
      "24      0.558913 -0.129505      0.467181   0.093703     0.850164 -0.067126   \n",
      "\n",
      "    test_calmar  n_test_days  \n",
      "0      1.570167          252  \n",
      "1     -0.634244          248  \n",
      "2     -0.153739          252  \n",
      "3      2.919510          252  \n",
      "4      1.642131          252  \n",
      "5      1.743466          252  \n",
      "6      1.284509          251  \n",
      "7      1.573815          251  \n",
      "8     -0.603031          253  \n",
      "9      3.550073          252  \n",
      "10     1.102446          252  \n",
      "11     0.344719          252  \n",
      "12     2.401311          250  \n",
      "13     5.795719          252  \n",
      "14     1.086000          252  \n",
      "15     1.593954          252  \n",
      "16     3.217961          252  \n",
      "17     3.856414          251  \n",
      "18    -0.100171          251  \n",
      "19     2.324739          252  \n",
      "20     1.050753          253  \n",
      "21     2.712866          252  \n",
      "22    -0.250032          251  \n",
      "23     0.466733          250  \n",
      "24     1.395932          252  \n",
      "\n",
      "=== Overall walk-forward performance ===\n",
      "WFO CAGR   : 15.834%\n",
      "WFO Sharpe : 1.02\n",
      "WFO MaxDD  : -28.21%\n",
      "WFO Calmar : 0.56\n",
      "Saved: ./13b-wfo\\walkforward_top48_windows.csv\n",
      "Saved: ./13b-wfo\\walkforward_top48_equity_curve.csv\n",
      "\n",
      "=== COMPLETE ===\n",
      "All outputs saved to: ./13b-wfo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TRADING_DAYS = 248\n",
    "\n",
    "# =========================\n",
    "# INPUTS\n",
    "# =========================\n",
    "\n",
    "\n",
    "SWEEP_RESULTS_CSV = \"./13a-trading_output_sweep_performance_daily_returns/sweep_summary_20260105-142026.csv\"\n",
    "DAILY_RETURNS_DIR = \"./13a-trading_output_sweep_performance_daily_returns/daily_returns/20260105-142026\"\n",
    "\n",
    "TOP_N = 48\n",
    "\n",
    "TRAIN_YEARS = 1\n",
    "TEST_YEARS  = 1\n",
    "\n",
    "START_DATE = pd.Timestamp(\"1999-01-01\")\n",
    "END_DATE   = pd.Timestamp(\"2025-12-24\")\n",
    "\n",
    "SELECTION_METRIC = \"cagr\"  # \"cagr\" | \"sharpe\" | \"calmar\"\n",
    "\n",
    "SAVE_WF_RESULTS_CSV = True\n",
    "SAVE_EQUITY_CURVE_CSV = True\n",
    "\n",
    "# =========================\n",
    "# OUTPUT DIRECTORY\n",
    "# =========================\n",
    "OUTPUT_DIR = \"./13b-wfo\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_PREFIX = \"walkforward\"\n",
    "\n",
    "# =========================\n",
    "# CONFIG ID + KEYS (NOW INCLUDE ALL SWEPT PARAMS)\n",
    "# =========================\n",
    "def make_config_id(row: dict) -> str:\n",
    "    \"\"\"\n",
    "    Must MATCH the sweep daily-returns writer naming.\n",
    "    Includes all swept params so filenames are unique.\n",
    "    \"\"\"\n",
    "    lb  = int(row[\"lookback_days\"])\n",
    "    atr = int(row[\"atr_days\"])\n",
    "    tp  = int(round(float(row[\"top_percentile\"]) * 1000))         # 0.95 -> 950\n",
    "    mtv = int(round(float(row[\"min_trade_value\"])))\n",
    "\n",
    "    mpw = int(round(float(row[\"max_position_weight\"]) * 1000))    # 0.12 -> 120\n",
    "    mcr = int(round(float(row[\"min_cash_reserve\"])))              # 5000.0 -> 5000\n",
    "    dr  = int(round(float(row[\"drift_threshold\"]) * 10000))       # 0.05 -> 500\n",
    "    mnw = int(round(float(row[\"min_new_position_weight\"]) * 10000))  # 0.005 -> 50\n",
    "\n",
    "    return f\"lb{lb}_atr{atr}_tp{tp:04d}_mtv{mtv}_mpw{mpw}_cash{mcr}_dr{dr}_mnw{mnw}\"\n",
    "\n",
    "def config_key(c):\n",
    "    return (\n",
    "        int(c[\"lookback_days\"]),\n",
    "        int(c[\"atr_days\"]),\n",
    "        float(c[\"top_percentile\"]),\n",
    "        float(c[\"min_trade_value\"]),\n",
    "        float(c[\"max_position_weight\"]),\n",
    "        float(c[\"min_cash_reserve\"]),\n",
    "        float(c[\"drift_threshold\"]),\n",
    "        float(c[\"min_new_position_weight\"]),\n",
    "    )\n",
    "\n",
    "def config_id_from_row(lookback, atr, top_percentile, min_trade_value,\n",
    "                       max_position_weight, min_cash_reserve,\n",
    "                       drift_threshold, min_new_position_weight):\n",
    "\n",
    "    tp = int(round(float(top_percentile) * 1000))       # 0.95 -> 950  => tp0950\n",
    "    mtv_i = int(round(float(min_trade_value)))          # 10000 -> mtv10000\n",
    "\n",
    "    cap_i = int(round(float(max_position_weight) * 10000))      # 0.12 -> 1200 => cap1200\n",
    "    cash_i = int(round(float(min_cash_reserve)))                # 5000 -> cash5000\n",
    "    dr_i = int(round(float(drift_threshold) * 10000))           # 0.02 -> 200 => dr0200\n",
    "    mnw_i = int(round(float(min_new_position_weight) * 10000))  # 0.005 -> 50 => mnw0050\n",
    "\n",
    "    return (\n",
    "        f\"lb{int(lookback)}_atr{int(atr)}_tp{tp:04d}_mtv{mtv_i}\"\n",
    "        f\"_cap{cap_i:04d}_cash{cash_i}_dr{dr_i:04d}_mnw{mnw_i:04d}\"\n",
    "    )\n",
    "\n",
    "from pathlib import Path\n",
    "p = Path(DAILY_RETURNS_DIR)\n",
    "print(\"DAILY_RETURNS_DIR resolved:\", p.resolve())\n",
    "print(\"Exists?\", p.exists())\n",
    "print(\"Sample files:\", [x.name for x in list(p.glob(\"*\"))[:20]])\n",
    "\n",
    "def load_config_returns(base_dir: str | Path, cfg) -> pd.DataFrame:\n",
    "    base_dir = Path(base_dir)\n",
    "\n",
    "    cfg_id = config_id_from_row(\n",
    "        cfg[\"lookback_days\"], cfg[\"atr_days\"], cfg[\"top_percentile\"], cfg[\"min_trade_value\"],\n",
    "        cfg[\"max_position_weight\"], cfg[\"min_cash_reserve\"], cfg[\"drift_threshold\"], cfg[\"min_new_position_weight\"]\n",
    "    )\n",
    "\n",
    "    pq = base_dir / f\"{cfg_id}.parquet\"\n",
    "    csv = base_dir / f\"{cfg_id}.csv\"\n",
    "\n",
    "    if pq.exists():\n",
    "        df = pd.read_parquet(pq)\n",
    "    elif csv.exists():\n",
    "        df = pd.read_csv(csv)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Missing daily returns file for {cfg_id} in {base_dir}\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"date\")\n",
    "\n",
    "    if \"strat_ret\" not in df.columns:\n",
    "        raise ValueError(f\"{cfg_id} missing 'strat_ret'\")\n",
    "    if \"spy_ret\" not in df.columns:\n",
    "        df[\"spy_ret\"] = np.nan\n",
    "\n",
    "    return df[[\"date\", \"strat_ret\", \"spy_ret\"]]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# METRICS\n",
    "# =========================\n",
    "def annualized_cagr_from_returns(rets: pd.Series) -> float:\n",
    "    if rets.empty:\n",
    "        return np.nan\n",
    "    total = float((1.0 + rets).prod())\n",
    "    years = len(rets) / TRADING_DAYS\n",
    "    if years <= 0:\n",
    "        return np.nan\n",
    "    return total ** (1.0 / years) - 1.0\n",
    "\n",
    "def annualized_sharpe(rets: pd.Series) -> float:\n",
    "    if rets.empty:\n",
    "        return np.nan\n",
    "    sd = float(rets.std(ddof=0))\n",
    "    if sd <= 0 or np.isnan(sd):\n",
    "        return 0.0\n",
    "    return np.sqrt(TRADING_DAYS) * float(rets.mean()) / sd\n",
    "\n",
    "def max_drawdown_from_returns(rets: pd.Series) -> float:\n",
    "    if rets.empty:\n",
    "        return np.nan\n",
    "    equity = (1.0 + rets).cumprod()\n",
    "    dd = equity / equity.cummax() - 1.0\n",
    "    return float(dd.min())\n",
    "\n",
    "def score_training_slice(train_rets: pd.Series, metric: str):\n",
    "    c = annualized_cagr_from_returns(train_rets)\n",
    "    s = annualized_sharpe(train_rets)\n",
    "    dd = max_drawdown_from_returns(train_rets)\n",
    "    calmar = c / abs(dd) if (dd is not None and not np.isnan(dd) and dd != 0) else np.nan\n",
    "\n",
    "    if metric == \"cagr\":\n",
    "        return c, {\"cagr\": c, \"sharpe\": s, \"dd\": dd, \"calmar\": calmar}\n",
    "    if metric == \"sharpe\":\n",
    "        return s, {\"cagr\": c, \"sharpe\": s, \"dd\": dd, \"calmar\": calmar}\n",
    "    if metric == \"calmar\":\n",
    "        return calmar, {\"cagr\": c, \"sharpe\": s, \"dd\": dd, \"calmar\": calmar}\n",
    "    raise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "# =========================\n",
    "# 0) SANITY CHECK DIRECTORY\n",
    "# =========================\n",
    "daily_dir = Path(DAILY_RETURNS_DIR)\n",
    "if not daily_dir.exists():\n",
    "    raise FileNotFoundError(f\"DAILY_RETURNS_DIR not found: {daily_dir}\")\n",
    "\n",
    "parquets = sorted(daily_dir.glob(\"*.parquet\"))\n",
    "csvs = sorted(daily_dir.glob(\"*.csv\"))\n",
    "print(f\"Daily returns dir: {daily_dir}\")\n",
    "print(f\"Found {len(parquets)} parquet, {len(csvs)} csv files.\")\n",
    "\n",
    "if len(parquets) == 0 and len(csvs) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"No daily return files found in DAILY_RETURNS_DIR. \"\n",
    "        \"Either WRITE_DAILY_RETURNS was off, the RUN_ID folder is wrong, \"\n",
    "        \"or files were written to a different directory.\"\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# 1) LOAD SWEEP SUMMARY + PICK TOP_N UNIQUE CONFIGS\n",
    "# =========================\n",
    "df = pd.read_csv(SWEEP_RESULTS_CSV)\n",
    "\n",
    "required_cols = [\n",
    "    \"lookback_days\",\"atr_days\",\"top_percentile\",\"min_trade_value\",\n",
    "    \"max_position_weight\",\"min_cash_reserve\",\"drift_threshold\",\"min_new_position_weight\",\n",
    "    \"strat_cagr\"\n",
    "]\n",
    "missing_cols = [c for c in required_cols if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Sweep summary missing columns needed for expanded WFO: {missing_cols}\")\n",
    "\n",
    "# Sort best-to-worst and de-dupe by FULL config definition\n",
    "df_sorted = df.sort_values(\"strat_cagr\", ascending=False).copy()\n",
    "df_sorted[\"_cfgkey\"] = df_sorted.apply(lambda r: config_key(r.to_dict()), axis=1)\n",
    "df_unique = df_sorted.drop_duplicates(\"_cfgkey\", keep=\"first\").drop(columns=[\"_cfgkey\"])\n",
    "\n",
    "topN = (\n",
    "    df.sort_values(\"strat_cagr\", ascending=False)\n",
    "      .head(TOP_N)[[\n",
    "          \"lookback_days\",\"atr_days\",\"top_percentile\",\"min_trade_value\",\n",
    "          \"max_position_weight\",\"min_cash_reserve\",\"drift_threshold\",\"min_new_position_weight\",\n",
    "          \"strat_cagr\"\n",
    "      ]]\n",
    "      .to_dict(\"records\")\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"\\nTop {TOP_N} UNIQUE configs by full-sample CAGR (after de-dupe): {len(topN)}\")\n",
    "\n",
    "# =========================\n",
    "# 2) LOAD DAILY RETURNS FOR THOSE CONFIGS\n",
    "# =========================\n",
    "returns_data = {}\n",
    "missing = []\n",
    "\n",
    "for row in topN:\n",
    "    k = config_key(row)\n",
    "    try:\n",
    "        returns_data[k] = load_config_returns(DAILY_RETURNS_DIR, row)\n",
    "    except Exception as e:\n",
    "        missing.append((make_config_id(row), str(e)))\n",
    "\n",
    "if missing:\n",
    "    print(\"\\nWARNING: Some top configs are missing daily return files:\")\n",
    "    for cfg_id, msg in missing[:25]:\n",
    "        print(\" \", cfg_id, \"->\", msg)\n",
    "    if len(missing) > 25:\n",
    "        print(f\" ... and {len(missing)-25} more\")\n",
    "\n",
    "top_loaded = [row for row in topN if config_key(row) in returns_data]\n",
    "if not top_loaded:\n",
    "    raise RuntimeError(\n",
    "        \"Could not load daily returns for ANY of the top configs.\\n\"\n",
    "        \"Most likely: your sweep writer didn't create daily return files, \"\n",
    "        \"or the filename scheme doesn't match make_config_id().\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nLoaded daily returns for {len(top_loaded)}/{len(topN)} configs.\")\n",
    "\n",
    "# =========================\n",
    "# 3) BUILD WALK-FORWARD WINDOWS (YEAR-START ANCHORS)\n",
    "# =========================\n",
    "test_starts = pd.date_range(START_DATE, END_DATE, freq=\"YS\")\n",
    "windows = []\n",
    "for test_start in test_starts:\n",
    "    train_start = test_start - pd.DateOffset(years=TRAIN_YEARS)\n",
    "    test_end    = test_start + pd.DateOffset(years=TEST_YEARS)\n",
    "    if train_start < START_DATE:\n",
    "        continue\n",
    "    if test_end > END_DATE:\n",
    "        break\n",
    "    windows.append((train_start, test_start, test_end))\n",
    "\n",
    "print(f\"\\n{len(windows)} walk-forward windows ({TRAIN_YEARS}y train / {TEST_YEARS}y trade).\")\n",
    "if not windows:\n",
    "    raise RuntimeError(\"No valid windows. Check START_DATE / END_DATE / TRAIN_YEARS.\")\n",
    "\n",
    "# =========================\n",
    "# 4) WALK-FORWARD LOOP\n",
    "# =========================\n",
    "wf_rows = []\n",
    "equity_parts = []\n",
    "equity_level = 1.0\n",
    "\n",
    "for (train_start, test_start, test_end) in windows:\n",
    "    scores = []\n",
    "    for row in top_loaded:\n",
    "        k = config_key(row)\n",
    "        r = returns_data[k]\n",
    "\n",
    "        train = r.loc[(r[\"date\"] >= train_start) & (r[\"date\"] < test_start)]\n",
    "        if len(train) < TRADING_DAYS:\n",
    "            continue\n",
    "\n",
    "        score, stats = score_training_slice(train[\"strat_ret\"], SELECTION_METRIC)\n",
    "        if np.isnan(score):\n",
    "            continue\n",
    "\n",
    "        scores.append((k, score, stats))\n",
    "\n",
    "    if not scores:\n",
    "        print(f\"Skipping window {train_start.date()} → {test_end.date()} (no configs had enough training data).\")\n",
    "        continue\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    best_key, best_score, best_train_stats = scores[0]\n",
    "\n",
    "    test = returns_data[best_key].loc[\n",
    "        (returns_data[best_key][\"date\"] >= test_start) &\n",
    "        (returns_data[best_key][\"date\"] < test_end)\n",
    "    ].copy()\n",
    "\n",
    "    if test.empty:\n",
    "        print(f\"Skipping OOS period {test_start.date()} → {test_end.date()} (no test data).\")\n",
    "        continue\n",
    "\n",
    "    test = test.sort_values(\"date\")\n",
    "    test[\"equity\"] = equity_level * (1.0 + test[\"strat_ret\"]).cumprod()\n",
    "    equity_level = float(test[\"equity\"].iloc[-1])\n",
    "\n",
    "    test_cagr   = annualized_cagr_from_returns(test[\"strat_ret\"])\n",
    "    test_sharpe = annualized_sharpe(test[\"strat_ret\"])\n",
    "    test_dd     = max_drawdown_from_returns(test[\"strat_ret\"])\n",
    "    test_calmar = test_cagr / abs(test_dd) if (test_dd is not None and not np.isnan(test_dd) and test_dd != 0) else np.nan\n",
    "\n",
    "    wf_rows.append({\n",
    "        \"train_start\": train_start,\n",
    "        \"test_start\": test_start,\n",
    "        \"test_end\": test_end,\n",
    "        \"chosen_config_key\": str(best_key),\n",
    "        f\"train_{SELECTION_METRIC}\": best_score,\n",
    "        \"train_cagr\": best_train_stats[\"cagr\"],\n",
    "        \"train_sharpe\": best_train_stats[\"sharpe\"],\n",
    "        \"train_dd\": best_train_stats[\"dd\"],\n",
    "        \"train_calmar\": best_train_stats[\"calmar\"],\n",
    "        \"test_cagr\": test_cagr,\n",
    "        \"test_sharpe\": test_sharpe,\n",
    "        \"test_dd\": test_dd,\n",
    "        \"test_calmar\": test_calmar,\n",
    "        \"n_test_days\": int(len(test)),\n",
    "    })\n",
    "\n",
    "    equity_parts.append(test[[\"date\", \"equity\"]])\n",
    "\n",
    "wf_df = pd.DataFrame(wf_rows).sort_values(\"test_start\").reset_index(drop=True)\n",
    "print(\"\\n=== Walk-forward window results ===\")\n",
    "print(wf_df)\n",
    "\n",
    "if not equity_parts:\n",
    "    raise RuntimeError(\"No equity curve produced (all windows skipped).\")\n",
    "\n",
    "equity_curve = pd.concat(equity_parts, ignore_index=True).drop_duplicates(\"date\").sort_values(\"date\")\n",
    "equity_curve = equity_curve.set_index(\"date\")\n",
    "\n",
    "overall_rets = equity_curve[\"equity\"].pct_change().fillna(0.0)\n",
    "overall_cagr = annualized_cagr_from_returns(overall_rets)\n",
    "overall_sharpe = annualized_sharpe(overall_rets)\n",
    "overall_dd = max_drawdown_from_returns(overall_rets)\n",
    "overall_calmar = overall_cagr / abs(overall_dd) if overall_dd != 0 else np.nan\n",
    "\n",
    "print(\"\\n=== Overall walk-forward performance ===\")\n",
    "print(f\"WFO CAGR   : {overall_cagr:.3%}\")\n",
    "print(f\"WFO Sharpe : {overall_sharpe:.2f}\")\n",
    "print(f\"WFO MaxDD  : {overall_dd:.2%}\")\n",
    "print(f\"WFO Calmar : {overall_calmar:.2f}\")\n",
    "\n",
    "# =========================\n",
    "# 5) SAVE OUTPUTS TO 13b-wfo\n",
    "# =========================\n",
    "if SAVE_WF_RESULTS_CSV:\n",
    "    out1 = os.path.join(OUTPUT_DIR, f\"{OUT_PREFIX}_top{TOP_N}_windows.csv\")\n",
    "    wf_df.to_csv(out1, index=False)\n",
    "    print(f\"Saved: {out1}\")\n",
    "\n",
    "if SAVE_EQUITY_CURVE_CSV:\n",
    "    out2 = os.path.join(OUTPUT_DIR, f\"{OUT_PREFIX}_top{TOP_N}_equity_curve.csv\")\n",
    "    equity_curve.reset_index().to_csv(out2, index=False)\n",
    "    print(f\"Saved: {out2}\")\n",
    "\n",
    "print(f\"\\n=== COMPLETE ===\")\n",
    "print(f\"All outputs saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44dc79ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0050.parquet\n",
      "Date range: 1999-01-04 00:00:00 to 2025-12-30 00:00:00\n",
      "Total rows: 6790\n",
      "\n",
      "Rows per year:\n",
      "date\n",
      "1999    252\n",
      "2000    252\n",
      "2001    248\n",
      "2002    252\n",
      "2003    252\n",
      "2004    252\n",
      "2005    252\n",
      "2006    251\n",
      "2007    251\n",
      "2008    253\n",
      "2009    252\n",
      "2010    252\n",
      "2011    252\n",
      "2012    250\n",
      "2013    252\n",
      "2014    252\n",
      "2015    252\n",
      "2016    252\n",
      "2017    251\n",
      "2018    251\n",
      "2019    252\n",
      "2020    253\n",
      "2021    252\n",
      "2022    251\n",
      "2023    250\n",
      "2024    252\n",
      "2025    249\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DAILY_RETURNS_DIR = \"./13a-trading_output_sweep_performance_daily_returns/daily_returns/20260105-142026\"\n",
    "\n",
    "# Load one file and check its date range\n",
    "files = list(Path(DAILY_RETURNS_DIR).glob(\"*.parquet\"))\n",
    "if files:\n",
    "    df = pd.read_parquet(files[0])\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"date\")\n",
    "    \n",
    "    print(f\"File: {files[0].name}\")\n",
    "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"\\nRows per year:\")\n",
    "    print(df.groupby(df[\"date\"].dt.year).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "531180c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top config values: {'lookback_days': 90, 'atr_days': 20, 'top_percentile': 0.95, 'min_trade_value': 10000.0, 'max_position_weight': 0.12, 'min_cash_reserve': 5000.0, 'drift_threshold': 0.02, 'min_new_position_weight': 0.02, 'strat_cagr': 0.1965560535444952}\n",
      "Generated config_id: lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0200_mnw0200\n",
      "\n",
      "Actual files in directory:\n",
      "  lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0050.parquet\n",
      "  lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0100.parquet\n",
      "  lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0200.parquet\n",
      "  lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0300.parquet\n",
      "  lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0500_mnw0050.parquet\n"
     ]
    }
   ],
   "source": [
    "# Check what config_id_from_row generates vs actual filenames\n",
    "from pathlib import Path\n",
    "\n",
    "topN_sample = topN[0]  # First config\n",
    "generated_id = config_id_from_row(\n",
    "    topN_sample[\"lookback_days\"], \n",
    "    topN_sample[\"atr_days\"], \n",
    "    topN_sample[\"top_percentile\"], \n",
    "    topN_sample[\"min_trade_value\"],\n",
    "    topN_sample[\"max_position_weight\"], \n",
    "    topN_sample[\"min_cash_reserve\"],\n",
    "    topN_sample[\"drift_threshold\"], \n",
    "    topN_sample[\"min_new_position_weight\"]\n",
    ")\n",
    "\n",
    "print(f\"Top config values: {topN_sample}\")\n",
    "print(f\"Generated config_id: {generated_id}\")\n",
    "print(f\"\\nActual files in directory:\")\n",
    "for f in list(Path(DAILY_RETURNS_DIR).glob(\"*.parquet\"))[:5]:\n",
    "    print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b7c84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files: 48\n",
      "Matched: 48/48\n",
      "\n",
      "First 10 unmatched config IDs:\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get all available config IDs from actual files\n",
    "available_files = {f.stem for f in Path(DAILY_RETURNS_DIR).glob(\"*.parquet\")}\n",
    "print(f\"Available files: {len(available_files)}\")\n",
    "\n",
    "# Check how many of topN actually exist\n",
    "matched = 0\n",
    "unmatched = []\n",
    "for row in topN:\n",
    "    cfg_id = config_id_from_row(\n",
    "        row[\"lookback_days\"], row[\"atr_days\"], row[\"top_percentile\"], row[\"min_trade_value\"],\n",
    "        row[\"max_position_weight\"], row[\"min_cash_reserve\"], row[\"drift_threshold\"], row[\"min_new_position_weight\"]\n",
    "    )\n",
    "    if cfg_id in available_files:\n",
    "        matched += 1\n",
    "    else:\n",
    "        unmatched.append((cfg_id, row))\n",
    "\n",
    "print(f\"Matched: {matched}/{len(topN)}\")\n",
    "print(f\"\\nFirst 10 unmatched config IDs:\")\n",
    "for cfg_id, row in unmatched[:10]:\n",
    "    print(f\"  {cfg_id}\")\n",
    "    print(f\"    cash_reserve={row['min_cash_reserve']}, mnw={row['min_new_position_weight']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c65fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 48\n",
      "\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0050\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0100\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0200\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0300\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0500_mnw0050\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0500_mnw0100\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0500_mnw0200\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0500_mnw0300\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr1000_mnw0050\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr1000_mnw0100\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr1000_mnw0200\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr1000_mnw0300\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0200_mnw0050\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0200_mnw0100\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0200_mnw0200\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0200_mnw0300\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0500_mnw0050\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0500_mnw0100\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0500_mnw0200\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0500_mnw0300\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr1000_mnw0050\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr1000_mnw0100\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr1000_mnw0200\n",
      "lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr1000_mnw0300\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash10000_dr0200_mnw0050\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash10000_dr0200_mnw0100\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash10000_dr0200_mnw0200\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash10000_dr0200_mnw0300\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash10000_dr0500_mnw0050\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash10000_dr0500_mnw0100\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash10000_dr0500_mnw0200\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash10000_dr0500_mnw0300\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash10000_dr1000_mnw0050\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash10000_dr1000_mnw0100\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash10000_dr1000_mnw0200\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash10000_dr1000_mnw0300\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash5000_dr0200_mnw0050\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash5000_dr0200_mnw0100\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash5000_dr0200_mnw0200\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash5000_dr0200_mnw0300\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash5000_dr0500_mnw0050\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash5000_dr0500_mnw0100\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash5000_dr0500_mnw0200\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash5000_dr0500_mnw0300\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash5000_dr1000_mnw0050\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash5000_dr1000_mnw0100\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash5000_dr1000_mnw0200\n",
      "lb90_atr20_tp0950_mtv5000_cap1200_cash5000_dr1000_mnw0300\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "files = sorted(Path(DAILY_RETURNS_DIR).glob(\"*.parquet\"))\n",
    "print(f\"Total files: {len(files)}\\n\")\n",
    "for f in files:\n",
    "    print(f.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4456a456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)\n",
      "Total rows: 6790\n",
      "Date range: 1999-01-04 00:00:00 to 2025-12-30 00:00:00\n",
      "\n",
      "Training window: 2005-01-01 to 2006-01-01\n",
      "Rows found: 252\n",
      "Required (TRADING_DAYS): 252\n",
      "Actual date range: 2005-01-03 00:00:00 to 2005-12-30 00:00:00\n",
      "\n",
      "Dates in 2005:\n",
      "Count: 252\n",
      "Range: 2005-01-03 00:00:00 to 2005-12-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pick one config and load its data\n",
    "k = list(returns_data.keys())[0]\n",
    "r = returns_data[k]\n",
    "\n",
    "print(f\"Config: {k}\")\n",
    "print(f\"Total rows: {len(r)}\")\n",
    "print(f\"Date range: {r['date'].min()} to {r['date'].max()}\")\n",
    "\n",
    "# Check a skipped window - 2005-01-01 training for 2006-01-01 test\n",
    "train_start = pd.Timestamp(\"2005-01-01\")\n",
    "test_start = pd.Timestamp(\"2006-01-01\")\n",
    "\n",
    "train = r.loc[(r[\"date\"] >= train_start) & (r[\"date\"] < test_start)]\n",
    "print(f\"\\nTraining window: {train_start.date()} to {test_start.date()}\")\n",
    "print(f\"Rows found: {len(train)}\")\n",
    "print(f\"Required (TRADING_DAYS): {TRADING_DAYS}\")\n",
    "\n",
    "if len(train) > 0:\n",
    "    print(f\"Actual date range: {train['date'].min()} to {train['date'].max()}\")\n",
    "else:\n",
    "    print(\"NO DATA in this range!\")\n",
    "    \n",
    "# Check what dates ARE in the data around that period\n",
    "print(f\"\\nDates in 2005:\")\n",
    "dates_2005 = r[r[\"date\"].dt.year == 2005]\n",
    "print(f\"Count: {len(dates_2005)}\")\n",
    "if len(dates_2005) > 0:\n",
    "    print(f\"Range: {dates_2005['date'].min()} to {dates_2005['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30665b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking all 48 configs for window 2005-01-01 → 2006-01-01\n",
      "================================================================================\n",
      "\n",
      "Passing: 48\n",
      "Failing: 0\n",
      "\n",
      "First 5 passing configs:\n",
      "  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02): 252 days, score=0.1651\n",
      "  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.01): 252 days, score=0.1651\n",
      "  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.005): 252 days, score=0.1651\n",
      "  (90, 20, 0.95, 10000.0, 0.12, 10000.0, 0.02, 0.02): 252 days, score=0.1645\n",
      "  (90, 20, 0.95, 10000.0, 0.12, 10000.0, 0.02, 0.01): 252 days, score=0.1645\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_start = pd.Timestamp(\"2005-01-01\")\n",
    "test_start = pd.Timestamp(\"2006-01-01\")\n",
    "\n",
    "print(f\"Checking all {len(top_loaded)} configs for window {train_start.date()} → {test_start.date()}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "passing = []\n",
    "failing = []\n",
    "\n",
    "for row in top_loaded:\n",
    "    k = config_key(row)\n",
    "    r = returns_data[k]\n",
    "    \n",
    "    train = r.loc[(r[\"date\"] >= train_start) & (r[\"date\"] < test_start)]\n",
    "    \n",
    "    if len(train) < TRADING_DAYS:\n",
    "        failing.append((k, len(train), \"not enough days\"))\n",
    "        continue\n",
    "    \n",
    "    score, stats = score_training_slice(train[\"strat_ret\"], SELECTION_METRIC)\n",
    "    \n",
    "    if np.isnan(score):\n",
    "        failing.append((k, len(train), f\"score is NaN, cagr={stats['cagr']:.4f}\"))\n",
    "        continue\n",
    "    \n",
    "    passing.append((k, len(train), score))\n",
    "\n",
    "print(f\"\\nPassing: {len(passing)}\")\n",
    "print(f\"Failing: {len(failing)}\")\n",
    "\n",
    "if failing:\n",
    "    print(\"\\nFirst 10 failing configs:\")\n",
    "    for k, n_days, reason in failing[:10]:\n",
    "        print(f\"  {k}: {n_days} days, reason: {reason}\")\n",
    "\n",
    "if passing:\n",
    "    print(\"\\nFirst 5 passing configs:\")\n",
    "    for k, n_days, score in passing[:5]:\n",
    "        print(f\"  {k}: {n_days} days, score={score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "251f1fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows: 24\n",
      "\n",
      "1: train=2000-01-01 → test=2001-01-01 → 2002-01-01\n",
      "2: train=2001-01-01 → test=2002-01-01 → 2003-01-01\n",
      "3: train=2002-01-01 → test=2003-01-01 → 2004-01-01\n",
      "4: train=2003-01-01 → test=2004-01-01 → 2005-01-01\n",
      "5: train=2004-01-01 → test=2005-01-01 → 2006-01-01\n",
      "6: train=2005-01-01 → test=2006-01-01 → 2007-01-01\n",
      "7: train=2006-01-01 → test=2007-01-01 → 2008-01-01\n",
      "8: train=2007-01-01 → test=2008-01-01 → 2009-01-01\n",
      "9: train=2008-01-01 → test=2009-01-01 → 2010-01-01\n",
      "10: train=2009-01-01 → test=2010-01-01 → 2011-01-01\n",
      "11: train=2010-01-01 → test=2011-01-01 → 2012-01-01\n",
      "12: train=2011-01-01 → test=2012-01-01 → 2013-01-01\n",
      "13: train=2012-01-01 → test=2013-01-01 → 2014-01-01\n",
      "14: train=2013-01-01 → test=2014-01-01 → 2015-01-01\n",
      "15: train=2014-01-01 → test=2015-01-01 → 2016-01-01\n",
      "16: train=2015-01-01 → test=2016-01-01 → 2017-01-01\n",
      "17: train=2016-01-01 → test=2017-01-01 → 2018-01-01\n",
      "18: train=2017-01-01 → test=2018-01-01 → 2019-01-01\n",
      "19: train=2018-01-01 → test=2019-01-01 → 2020-01-01\n",
      "20: train=2019-01-01 → test=2020-01-01 → 2021-01-01\n",
      "21: train=2020-01-01 → test=2021-01-01 → 2022-01-01\n",
      "22: train=2021-01-01 → test=2022-01-01 → 2023-01-01\n",
      "23: train=2022-01-01 → test=2023-01-01 → 2024-01-01\n",
      "24: train=2023-01-01 → test=2024-01-01 → 2025-01-01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "START_DATE = pd.Timestamp(\"2000-01-01\")\n",
    "END_DATE   = pd.Timestamp(\"2025-12-24\")\n",
    "TRAIN_YEARS = 1\n",
    "TEST_YEARS  = 1\n",
    "\n",
    "test_starts = pd.date_range(START_DATE, END_DATE, freq=\"YS\")\n",
    "windows = []\n",
    "for test_start in test_starts:\n",
    "    train_start = test_start - pd.DateOffset(years=TRAIN_YEARS)\n",
    "    test_end    = test_start + pd.DateOffset(years=TEST_YEARS)\n",
    "    if train_start < START_DATE:\n",
    "        continue\n",
    "    if test_end > END_DATE:\n",
    "        break\n",
    "    windows.append((train_start, test_start, test_end))\n",
    "\n",
    "print(f\"Total windows: {len(windows)}\\n\")\n",
    "for i, (train_start, test_start, test_end) in enumerate(windows):\n",
    "    print(f\"{i+1}: train={train_start.date()} → test={test_start.date()} → {test_end.date()}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
