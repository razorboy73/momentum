{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f387bf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAILY_RETURNS_DIR resolved: C:\\TWS API\\source\\pythonclient\\TradingIdeas\\MomentumSystem\\13a-trading_output_sweep_performance_daily_returns\\daily_returns\\20251231-100856\n",
      "Exists? True\n",
      "Sample files: ['lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0050.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0100.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0200.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0200_mnw0300.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0500_mnw0050.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0500_mnw0100.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0500_mnw0200.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr0500_mnw0300.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr1000_mnw0050.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr1000_mnw0100.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr1000_mnw0200.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash10000_dr1000_mnw0300.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0200_mnw0050.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0200_mnw0100.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0200_mnw0200.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0200_mnw0300.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0500_mnw0050.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0500_mnw0100.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0500_mnw0200.parquet', 'lb90_atr20_tp0950_mtv10000_cap1200_cash5000_dr0500_mnw0300.parquet']\n",
      "Daily returns dir: 13a-trading_output_sweep_performance_daily_returns\\daily_returns\\20251231-100856\n",
      "Found 48 parquet, 0 csv files.\n",
      "\n",
      "Top 136 UNIQUE configs by full-sample CAGR (after de-dupe): 48\n",
      "\n",
      "Loaded daily returns for 48/48 configs.\n",
      "\n",
      "20 walk-forward windows (5y train / 1y trade).\n",
      "\n",
      "=== Walk-forward window results ===\n",
      "   train_start test_start   test_end  \\\n",
      "0   2000-01-01 2005-01-01 2006-01-01   \n",
      "1   2001-01-01 2006-01-01 2007-01-01   \n",
      "2   2002-01-01 2007-01-01 2008-01-01   \n",
      "3   2003-01-01 2008-01-01 2009-01-01   \n",
      "4   2004-01-01 2009-01-01 2010-01-01   \n",
      "5   2005-01-01 2010-01-01 2011-01-01   \n",
      "6   2006-01-01 2011-01-01 2012-01-01   \n",
      "7   2007-01-01 2012-01-01 2013-01-01   \n",
      "8   2008-01-01 2013-01-01 2014-01-01   \n",
      "9   2009-01-01 2014-01-01 2015-01-01   \n",
      "10  2010-01-01 2015-01-01 2016-01-01   \n",
      "11  2011-01-01 2016-01-01 2017-01-01   \n",
      "12  2012-01-01 2017-01-01 2018-01-01   \n",
      "13  2013-01-01 2018-01-01 2019-01-01   \n",
      "14  2014-01-01 2019-01-01 2020-01-01   \n",
      "15  2015-01-01 2020-01-01 2021-01-01   \n",
      "16  2016-01-01 2021-01-01 2022-01-01   \n",
      "17  2017-01-01 2022-01-01 2023-01-01   \n",
      "18  2018-01-01 2023-01-01 2024-01-01   \n",
      "19  2019-01-01 2024-01-01 2025-01-01   \n",
      "\n",
      "                                    chosen_config_key  train_cagr  \\\n",
      "0    (90, 20, 0.95, 5000.0, 0.12, 5000.0, 0.02, 0.01)    0.169428   \n",
      "1   (90, 20, 0.95, 10000.0, 0.12, 10000.0, 0.02, 0...    0.136421   \n",
      "2   (90, 20, 0.95, 10000.0, 0.12, 10000.0, 0.02, 0...    0.173993   \n",
      "3   (90, 20, 0.95, 10000.0, 0.12, 10000.0, 0.02, 0...    0.220327   \n",
      "4   (90, 20, 0.95, 10000.0, 0.12, 10000.0, 0.02, 0...    0.137068   \n",
      "5   (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.196662   \n",
      "6   (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.205574   \n",
      "7    (90, 20, 0.95, 5000.0, 0.12, 5000.0, 0.02, 0.01)    0.179558   \n",
      "8   (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.188539   \n",
      "9   (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.298924   \n",
      "10  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.215727   \n",
      "11  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.201674   \n",
      "12  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.259234   \n",
      "13  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.02)    0.239670   \n",
      "14  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.03)    0.149205   \n",
      "15  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.03)    0.162805   \n",
      "16  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.03)    0.195723   \n",
      "17  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.02, 0.03)    0.185402   \n",
      "18  (90, 20, 0.95, 10000.0, 0.12, 10000.0, 0.05, 0...    0.157814   \n",
      "19  (90, 20, 0.95, 10000.0, 0.12, 5000.0, 0.05, 0.03)    0.152839   \n",
      "\n",
      "    train_sharpe  train_dd  train_calmar  test_cagr  test_sharpe   test_dd  \\\n",
      "0       0.952423 -0.246254      0.688022   0.160244     1.037786 -0.109804   \n",
      "1       1.125944 -0.116802      1.167969   0.172584     1.316787 -0.132054   \n",
      "2       1.277163 -0.132054      1.317584   0.211100     1.268285 -0.131409   \n",
      "3       1.418838 -0.132054      1.668457  -0.052468    -0.926081 -0.086582   \n",
      "4       1.079010 -0.176617      0.776073   0.470599     1.757337 -0.138337   \n",
      "5       1.120048 -0.261461      0.752166   0.209114     1.034329 -0.186381   \n",
      "6       1.110475 -0.261461      0.786249   0.053383     0.383936 -0.152338   \n",
      "7       0.964014 -0.260798      0.688495   0.288719     1.666654 -0.132079   \n",
      "8       1.027324 -0.187879      1.003516   0.465822     2.396314 -0.078811   \n",
      "9       1.427828 -0.186381      1.603833   0.103688     0.872637 -0.093887   \n",
      "10      1.249783 -0.186381      1.157451   0.140934     1.069104 -0.090027   \n",
      "11      1.281812 -0.152338      1.323860   0.330527     1.744305 -0.103360   \n",
      "12      1.597082 -0.134817      1.922859   0.191931     1.444000 -0.048909   \n",
      "13      1.551978 -0.134817      1.777746  -0.014778    -0.011558 -0.145191   \n",
      "14      1.115062 -0.132487      1.126193   0.183561     1.548585 -0.076887   \n",
      "15      1.199119 -0.132487      1.228841   0.301806     1.140015 -0.282053   \n",
      "16      1.141857 -0.282053      0.693924   0.263418     1.181240 -0.105125   \n",
      "17      1.026759 -0.282053      0.657330  -0.005847     0.029989 -0.103816   \n",
      "18      1.020417 -0.197375      0.799564   0.061443     0.563021 -0.129501   \n",
      "19      0.990247 -0.197481      0.773943   0.095304     0.857152 -0.067127   \n",
      "\n",
      "    test_calmar  n_test_days  \n",
      "0      1.459363          252  \n",
      "1      1.306920          251  \n",
      "2      1.606441          251  \n",
      "3     -0.605990          253  \n",
      "4      3.401829          252  \n",
      "5      1.121970          252  \n",
      "6      0.350425          252  \n",
      "7      2.185961          250  \n",
      "8      5.910652          252  \n",
      "9      1.104395          252  \n",
      "10     1.565453          252  \n",
      "11     3.197825          252  \n",
      "12     3.924237          251  \n",
      "13    -0.101781          251  \n",
      "14     2.387398          252  \n",
      "15     1.070035          253  \n",
      "16     2.505764          252  \n",
      "17    -0.056320          251  \n",
      "18     0.474462          250  \n",
      "19     1.419770          252  \n",
      "\n",
      "=== Overall walk-forward performance ===\n",
      "WFO CAGR   : 17.443%\n",
      "WFO Sharpe : 1.07\n",
      "WFO MaxDD  : -28.21%\n",
      "WFO Calmar : 0.62\n",
      "Saved: ./13b-wfo\\walkforward_top136_windows.csv\n",
      "Saved: ./13b-wfo\\walkforward_top136_equity_curve.csv\n",
      "\n",
      "=== COMPLETE ===\n",
      "All outputs saved to: ./13b-wfo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TRADING_DAYS = 252\n",
    "\n",
    "# =========================\n",
    "# INPUTS\n",
    "# =========================\n",
    "\n",
    "\n",
    "SWEEP_RESULTS_CSV = \"./13a-trading_output_sweep_performance_daily_returns/sweep_summary_20251231-100856.csv\"\n",
    "DAILY_RETURNS_DIR = \"./13a-trading_output_sweep_performance_daily_returns/daily_returns/20251231-100856\"\n",
    "\n",
    "TOP_N = 136\n",
    "\n",
    "TRAIN_YEARS = 5\n",
    "TEST_YEARS  = 1\n",
    "\n",
    "START_DATE = pd.Timestamp(\"2000-01-01\")\n",
    "END_DATE   = pd.Timestamp(\"2025-12-24\")\n",
    "\n",
    "SELECTION_METRIC = \"cagr\"  # \"cagr\" | \"sharpe\" | \"calmar\"\n",
    "\n",
    "SAVE_WF_RESULTS_CSV = True\n",
    "SAVE_EQUITY_CURVE_CSV = True\n",
    "\n",
    "# =========================\n",
    "# OUTPUT DIRECTORY\n",
    "# =========================\n",
    "OUTPUT_DIR = \"./13b-wfo\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_PREFIX = \"walkforward\"\n",
    "\n",
    "# =========================\n",
    "# CONFIG ID + KEYS (NOW INCLUDE ALL SWEPT PARAMS)\n",
    "# =========================\n",
    "def make_config_id(row: dict) -> str:\n",
    "    \"\"\"\n",
    "    Must MATCH the sweep daily-returns writer naming.\n",
    "    Includes all swept params so filenames are unique.\n",
    "    \"\"\"\n",
    "    lb  = int(row[\"lookback_days\"])\n",
    "    atr = int(row[\"atr_days\"])\n",
    "    tp  = int(round(float(row[\"top_percentile\"]) * 1000))         # 0.95 -> 950\n",
    "    mtv = int(round(float(row[\"min_trade_value\"])))\n",
    "\n",
    "    mpw = int(round(float(row[\"max_position_weight\"]) * 1000))    # 0.12 -> 120\n",
    "    mcr = int(round(float(row[\"min_cash_reserve\"])))              # 5000.0 -> 5000\n",
    "    dr  = int(round(float(row[\"drift_threshold\"]) * 10000))       # 0.05 -> 500\n",
    "    mnw = int(round(float(row[\"min_new_position_weight\"]) * 10000))  # 0.005 -> 50\n",
    "\n",
    "    return f\"lb{lb}_atr{atr}_tp{tp:04d}_mtv{mtv}_mpw{mpw}_cash{mcr}_dr{dr}_mnw{mnw}\"\n",
    "\n",
    "def config_key(c):\n",
    "    return (\n",
    "        int(c[\"lookback_days\"]),\n",
    "        int(c[\"atr_days\"]),\n",
    "        float(c[\"top_percentile\"]),\n",
    "        float(c[\"min_trade_value\"]),\n",
    "        float(c[\"max_position_weight\"]),\n",
    "        float(c[\"min_cash_reserve\"]),\n",
    "        float(c[\"drift_threshold\"]),\n",
    "        float(c[\"min_new_position_weight\"]),\n",
    "    )\n",
    "\n",
    "def config_id_from_row(lookback, atr, top_percentile, min_trade_value,\n",
    "                       max_position_weight, min_cash_reserve,\n",
    "                       drift_threshold, min_new_position_weight):\n",
    "\n",
    "    tp = int(round(float(top_percentile) * 1000))       # 0.95 -> 950  => tp0950\n",
    "    mtv_i = int(round(float(min_trade_value)))          # 10000 -> mtv10000\n",
    "\n",
    "    cap_i = int(round(float(max_position_weight) * 10000))      # 0.12 -> 1200 => cap1200\n",
    "    cash_i = int(round(float(min_cash_reserve)))                # 5000 -> cash5000\n",
    "    dr_i = int(round(float(drift_threshold) * 10000))           # 0.02 -> 200 => dr0200\n",
    "    mnw_i = int(round(float(min_new_position_weight) * 10000))  # 0.005 -> 50 => mnw0050\n",
    "\n",
    "    return (\n",
    "        f\"lb{int(lookback)}_atr{int(atr)}_tp{tp:04d}_mtv{mtv_i}\"\n",
    "        f\"_cap{cap_i:04d}_cash{cash_i}_dr{dr_i:04d}_mnw{mnw_i:04d}\"\n",
    "    )\n",
    "\n",
    "from pathlib import Path\n",
    "p = Path(DAILY_RETURNS_DIR)\n",
    "print(\"DAILY_RETURNS_DIR resolved:\", p.resolve())\n",
    "print(\"Exists?\", p.exists())\n",
    "print(\"Sample files:\", [x.name for x in list(p.glob(\"*\"))[:20]])\n",
    "\n",
    "def load_config_returns(base_dir: str | Path, cfg) -> pd.DataFrame:\n",
    "    base_dir = Path(base_dir)\n",
    "\n",
    "    cfg_id = config_id_from_row(\n",
    "        cfg[\"lookback_days\"], cfg[\"atr_days\"], cfg[\"top_percentile\"], cfg[\"min_trade_value\"],\n",
    "        cfg[\"max_position_weight\"], cfg[\"min_cash_reserve\"], cfg[\"drift_threshold\"], cfg[\"min_new_position_weight\"]\n",
    "    )\n",
    "\n",
    "    pq = base_dir / f\"{cfg_id}.parquet\"\n",
    "    csv = base_dir / f\"{cfg_id}.csv\"\n",
    "\n",
    "    if pq.exists():\n",
    "        df = pd.read_parquet(pq)\n",
    "    elif csv.exists():\n",
    "        df = pd.read_csv(csv)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Missing daily returns file for {cfg_id} in {base_dir}\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"date\")\n",
    "\n",
    "    if \"strat_ret\" not in df.columns:\n",
    "        raise ValueError(f\"{cfg_id} missing 'strat_ret'\")\n",
    "    if \"spy_ret\" not in df.columns:\n",
    "        df[\"spy_ret\"] = np.nan\n",
    "\n",
    "    return df[[\"date\", \"strat_ret\", \"spy_ret\"]]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# METRICS\n",
    "# =========================\n",
    "def annualized_cagr_from_returns(rets: pd.Series) -> float:\n",
    "    if rets.empty:\n",
    "        return np.nan\n",
    "    total = float((1.0 + rets).prod())\n",
    "    years = len(rets) / TRADING_DAYS\n",
    "    if years <= 0:\n",
    "        return np.nan\n",
    "    return total ** (1.0 / years) - 1.0\n",
    "\n",
    "def annualized_sharpe(rets: pd.Series) -> float:\n",
    "    if rets.empty:\n",
    "        return np.nan\n",
    "    sd = float(rets.std(ddof=0))\n",
    "    if sd <= 0 or np.isnan(sd):\n",
    "        return 0.0\n",
    "    return np.sqrt(TRADING_DAYS) * float(rets.mean()) / sd\n",
    "\n",
    "def max_drawdown_from_returns(rets: pd.Series) -> float:\n",
    "    if rets.empty:\n",
    "        return np.nan\n",
    "    equity = (1.0 + rets).cumprod()\n",
    "    dd = equity / equity.cummax() - 1.0\n",
    "    return float(dd.min())\n",
    "\n",
    "def score_training_slice(train_rets: pd.Series, metric: str):\n",
    "    c = annualized_cagr_from_returns(train_rets)\n",
    "    s = annualized_sharpe(train_rets)\n",
    "    dd = max_drawdown_from_returns(train_rets)\n",
    "    calmar = c / abs(dd) if (dd is not None and not np.isnan(dd) and dd != 0) else np.nan\n",
    "\n",
    "    if metric == \"cagr\":\n",
    "        return c, {\"cagr\": c, \"sharpe\": s, \"dd\": dd, \"calmar\": calmar}\n",
    "    if metric == \"sharpe\":\n",
    "        return s, {\"cagr\": c, \"sharpe\": s, \"dd\": dd, \"calmar\": calmar}\n",
    "    if metric == \"calmar\":\n",
    "        return calmar, {\"cagr\": c, \"sharpe\": s, \"dd\": dd, \"calmar\": calmar}\n",
    "    raise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "# =========================\n",
    "# 0) SANITY CHECK DIRECTORY\n",
    "# =========================\n",
    "daily_dir = Path(DAILY_RETURNS_DIR)\n",
    "if not daily_dir.exists():\n",
    "    raise FileNotFoundError(f\"DAILY_RETURNS_DIR not found: {daily_dir}\")\n",
    "\n",
    "parquets = sorted(daily_dir.glob(\"*.parquet\"))\n",
    "csvs = sorted(daily_dir.glob(\"*.csv\"))\n",
    "print(f\"Daily returns dir: {daily_dir}\")\n",
    "print(f\"Found {len(parquets)} parquet, {len(csvs)} csv files.\")\n",
    "\n",
    "if len(parquets) == 0 and len(csvs) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"No daily return files found in DAILY_RETURNS_DIR. \"\n",
    "        \"Either WRITE_DAILY_RETURNS was off, the RUN_ID folder is wrong, \"\n",
    "        \"or files were written to a different directory.\"\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# 1) LOAD SWEEP SUMMARY + PICK TOP_N UNIQUE CONFIGS\n",
    "# =========================\n",
    "df = pd.read_csv(SWEEP_RESULTS_CSV)\n",
    "\n",
    "required_cols = [\n",
    "    \"lookback_days\",\"atr_days\",\"top_percentile\",\"min_trade_value\",\n",
    "    \"max_position_weight\",\"min_cash_reserve\",\"drift_threshold\",\"min_new_position_weight\",\n",
    "    \"strat_cagr\"\n",
    "]\n",
    "missing_cols = [c for c in required_cols if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Sweep summary missing columns needed for expanded WFO: {missing_cols}\")\n",
    "\n",
    "# Sort best-to-worst and de-dupe by FULL config definition\n",
    "df_sorted = df.sort_values(\"strat_cagr\", ascending=False).copy()\n",
    "df_sorted[\"_cfgkey\"] = df_sorted.apply(lambda r: config_key(r.to_dict()), axis=1)\n",
    "df_unique = df_sorted.drop_duplicates(\"_cfgkey\", keep=\"first\").drop(columns=[\"_cfgkey\"])\n",
    "\n",
    "topN = (\n",
    "    df.sort_values(\"strat_cagr\", ascending=False)\n",
    "      .head(TOP_N)[[\n",
    "          \"lookback_days\",\"atr_days\",\"top_percentile\",\"min_trade_value\",\n",
    "          \"max_position_weight\",\"min_cash_reserve\",\"drift_threshold\",\"min_new_position_weight\",\n",
    "          \"strat_cagr\"\n",
    "      ]]\n",
    "      .to_dict(\"records\")\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"\\nTop {TOP_N} UNIQUE configs by full-sample CAGR (after de-dupe): {len(topN)}\")\n",
    "\n",
    "# =========================\n",
    "# 2) LOAD DAILY RETURNS FOR THOSE CONFIGS\n",
    "# =========================\n",
    "returns_data = {}\n",
    "missing = []\n",
    "\n",
    "for row in topN:\n",
    "    k = config_key(row)\n",
    "    try:\n",
    "        returns_data[k] = load_config_returns(DAILY_RETURNS_DIR, row)\n",
    "    except Exception as e:\n",
    "        missing.append((make_config_id(row), str(e)))\n",
    "\n",
    "if missing:\n",
    "    print(\"\\nWARNING: Some top configs are missing daily return files:\")\n",
    "    for cfg_id, msg in missing[:25]:\n",
    "        print(\" \", cfg_id, \"->\", msg)\n",
    "    if len(missing) > 25:\n",
    "        print(f\" ... and {len(missing)-25} more\")\n",
    "\n",
    "top_loaded = [row for row in topN if config_key(row) in returns_data]\n",
    "if not top_loaded:\n",
    "    raise RuntimeError(\n",
    "        \"Could not load daily returns for ANY of the top configs.\\n\"\n",
    "        \"Most likely: your sweep writer didn't create daily return files, \"\n",
    "        \"or the filename scheme doesn't match make_config_id().\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nLoaded daily returns for {len(top_loaded)}/{len(topN)} configs.\")\n",
    "\n",
    "# =========================\n",
    "# 3) BUILD WALK-FORWARD WINDOWS (YEAR-START ANCHORS)\n",
    "# =========================\n",
    "test_starts = pd.date_range(START_DATE, END_DATE, freq=\"YS\")\n",
    "windows = []\n",
    "for test_start in test_starts:\n",
    "    train_start = test_start - pd.DateOffset(years=TRAIN_YEARS)\n",
    "    test_end    = test_start + pd.DateOffset(years=TEST_YEARS)\n",
    "    if train_start < START_DATE:\n",
    "        continue\n",
    "    if test_end > END_DATE:\n",
    "        break\n",
    "    windows.append((train_start, test_start, test_end))\n",
    "\n",
    "print(f\"\\n{len(windows)} walk-forward windows ({TRAIN_YEARS}y train / {TEST_YEARS}y trade).\")\n",
    "if not windows:\n",
    "    raise RuntimeError(\"No valid windows. Check START_DATE / END_DATE / TRAIN_YEARS.\")\n",
    "\n",
    "# =========================\n",
    "# 4) WALK-FORWARD LOOP\n",
    "# =========================\n",
    "wf_rows = []\n",
    "equity_parts = []\n",
    "equity_level = 1.0\n",
    "\n",
    "for (train_start, test_start, test_end) in windows:\n",
    "    scores = []\n",
    "    for row in top_loaded:\n",
    "        k = config_key(row)\n",
    "        r = returns_data[k]\n",
    "\n",
    "        train = r.loc[(r[\"date\"] >= train_start) & (r[\"date\"] < test_start)]\n",
    "        if len(train) < TRADING_DAYS:\n",
    "            continue\n",
    "\n",
    "        score, stats = score_training_slice(train[\"strat_ret\"], SELECTION_METRIC)\n",
    "        if np.isnan(score):\n",
    "            continue\n",
    "\n",
    "        scores.append((k, score, stats))\n",
    "\n",
    "    if not scores:\n",
    "        print(f\"Skipping window {train_start.date()} → {test_end.date()} (no configs had enough training data).\")\n",
    "        continue\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    best_key, best_score, best_train_stats = scores[0]\n",
    "\n",
    "    test = returns_data[best_key].loc[\n",
    "        (returns_data[best_key][\"date\"] >= test_start) &\n",
    "        (returns_data[best_key][\"date\"] < test_end)\n",
    "    ].copy()\n",
    "\n",
    "    if test.empty:\n",
    "        print(f\"Skipping OOS period {test_start.date()} → {test_end.date()} (no test data).\")\n",
    "        continue\n",
    "\n",
    "    test = test.sort_values(\"date\")\n",
    "    test[\"equity\"] = equity_level * (1.0 + test[\"strat_ret\"]).cumprod()\n",
    "    equity_level = float(test[\"equity\"].iloc[-1])\n",
    "\n",
    "    test_cagr   = annualized_cagr_from_returns(test[\"strat_ret\"])\n",
    "    test_sharpe = annualized_sharpe(test[\"strat_ret\"])\n",
    "    test_dd     = max_drawdown_from_returns(test[\"strat_ret\"])\n",
    "    test_calmar = test_cagr / abs(test_dd) if (test_dd is not None and not np.isnan(test_dd) and test_dd != 0) else np.nan\n",
    "\n",
    "    wf_rows.append({\n",
    "        \"train_start\": train_start,\n",
    "        \"test_start\": test_start,\n",
    "        \"test_end\": test_end,\n",
    "        \"chosen_config_key\": str(best_key),\n",
    "        f\"train_{SELECTION_METRIC}\": best_score,\n",
    "        \"train_cagr\": best_train_stats[\"cagr\"],\n",
    "        \"train_sharpe\": best_train_stats[\"sharpe\"],\n",
    "        \"train_dd\": best_train_stats[\"dd\"],\n",
    "        \"train_calmar\": best_train_stats[\"calmar\"],\n",
    "        \"test_cagr\": test_cagr,\n",
    "        \"test_sharpe\": test_sharpe,\n",
    "        \"test_dd\": test_dd,\n",
    "        \"test_calmar\": test_calmar,\n",
    "        \"n_test_days\": int(len(test)),\n",
    "    })\n",
    "\n",
    "    equity_parts.append(test[[\"date\", \"equity\"]])\n",
    "\n",
    "wf_df = pd.DataFrame(wf_rows).sort_values(\"test_start\").reset_index(drop=True)\n",
    "print(\"\\n=== Walk-forward window results ===\")\n",
    "print(wf_df)\n",
    "\n",
    "if not equity_parts:\n",
    "    raise RuntimeError(\"No equity curve produced (all windows skipped).\")\n",
    "\n",
    "equity_curve = pd.concat(equity_parts, ignore_index=True).drop_duplicates(\"date\").sort_values(\"date\")\n",
    "equity_curve = equity_curve.set_index(\"date\")\n",
    "\n",
    "overall_rets = equity_curve[\"equity\"].pct_change().fillna(0.0)\n",
    "overall_cagr = annualized_cagr_from_returns(overall_rets)\n",
    "overall_sharpe = annualized_sharpe(overall_rets)\n",
    "overall_dd = max_drawdown_from_returns(overall_rets)\n",
    "overall_calmar = overall_cagr / abs(overall_dd) if overall_dd != 0 else np.nan\n",
    "\n",
    "print(\"\\n=== Overall walk-forward performance ===\")\n",
    "print(f\"WFO CAGR   : {overall_cagr:.3%}\")\n",
    "print(f\"WFO Sharpe : {overall_sharpe:.2f}\")\n",
    "print(f\"WFO MaxDD  : {overall_dd:.2%}\")\n",
    "print(f\"WFO Calmar : {overall_calmar:.2f}\")\n",
    "\n",
    "# =========================\n",
    "# 5) SAVE OUTPUTS TO 13b-wfo\n",
    "# =========================\n",
    "if SAVE_WF_RESULTS_CSV:\n",
    "    out1 = os.path.join(OUTPUT_DIR, f\"{OUT_PREFIX}_top{TOP_N}_windows.csv\")\n",
    "    wf_df.to_csv(out1, index=False)\n",
    "    print(f\"Saved: {out1}\")\n",
    "\n",
    "if SAVE_EQUITY_CURVE_CSV:\n",
    "    out2 = os.path.join(OUTPUT_DIR, f\"{OUT_PREFIX}_top{TOP_N}_equity_curve.csv\")\n",
    "    equity_curve.reset_index().to_csv(out2, index=False)\n",
    "    print(f\"Saved: {out2}\")\n",
    "\n",
    "print(f\"\\n=== COMPLETE ===\")\n",
    "print(f\"All outputs saved to: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
