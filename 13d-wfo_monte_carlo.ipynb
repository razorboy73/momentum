{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd900a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading WFO equity curve ===\n",
      "Loaded 5,033 equity points, 5032 return observations.\n",
      "Date range: 2005-01-03 → 2024-12-31\n",
      "Scaled start equity: $360,000\n",
      "\n",
      "=== TRUE WFO RESULTS (single realized path) ===\n",
      "CAGR:   0.1729\n",
      "Sharpe: 0.9820\n",
      "MaxDD:  -0.2991\n",
      "IR (Sharpe of excess vs SPY): 0.3580\n",
      "\n",
      "=== Running Monte Carlo simulations (block bootstrap) ===\n",
      "\n",
      "=== SHARPE UNCERTAINTY (bootstrap CI) ===\n",
      "Sharpe 95% CI (2.5/50/97.5): 0.570, 1.012, 1.458\n",
      "\n",
      "=== IR UNCERTAINTY (bootstrap CI) ===\n",
      "IR 95% CI (2.5/50/97.5): -0.021, 0.369, 0.754\n",
      "\n",
      "=== SHARPE SIGNIFICANCE (null: mean=0) ===\n",
      "Observed Sharpe: 0.9820\n",
      "p-value one-sided (Sharpe > 0): 0.0000\n",
      "p-value two-sided (Sharpe != 0): 0.0000\n",
      "\n",
      "=== IR SIGNIFICANCE (excess vs SPY, null: mean=0) ===\n",
      "Observed IR: 0.3580\n",
      "p-value one-sided (IR > 0): 0.0369\n",
      "p-value two-sided (IR != 0): 0.0727\n",
      "\n",
      "=== MONTE CARLO STATISTICS (WFO) ===\n",
      "CAGR_mean: 0.1790\n",
      "CAGR_median: 0.1784\n",
      "CAGR_5pct: 0.1065\n",
      "CAGR_95pct: 0.2532\n",
      "Sharpe_mean: 1.0127\n",
      "Sharpe_5pct: 0.6471\n",
      "Sharpe_95pct: 1.3823\n",
      "DD_mean: -0.3274\n",
      "DD_5pct: -0.4675\n",
      "DD_95pct: -0.2167\n",
      "Prob_MaxDD_ge_50pct: 0.0274\n",
      "Prob_CAGR_lt_0: 0.0000\n",
      "Prob_Sharpe_lt_0: 0.0000\n",
      "Prob_Sharpe_lt_1: 0.4777\n",
      "\n",
      "Percentile of True CAGR: 0.4491\n",
      "Percentile of True Sharpe: 0.4485\n",
      "\n",
      "=== DOLLAR DRAWDOWN ANALYSIS (scaled to START_CAPITAL) ===\n",
      "Start capital: $360,000\n",
      "Median max $ drawdown: $1,841,157\n",
      "95th pct max $ drawdown: $6,293,870\n",
      "Worst case max $ drawdown: $23,294,877\n",
      "\n",
      "Median max DD as % of peak: 22.8%\n",
      "95th pct max DD as % of peak: 42.9%\n",
      "\n",
      "Prob(≥40% DD after +60% growth): 0.077\n",
      "\n",
      "Prob(min equity <= 50% of start): 0.0010\n",
      "Prob(min equity <= 25% of start): 0.0000\n",
      "\n",
      "Saved Monte Carlo results → ./13d-monte_carlo_output_wfo\\16-mc_wfo_results_20260105-144249.csv\n",
      "\n",
      "All plots saved.\n",
      "\n",
      "=== COMPLETE ===\n",
      "\n",
      "Prob(final equity < $3,000,000 after 25 years): 0.0672\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TRADING_DAYS = 252\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "WFO_EQUITY_FILE = \"./walkforward_top10_by_cagr_equity_curve.csv\"  # WFO output\n",
    "SPY_FILE = \"./8-SPY_200DMA_market_regime/8-SPY_200DMA_regime.parquet\"  # optional benchmark\n",
    "\n",
    "OUT_DIR = \"./13d-monte_carlo_output_wfo\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "START_CAPITAL = 342000  # <-- scale results to dollars\n",
    "N_SIMS = 10000\n",
    "BLOCK_SIZE = 25\n",
    "SEED = 123\n",
    "\n",
    "DD_THRESHOLD = 0.50\n",
    "LATE_GROWTH_MULT = 1.6\n",
    "LATE_DD_FRAC = 0.40\n",
    "\n",
    "# ============================================================\n",
    "# METRICS\n",
    "# ============================================================\n",
    "def cagr_from_path(values, start_date, end_date):\n",
    "    values = np.asarray(values, dtype=float)\n",
    "    if len(values) < 2 or values[0] <= 0:\n",
    "        return np.nan\n",
    "    years = (pd.Timestamp(end_date) - pd.Timestamp(start_date)).days / 365.25\n",
    "    if years <= 0:\n",
    "        return np.nan\n",
    "    return (values[-1] / values[0]) ** (1.0 / years) - 1.0\n",
    "\n",
    "def ann_sharpe(returns):\n",
    "    r = np.asarray(returns, dtype=float)\n",
    "    std = np.std(r, ddof=1)\n",
    "    if std <= 0 or np.isnan(std):\n",
    "        return np.nan\n",
    "    return np.sqrt(TRADING_DAYS) * np.mean(r) / std\n",
    "\n",
    "def max_drawdown(values):\n",
    "    arr = np.asarray(values, dtype=float)\n",
    "    peaks = np.maximum.accumulate(arr)\n",
    "    dd = arr / peaks - 1.0\n",
    "    return float(np.min(dd))\n",
    "\n",
    "def block_bootstrap(returns, n, block_size, rng):\n",
    "    r = np.asarray(returns, dtype=float)\n",
    "    L = len(r)\n",
    "    if L <= 0 or n <= 0:\n",
    "        return np.array([], dtype=float)\n",
    "    if block_size <= 0:\n",
    "        raise ValueError(\"BLOCK_SIZE must be > 0\")\n",
    "    if block_size > L:\n",
    "        raise ValueError(f\"BLOCK_SIZE ({block_size}) cannot exceed number of returns ({L})\")\n",
    "\n",
    "    out = []\n",
    "    while len(out) < n:\n",
    "        i = int(rng.integers(0, L - block_size + 1))\n",
    "        out.extend(r[i:i + block_size])\n",
    "    return np.array(out[:n], dtype=float)\n",
    "\n",
    "def two_sided_p_from_null(null_dist, observed):\n",
    "    # P(|X| >= |obs|)\n",
    "    null_dist = np.asarray(null_dist, float)\n",
    "    return float(np.mean(np.abs(null_dist) >= abs(observed)))\n",
    "\n",
    "# ============================================================\n",
    "# LOAD WFO EQUITY\n",
    "# ============================================================\n",
    "def load_wfo_equity(path: str | Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing WFO equity file: {path}\")\n",
    "\n",
    "    if path.suffix.lower() == \".parquet\":\n",
    "        eq = pd.read_parquet(path)\n",
    "    else:\n",
    "        eq = pd.read_csv(path)\n",
    "\n",
    "    if \"date\" not in eq.columns:\n",
    "        raise ValueError(\"WFO equity file must have a 'date' column\")\n",
    "\n",
    "    if \"equity\" in eq.columns:\n",
    "        val_col = \"equity\"\n",
    "    elif \"portfolio_value\" in eq.columns:\n",
    "        val_col = \"portfolio_value\"\n",
    "    else:\n",
    "        raise ValueError(\"WFO equity file must have 'equity' or 'portfolio_value'\")\n",
    "\n",
    "    eq = eq.copy()\n",
    "    eq[\"date\"] = pd.to_datetime(eq[\"date\"])\n",
    "    eq[val_col] = pd.to_numeric(eq[val_col], errors=\"coerce\")\n",
    "    eq = eq.dropna(subset=[\"date\", val_col]).sort_values(\"date\")\n",
    "    eq = eq.drop_duplicates(subset=[\"date\"]).reset_index(drop=True)\n",
    "    eq = eq[eq[val_col] > 0].reset_index(drop=True)\n",
    "    eq = eq.rename(columns={val_col: \"equity\"})\n",
    "    return eq[[\"date\", \"equity\"]]\n",
    "\n",
    "print(\"=== Loading WFO equity curve ===\")\n",
    "equity = load_wfo_equity(WFO_EQUITY_FILE)\n",
    "if len(equity) < 3:\n",
    "    raise ValueError(\"Not enough equity observations after cleaning (need at least 3 rows).\")\n",
    "\n",
    "# Scale normalized equity to dollars\n",
    "equity0 = float(equity[\"equity\"].iloc[0])\n",
    "scale = START_CAPITAL / equity0\n",
    "equity[\"equity_usd\"] = equity[\"equity\"] * scale\n",
    "\n",
    "equity = equity.sort_values(\"date\").reset_index(drop=True)\n",
    "equity[\"strat_ret\"] = equity[\"equity\"].pct_change().fillna(0.0)\n",
    "\n",
    "values_usd = equity[\"equity_usd\"].to_numpy()\n",
    "values_idx = equity[\"equity\"].to_numpy()\n",
    "dates = equity[\"date\"].to_numpy()\n",
    "rets = equity[\"strat_ret\"].to_numpy()[1:]  # drop first\n",
    "\n",
    "n_rets = len(rets)\n",
    "start_date = equity[\"date\"].iloc[0]\n",
    "end_date = equity[\"date\"].iloc[-1]\n",
    "\n",
    "print(f\"Loaded {len(equity):,} equity points, {n_rets} return observations.\")\n",
    "print(f\"Date range: {start_date.date()} → {end_date.date()}\")\n",
    "print(f\"Scaled start equity: ${values_usd[0]:,.0f}\")\n",
    "\n",
    "# ============================================================\n",
    "# OPTIONAL SPY ALIGNMENT (for IR)\n",
    "# ============================================================\n",
    "have_spy = False\n",
    "ex_rets = None\n",
    "\n",
    "try:\n",
    "    spy = pd.read_parquet(SPY_FILE).copy()\n",
    "    if \"date\" not in spy.columns:\n",
    "        spy = spy.reset_index().rename(columns={\"index\": \"date\", \"Date\": \"date\"})\n",
    "    spy[\"date\"] = pd.to_datetime(spy[\"date\"])\n",
    "    if \"spy_close\" not in spy.columns:\n",
    "        raise ValueError(\"SPY file missing 'spy_close'\")\n",
    "    spy = spy.sort_values(\"date\")\n",
    "    spy[\"spy_ret\"] = spy[\"spy_close\"].pct_change().fillna(0.0)\n",
    "\n",
    "    aligned = equity.merge(spy[[\"date\", \"spy_ret\"]], on=\"date\", how=\"inner\").dropna()\n",
    "    if len(aligned) >= 3:\n",
    "        have_spy = True\n",
    "        ex_rets = (aligned[\"strat_ret\"] - aligned[\"spy_ret\"]).to_numpy()[1:]\n",
    "    else:\n",
    "        have_spy = False\n",
    "except Exception as e:\n",
    "    print(f\"NOTE: SPY not loaded/aligned. IR stats disabled. Reason: {e}\")\n",
    "    have_spy = False\n",
    "\n",
    "# ============================================================\n",
    "# TRUE METRICS (realized WFO path)\n",
    "# ============================================================\n",
    "true_cagr = cagr_from_path(values_usd, start_date, end_date)\n",
    "true_sharpe = ann_sharpe(rets)\n",
    "true_dd = max_drawdown(values_idx)  # same as values_usd, % is invariant\n",
    "\n",
    "print(\"\\n=== TRUE WFO RESULTS (single realized path) ===\")\n",
    "print(f\"CAGR:   {true_cagr:.4f}\")\n",
    "print(f\"Sharpe: {true_sharpe:.4f}\")\n",
    "print(f\"MaxDD:  {true_dd:.4f}\")\n",
    "if have_spy:\n",
    "    true_ir = ann_sharpe(ex_rets)\n",
    "    print(f\"IR (Sharpe of excess vs SPY): {true_ir:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# MONTE CARLO (block bootstrap)\n",
    "# ============================================================\n",
    "print(\"\\n=== Running Monte Carlo simulations (block bootstrap) ===\")\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "sim_cagrs = np.zeros(N_SIMS, float)\n",
    "sim_sharpes = np.zeros(N_SIMS, float)\n",
    "sim_dds = np.zeros(N_SIMS, float)\n",
    "\n",
    "sim_max_dd_dollars = np.zeros(N_SIMS, float)\n",
    "sim_peak_equity = np.zeros(N_SIMS, float)\n",
    "sim_trough_equity = np.zeros(N_SIMS, float)\n",
    "\n",
    "sim_final_equity = np.zeros(N_SIMS, float)\n",
    "sim_min_equity = np.zeros(N_SIMS, float)\n",
    "\n",
    "sim_dd50 = np.zeros(N_SIMS, float)\n",
    "\n",
    "if have_spy:\n",
    "    sim_irs = np.zeros(N_SIMS, float)\n",
    "\n",
    "for i in range(N_SIMS):\n",
    "    sim_rets = block_bootstrap(rets, n_rets, BLOCK_SIZE, rng)\n",
    "\n",
    "    sim_curve_idx = np.cumprod(np.r_[1.0, 1.0 + sim_rets])         # normalized\n",
    "    sim_curve_usd = sim_curve_idx * START_CAPITAL\n",
    "\n",
    "    sim_final_equity[i] = sim_curve_usd[-1]\n",
    "    sim_min_equity[i] = np.min(sim_curve_usd)\n",
    "\n",
    "    running_peak = np.maximum.accumulate(sim_curve_usd)\n",
    "    dd_abs = running_peak - sim_curve_usd\n",
    "    dd_idx = int(np.argmax(dd_abs))\n",
    "    peak_idx = int(np.argmax(sim_curve_usd[:dd_idx + 1])) if dd_idx >= 0 else 0\n",
    "\n",
    "    sim_peak_equity[i] = sim_curve_usd[peak_idx]\n",
    "    sim_trough_equity[i] = sim_curve_usd[dd_idx]\n",
    "    sim_max_dd_dollars[i] = dd_abs[dd_idx]\n",
    "\n",
    "    sim_cagrs[i] = cagr_from_path(sim_curve_usd, start_date, end_date)\n",
    "    sim_sharpes[i] = ann_sharpe(sim_rets)\n",
    "    sim_dds[i] = max_drawdown(sim_curve_idx)\n",
    "    sim_dd50[i] = float(sim_dds[i] <= -DD_THRESHOLD)\n",
    "\n",
    "    if have_spy:\n",
    "        sim_ex = block_bootstrap(ex_rets, len(ex_rets), BLOCK_SIZE, rng)\n",
    "        sim_irs[i] = ann_sharpe(sim_ex)\n",
    "\n",
    "# ============================================================\n",
    "# CONFIDENCE INTERVALS\n",
    "# ============================================================\n",
    "sharpe_ci_95 = np.percentile(sim_sharpes[~np.isnan(sim_sharpes)], [2.5, 50, 97.5])\n",
    "print(\"\\n=== SHARPE UNCERTAINTY (bootstrap CI) ===\")\n",
    "print(f\"Sharpe 95% CI (2.5/50/97.5): {sharpe_ci_95[0]:.3f}, {sharpe_ci_95[1]:.3f}, {sharpe_ci_95[2]:.3f}\")\n",
    "\n",
    "if have_spy:\n",
    "    ir_ci_95 = np.percentile(sim_irs[~np.isnan(sim_irs)], [2.5, 50, 97.5])\n",
    "    print(\"\\n=== IR UNCERTAINTY (bootstrap CI) ===\")\n",
    "    print(f\"IR 95% CI (2.5/50/97.5): {ir_ci_95[0]:.3f}, {ir_ci_95[1]:.3f}, {ir_ci_95[2]:.3f}\")\n",
    "\n",
    "# ============================================================\n",
    "# SIGNIFICANCE TESTS USING A NULL (mean=0) + block bootstrap\n",
    "# ============================================================\n",
    "rets_centered = rets - np.mean(rets)\n",
    "\n",
    "sim_sharpes_null0 = np.zeros(N_SIMS, float)\n",
    "for i in range(N_SIMS):\n",
    "    sim0 = block_bootstrap(rets_centered, n_rets, BLOCK_SIZE, rng)\n",
    "    sim_sharpes_null0[i] = ann_sharpe(sim0)\n",
    "\n",
    "p_sharpe_one = float(np.mean(sim_sharpes_null0 >= true_sharpe))\n",
    "p_sharpe_two = two_sided_p_from_null(sim_sharpes_null0, true_sharpe)\n",
    "\n",
    "print(\"\\n=== SHARPE SIGNIFICANCE (null: mean=0) ===\")\n",
    "print(f\"Observed Sharpe: {true_sharpe:.4f}\")\n",
    "print(f\"p-value one-sided (Sharpe > 0): {p_sharpe_one:.4f}\")\n",
    "print(f\"p-value two-sided (Sharpe != 0): {p_sharpe_two:.4f}\")\n",
    "\n",
    "if have_spy:\n",
    "    ex_centered = ex_rets - np.mean(ex_rets)\n",
    "    sim_ir_null0 = np.zeros(N_SIMS, float)\n",
    "    for i in range(N_SIMS):\n",
    "        sim0 = block_bootstrap(ex_centered, len(ex_centered), BLOCK_SIZE, rng)\n",
    "        sim_ir_null0[i] = ann_sharpe(sim0)\n",
    "\n",
    "    p_ir_one = float(np.mean(sim_ir_null0 >= true_ir))\n",
    "    p_ir_two = two_sided_p_from_null(sim_ir_null0, true_ir)\n",
    "\n",
    "    print(\"\\n=== IR SIGNIFICANCE (excess vs SPY, null: mean=0) ===\")\n",
    "    print(f\"Observed IR: {true_ir:.4f}\")\n",
    "    print(f\"p-value one-sided (IR > 0): {p_ir_one:.4f}\")\n",
    "    print(f\"p-value two-sided (IR != 0): {p_ir_two:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY STATS\n",
    "# ============================================================\n",
    "stats = {\n",
    "    \"CAGR_mean\": np.nanmean(sim_cagrs),\n",
    "    \"CAGR_median\": np.nanmedian(sim_cagrs),\n",
    "    \"CAGR_5pct\": np.nanpercentile(sim_cagrs, 5),\n",
    "    \"CAGR_95pct\": np.nanpercentile(sim_cagrs, 95),\n",
    "\n",
    "    \"Sharpe_mean\": np.nanmean(sim_sharpes),\n",
    "    \"Sharpe_5pct\": np.nanpercentile(sim_sharpes, 5),\n",
    "    \"Sharpe_95pct\": np.nanpercentile(sim_sharpes, 95),\n",
    "\n",
    "    \"DD_mean\": np.nanmean(sim_dds),\n",
    "    \"DD_5pct\": np.nanpercentile(sim_dds, 5),\n",
    "    \"DD_95pct\": np.nanpercentile(sim_dds, 95),\n",
    "\n",
    "    f\"Prob_MaxDD_ge_{int(DD_THRESHOLD*100)}pct\": float(np.mean(sim_dd50)),\n",
    "    \"Prob_CAGR_lt_0\": float(np.mean(sim_cagrs < 0)),\n",
    "    \"Prob_Sharpe_lt_0\": float(np.mean(sim_sharpes < 0)),\n",
    "    \"Prob_Sharpe_lt_1\": float(np.mean(sim_sharpes < 1)),\n",
    "}\n",
    "\n",
    "print(\"\\n=== MONTE CARLO STATISTICS (WFO) ===\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPercentile of True CAGR:\", float(np.mean(sim_cagrs <= true_cagr)))\n",
    "print(\"Percentile of True Sharpe:\", float(np.mean(sim_sharpes <= true_sharpe)))\n",
    "\n",
    "# ============================================================\n",
    "# DOLLAR DRAWDOWN ANALYSIS (now real $)\n",
    "# ============================================================\n",
    "print(\"\\n=== DOLLAR DRAWDOWN ANALYSIS (scaled to START_CAPITAL) ===\")\n",
    "print(f\"Start capital: ${START_CAPITAL:,.0f}\")\n",
    "print(f\"Median max $ drawdown: ${np.median(sim_max_dd_dollars):,.0f}\")\n",
    "print(f\"95th pct max $ drawdown: ${np.percentile(sim_max_dd_dollars, 95):,.0f}\")\n",
    "print(f\"Worst case max $ drawdown: ${sim_max_dd_dollars.max():,.0f}\")\n",
    "\n",
    "# Drawdown in $ as % of peak (more meaningful than % of start when equity grows)\n",
    "dd_pct_of_peak = sim_max_dd_dollars / np.maximum(sim_peak_equity, 1e-12)\n",
    "print(f\"\\nMedian max DD as % of peak: {np.median(dd_pct_of_peak)*100:.1f}%\")\n",
    "print(f\"95th pct max DD as % of peak: {np.percentile(dd_pct_of_peak,95)*100:.1f}%\")\n",
    "\n",
    "late_bad = np.mean((sim_peak_equity >= LATE_GROWTH_MULT * START_CAPITAL) &\n",
    "                   (sim_max_dd_dollars >= LATE_DD_FRAC * sim_peak_equity))\n",
    "print(f\"\\nProb(≥{int(LATE_DD_FRAC*100)}% DD after +{int((LATE_GROWTH_MULT-1)*100)}% growth): {late_bad:.3f}\")\n",
    "\n",
    "# “Ruin” style metrics (pick your definition)\n",
    "ruin_50 = float(np.mean(sim_min_equity <= 0.50 * START_CAPITAL))\n",
    "ruin_25 = float(np.mean(sim_min_equity <= 0.25 * START_CAPITAL))\n",
    "print(f\"\\nProb(min equity <= 50% of start): {ruin_50:.4f}\")\n",
    "print(f\"Prob(min equity <= 25% of start): {ruin_25:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# SAVE CSV + PLOTS\n",
    "# ============================================================\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"sim_cagr\": sim_cagrs,\n",
    "    \"sim_sharpe\": sim_sharpes,\n",
    "    \"sim_maxdd\": sim_dds,\n",
    "    \"sim_maxdd_dollars\": sim_max_dd_dollars,\n",
    "    \"sim_peak_equity\": sim_peak_equity,\n",
    "    \"sim_trough_equity\": sim_trough_equity,\n",
    "    \"sim_final_equity\": sim_final_equity,\n",
    "    \"sim_min_equity\": sim_min_equity,\n",
    "    \"sim_sharpe_null_mean0\": sim_sharpes_null0,\n",
    "})\n",
    "\n",
    "if have_spy:\n",
    "    results[\"sim_ir_excess_vs_spy\"] = sim_irs\n",
    "\n",
    "out_csv = os.path.join(OUT_DIR, f\"16-mc_wfo_results_{timestamp}.csv\")\n",
    "results.to_csv(out_csv, index=False)\n",
    "print(f\"\\nSaved Monte Carlo results → {out_csv}\")\n",
    "\n",
    "def save_hist(data, title, filename, true_value=None):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    data = np.asarray(data, float)\n",
    "    plt.hist(data[~np.isnan(data)], bins=60, alpha=0.7)\n",
    "    if true_value is not None:\n",
    "        plt.axvline(true_value, linewidth=2, label=\"Actual\")\n",
    "        plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, filename))\n",
    "    plt.close()\n",
    "\n",
    "save_hist(sim_cagrs, \"WFO CAGR Distribution (block bootstrap)\", f\"mc_wfo_cagr_{timestamp}.png\", true_cagr)\n",
    "save_hist(sim_sharpes, \"WFO Sharpe Distribution (block bootstrap)\", f\"mc_wfo_sharpe_{timestamp}.png\", true_sharpe)\n",
    "save_hist(sim_dds, \"WFO Max Drawdown Distribution (block bootstrap)\", f\"mc_wfo_dd_{timestamp}.png\", true_dd)\n",
    "save_hist(sim_sharpes_null0, \"WFO Sharpe Null (mean=0) Distribution\", f\"mc_wfo_sharpe_null_mean0_{timestamp}.png\", true_sharpe)\n",
    "\n",
    "if have_spy:\n",
    "    save_hist(sim_irs, \"WFO IR (excess vs SPY) Distribution (block bootstrap)\", f\"mc_wfo_ir_{timestamp}.png\", true_ir)\n",
    "\n",
    "print(\"\\nAll plots saved.\")\n",
    "print(\"\\n=== COMPLETE ===\")\n",
    "# === Custom probability question: equity < $2.5M after 12 years ===\n",
    "TARGET_YEARS = 25\n",
    "TARGET_VALUE = 3000_000\n",
    "\n",
    "# Convert years to fraction of sample length\n",
    "years_total = (pd.Timestamp(end_date) - pd.Timestamp(start_date)).days / 365.25\n",
    "frac = TARGET_YEARS / years_total\n",
    "steps = int(frac * n_rets)\n",
    "\n",
    "below_count = 0\n",
    "for i in range(N_SIMS):\n",
    "    sim_rets = block_bootstrap(rets, n_rets, BLOCK_SIZE, rng)\n",
    "    sim_curve = np.cumprod(np.r_[1.0, 1.0 + sim_rets]) * START_CAPITAL\n",
    "    val_at_12y = sim_curve[min(steps, len(sim_curve) - 1)]\n",
    "    if val_at_12y < TARGET_VALUE:\n",
    "        below_count += 1\n",
    "\n",
    "p_below = below_count / N_SIMS\n",
    "print(f\"\\nProb(final equity < ${TARGET_VALUE:,.0f} after {TARGET_YEARS} years): {p_below:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
