{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef3048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Building Enriched Universes (Add Jump90 Signals — No Filtering) per Lookback ===\n",
      "\n",
      "Loading Jump90 files...\n",
      "Loaded Jump90: 1167 tickers\n",
      "Unreadable/invalid Jump90 files: 0 tickers\n",
      "\n",
      "Found 5 MA-enriched universes to process.\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_60D\n",
      "Input MA file: ./10a-multiple_100MA_enriched_universe\\lookback_60D\\enriched_universe_ma100.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved → ./11a-multiple_universe_with_jump90\\lookback_60D\\universe_with_jump90.parquet\n",
      "✔ Validation log → ./system_verification/11a-multiple_universe_with_jump90\\lookback_60D\\validation_20251231-091148.csv\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_90D\n",
      "Input MA file: ./10a-multiple_100MA_enriched_universe\\lookback_90D\\enriched_universe_ma100.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved → ./11a-multiple_universe_with_jump90\\lookback_90D\\universe_with_jump90.parquet\n",
      "✔ Validation log → ./system_verification/11a-multiple_universe_with_jump90\\lookback_90D\\validation_20251231-091148.csv\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_120D\n",
      "Input MA file: ./10a-multiple_100MA_enriched_universe\\lookback_120D\\enriched_universe_ma100.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved → ./11a-multiple_universe_with_jump90\\lookback_120D\\universe_with_jump90.parquet\n",
      "✔ Validation log → ./system_verification/11a-multiple_universe_with_jump90\\lookback_120D\\validation_20251231-091148.csv\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_180D\n",
      "Input MA file: ./10a-multiple_100MA_enriched_universe\\lookback_180D\\enriched_universe_ma100.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved → ./11a-multiple_universe_with_jump90\\lookback_180D\\universe_with_jump90.parquet\n",
      "✔ Validation log → ./system_verification/11a-multiple_universe_with_jump90\\lookback_180D\\validation_20251231-091148.csv\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_252D\n",
      "Input MA file: ./10a-multiple_100MA_enriched_universe\\lookback_252D\\enriched_universe_ma100.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved → ./11a-multiple_universe_with_jump90\\lookback_252D\\universe_with_jump90.parquet\n",
      "✔ Validation log → ./system_verification/11a-multiple_universe_with_jump90\\lookback_252D\\validation_20251231-091148.csv\n",
      "\n",
      "=== COMPLETED ===\n",
      "Saved summary validation log → ./system_verification/11a-multiple_universe_with_jump90\\validation_summary_20251231-091148.csv\n",
      "Processed datasets: 5\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\"\n",
    "Purpose:\n",
    "    For each MA100-enriched universe (per regression lookback), merge Jump90 signals\n",
    "    and save a new parquet per lookback.\n",
    "\n",
    "Inputs (per lookback):\n",
    "    ./10a-multiple_100MA_enriched_universe/lookback_XXD/enriched_universe_ma100.parquet\n",
    "\n",
    "Jump90 per ticker:\n",
    "    ./6-90Day_jump_filter_adjusted_all_prices/{TICKER}.parquet\n",
    "\n",
    "Outputs (per lookback):\n",
    "    ./11a-multiple_universe_with_jump90/lookback_XXD/universe_with_jump90.parquet\n",
    "\n",
    "Validation (per lookback + summary):\n",
    "    ./system_verification/11a-multiple_universe_with_jump90/lookback_XXD/validation_<timestamp>.csv\n",
    "    ./system_verification/11a-multiple_universe_with_jump90/validation_summary_<timestamp>.csv\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "MA_ENRICHED_ROOT = \"./10a-multiple_100MA_enriched_universe\"\n",
    "JUMP90_DIR       = \"./6-90Day_jump_filter_adjusted_all_prices\"\n",
    "\n",
    "OUTPUT_ROOT      = \"./11a-multiple_universe_with_jump90\"\n",
    "VER_ROOT         = \"./system_verification/11a-multiple_universe_with_jump90\"\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "os.makedirs(VER_ROOT, exist_ok=True)\n",
    "\n",
    "MA_FILENAME  = \"enriched_universe_ma100.parquet\"   # from the prior script\n",
    "OUT_FILENAME = \"universe_with_jump90.parquet\"\n",
    "\n",
    "print(\"=== Building Enriched Universes (Add Jump90 Signals — No Filtering) per Lookback ===\")\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def discover_lookback_dirs(root: str):\n",
    "    \"\"\"\n",
    "    Finds subdirectories like lookback_60D, lookback_90D, etc., sorted by numeric window.\n",
    "    \"\"\"\n",
    "    pat = re.compile(r\"^lookback_(\\d+)D$\")\n",
    "    found = []\n",
    "    for name in os.listdir(root):\n",
    "        full = os.path.join(root, name)\n",
    "        if os.path.isdir(full):\n",
    "            m = pat.match(name)\n",
    "            if m:\n",
    "                found.append((int(m.group(1)), name))\n",
    "    found.sort(key=lambda x: x[0])\n",
    "    return [name for _, name in found]\n",
    "\n",
    "def safe_read_parquet(path: str):\n",
    "    try:\n",
    "        return pd.read_parquet(path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ============================================================\n",
    "# 1) LOAD JUMP90 FILES ONCE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nLoading Jump90 files...\")\n",
    "\n",
    "jump_map = {}\n",
    "bad_jump_files = []\n",
    "\n",
    "for fname in os.listdir(JUMP90_DIR):\n",
    "    if not fname.endswith(\".parquet\"):\n",
    "        continue\n",
    "\n",
    "    ticker = fname.replace(\".parquet\", \"\")\n",
    "    fpath = os.path.join(JUMP90_DIR, fname)\n",
    "\n",
    "    tmp = safe_read_parquet(fpath)\n",
    "    if tmp is None:\n",
    "        bad_jump_files.append(ticker)\n",
    "        continue\n",
    "\n",
    "    need = {\"date\", \"pct_change\", \"abs_pct\", \"abs_rollmax_90\", \"no_big_jump_90\"}\n",
    "    if not need.issubset(tmp.columns):\n",
    "        bad_jump_files.append(ticker)\n",
    "        continue\n",
    "\n",
    "    tmp = tmp[[\"date\", \"pct_change\", \"abs_pct\", \"abs_rollmax_90\", \"no_big_jump_90\"]].copy()\n",
    "    tmp[\"date\"] = pd.to_datetime(tmp[\"date\"])\n",
    "    jump_map[ticker] = tmp\n",
    "\n",
    "print(f\"Loaded Jump90: {len(jump_map)} tickers\")\n",
    "print(f\"Unreadable/invalid Jump90 files: {len(bad_jump_files)} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) DISCOVER MA-ENRICHED UNIVERSE FILES (PER LOOKBACK)\n",
    "# ============================================================\n",
    "\n",
    "lookback_dirs = discover_lookback_dirs(MA_ENRICHED_ROOT)\n",
    "\n",
    "jobs = []\n",
    "if lookback_dirs:\n",
    "    for lb in lookback_dirs:\n",
    "        f = os.path.join(MA_ENRICHED_ROOT, lb, MA_FILENAME)\n",
    "        if os.path.exists(f):\n",
    "            jobs.append((lb, f))\n",
    "        else:\n",
    "            print(f\"⚠ Missing MA-enriched file for {lb}: {f}\")\n",
    "else:\n",
    "    # Optional fallback: if someone created a root-level MA file\n",
    "    root_file = os.path.join(MA_ENRICHED_ROOT, MA_FILENAME)\n",
    "    if os.path.exists(root_file):\n",
    "        jobs.append((\"root\", root_file))\n",
    "\n",
    "if not jobs:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No MA-enriched universe files found. Expected \"\n",
    "        f\"{MA_ENRICHED_ROOT}/lookback_XXD/{MA_FILENAME} (or {MA_ENRICHED_ROOT}/{MA_FILENAME}).\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nFound {len(jobs)} MA-enriched universes to process.\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) PROCESS EACH MA-ENRICHED UNIVERSE AND MERGE JUMP90\n",
    "# ============================================================\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "summary_rows = []\n",
    "\n",
    "for lookback, ma_path in jobs:\n",
    "    print(\"\\n----------------------------------------------------\")\n",
    "    print(f\"Processing lookback: {lookback}\")\n",
    "    print(f\"Input MA file: {ma_path}\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "\n",
    "    df = safe_read_parquet(ma_path)\n",
    "    if df is None or df.empty:\n",
    "        print(f\"❌ Could not read or empty input: {ma_path}\")\n",
    "        continue\n",
    "\n",
    "    if \"date\" not in df.columns or \"ticker\" not in df.columns:\n",
    "        print(f\"❌ Input missing required columns 'date'/'ticker': {ma_path}\")\n",
    "        continue\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    # merge Jump90 per ticker (same pattern as your script)\n",
    "    rows = []\n",
    "    missing_jump_tickers = []\n",
    "\n",
    "    for ticker, sub in df.groupby(\"ticker\", sort=False):\n",
    "        if ticker in jump_map:\n",
    "            merged = sub.merge(jump_map[ticker], on=\"date\", how=\"left\")\n",
    "        else:\n",
    "            merged = sub.copy()\n",
    "            merged[\"pct_change\"]     = np.nan\n",
    "            merged[\"abs_pct\"]        = np.nan\n",
    "            merged[\"abs_rollmax_90\"] = np.nan\n",
    "            merged[\"no_big_jump_90\"] = np.nan\n",
    "            missing_jump_tickers.append(ticker)\n",
    "\n",
    "        rows.append(merged)\n",
    "\n",
    "    df2 = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # clean column order (like your script)\n",
    "    desired_cols = [\n",
    "        \"date\",\n",
    "        \"open_adj\", \"high_adj\", \"low_adj\", \"close_adj\", \"volume\",\n",
    "        \"ma100\", \"above_ma100\",\n",
    "        \"pct_change\", \"abs_pct\", \"abs_rollmax_90\", \"no_big_jump_90\",\n",
    "        \"slope_annual\", \"r2\", \"slope_adj\",\n",
    "        \"ticker\"\n",
    "    ]\n",
    "    existing_cols = [c for c in desired_cols if c in df2.columns]\n",
    "    df2 = df2[existing_cols].copy()\n",
    "\n",
    "    # sort\n",
    "    df2 = df2.sort_values([\"date\", \"ticker\"]).reset_index(drop=True)\n",
    "\n",
    "    # output dirs per lookback\n",
    "    out_dir = OUTPUT_ROOT if lookback == \"root\" else os.path.join(OUTPUT_ROOT, lookback)\n",
    "    ver_dir = VER_ROOT if lookback == \"root\" else os.path.join(VER_ROOT, lookback)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(ver_dir, exist_ok=True)\n",
    "\n",
    "    out_path = os.path.join(out_dir, OUT_FILENAME)\n",
    "    df2.to_parquet(out_path, index=False)\n",
    "\n",
    "    # validation per lookback\n",
    "    val = pd.DataFrame([{\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"lookback\": lookback,\n",
    "        \"input_ma_file\": ma_path,\n",
    "        \"rows_input\": int(df.shape[0]),\n",
    "        \"rows_after_merge\": int(df2.shape[0]),\n",
    "        \"unique_tickers\": int(df[\"ticker\"].nunique()),\n",
    "        \"missing_jump_tickers\": int(len(set(missing_jump_tickers))),\n",
    "        \"null_pct_change\": int(df2[\"pct_change\"].isna().sum()) if \"pct_change\" in df2.columns else -1,\n",
    "        \"null_no_big_jump_90\": int(df2[\"no_big_jump_90\"].isna().sum()) if \"no_big_jump_90\" in df2.columns else -1,\n",
    "        \"output_file\": out_path,\n",
    "    }])\n",
    "\n",
    "    val_path = os.path.join(ver_dir, f\"validation_{ts}.csv\")\n",
    "    val.to_csv(val_path, index=False)\n",
    "\n",
    "    print(f\"✔ Saved → {out_path}\")\n",
    "    print(f\"✔ Validation log → {val_path}\")\n",
    "\n",
    "    summary_rows.append(val.iloc[0].to_dict())\n",
    "\n",
    "# ============================================================\n",
    "# 4) SAVE OVERALL SUMMARY LOG\n",
    "# ============================================================\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_path = os.path.join(VER_ROOT, f\"validation_summary_{ts}.csv\")\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"\\n=== COMPLETED ===\")\n",
    "print(f\"Saved summary validation log → {summary_path}\")\n",
    "print(f\"Processed datasets: {len(summary_df):,}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
