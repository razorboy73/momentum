{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "314a3102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BUILDING TRADABLE SP500 UNIVERSE (post-1998) for EACH LOOKBACK ===\n",
      "Loaded membership table: 1,192 tickers\n",
      "Found 5 universes to trim.\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_60D\n",
      "Input: ./11a-multiple_universe_with_jump90\\lookback_60D\\universe_with_jump90.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved tradable SP500 universe → ./12a-multiple-tradable_sp500_universe\\12-tradable_sp500_universe_60D.parquet\n",
      "✔ Saved validation log          → ./system_verification/12a-multiple-tradable_sp500_universe\\validation_60D_20251231-100020.csv\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_90D\n",
      "Input: ./11a-multiple_universe_with_jump90\\lookback_90D\\universe_with_jump90.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved tradable SP500 universe → ./12a-multiple-tradable_sp500_universe\\12-tradable_sp500_universe_90D.parquet\n",
      "✔ Saved validation log          → ./system_verification/12a-multiple-tradable_sp500_universe\\validation_90D_20251231-100020.csv\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_120D\n",
      "Input: ./11a-multiple_universe_with_jump90\\lookback_120D\\universe_with_jump90.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved tradable SP500 universe → ./12a-multiple-tradable_sp500_universe\\12-tradable_sp500_universe_120D.parquet\n",
      "✔ Saved validation log          → ./system_verification/12a-multiple-tradable_sp500_universe\\validation_120D_20251231-100020.csv\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_180D\n",
      "Input: ./11a-multiple_universe_with_jump90\\lookback_180D\\universe_with_jump90.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved tradable SP500 universe → ./12a-multiple-tradable_sp500_universe\\12-tradable_sp500_universe_180D.parquet\n",
      "✔ Saved validation log          → ./system_verification/12a-multiple-tradable_sp500_universe\\validation_180D_20251231-100020.csv\n",
      "\n",
      "----------------------------------------------------\n",
      "Processing lookback: lookback_252D\n",
      "Input: ./11a-multiple_universe_with_jump90\\lookback_252D\\universe_with_jump90.parquet\n",
      "----------------------------------------------------\n",
      "✔ Saved tradable SP500 universe → ./12a-multiple-tradable_sp500_universe\\12-tradable_sp500_universe_252D.parquet\n",
      "✔ Saved validation log          → ./system_verification/12a-multiple-tradable_sp500_universe\\validation_252D_20251231-100020.csv\n",
      "\n",
      "=== COMPLETED ===\n",
      "Saved summary validation log → ./system_verification/12a-multiple-tradable_sp500_universe\\validation_summary_20251231-100020.csv\n",
      "Processed datasets: 5\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\"\n",
    "Purpose:\n",
    "    Trim each universe_with_jump90.parquet (per regression lookback) to only S&P 500\n",
    "    constituents on each date, and save the output file with the lookback in the filename.\n",
    "\n",
    "Inputs (per lookback):\n",
    "    ./11a-multiple_universe_with_jump90/lookback_XXD/universe_with_jump90.parquet\n",
    "\n",
    "Membership:\n",
    "    ./1-sp500_membership_daily_matrix/sp500_membership_join_exit_date.parquet\n",
    "    columns: first_join_date, last_exit_date\n",
    "    ticker may be index or column.\n",
    "\n",
    "Outputs (one file per lookback, filename includes lookback):\n",
    "    ./12a-multiple-tradable_sp500_universe/12a-multiple-tradable_sp500_universe_XXD.parquet\n",
    "\n",
    "Validation:\n",
    "    ./system_verification/12a-multiple-tradable_sp500_universe/validation_XXD_<timestamp>.csv\n",
    "    ./system_verification/12a-multiple-tradable_sp500_universe/validation_summary_<timestamp>.csv\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "INPUT_ROOT      = \"./11a-multiple_universe_with_jump90\"\n",
    "MEMBERSHIP_FILE = \"./1-sp500_membership_daily_matrix/sp500_membership_join_exit_date.parquet\"\n",
    "\n",
    "OUTPUT_DIR      = \"./12a-multiple-tradable_sp500_universe\"\n",
    "VER_DIR         = \"./system_verification/12a-multiple-tradable_sp500_universe\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(VER_DIR, exist_ok=True)\n",
    "\n",
    "INPUT_FILENAME = \"universe_with_jump90.parquet\"\n",
    "DATE_CUTOFF = pd.Timestamp(\"1998-01-01\")\n",
    "\n",
    "print(\"=== BUILDING TRADABLE SP500 UNIVERSE (post-1998) for EACH LOOKBACK ===\")\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def discover_lookback_dirs(root: str):\n",
    "    \"\"\"Find subdirectories like lookback_60D, lookback_90D, etc., sorted by numeric window.\"\"\"\n",
    "    pat = re.compile(r\"^lookback_(\\d+)D$\")\n",
    "    found = []\n",
    "    for name in os.listdir(root):\n",
    "        full = os.path.join(root, name)\n",
    "        if os.path.isdir(full):\n",
    "            m = pat.match(name)\n",
    "            if m:\n",
    "                found.append((int(m.group(1)), name))\n",
    "    found.sort(key=lambda x: x[0])\n",
    "    return found  # list of (window_int, dir_name)\n",
    "\n",
    "def safe_read_parquet(path: str):\n",
    "    try:\n",
    "        return pd.read_parquet(path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ============================================================\n",
    "# 1) LOAD MEMBERSHIP TABLE (ONCE)\n",
    "# ============================================================\n",
    "\n",
    "m = safe_read_parquet(MEMBERSHIP_FILE)\n",
    "if m is None or m.empty:\n",
    "    raise FileNotFoundError(f\"Could not read membership file: {MEMBERSHIP_FILE}\")\n",
    "\n",
    "# normalize to a 'ticker' column (membership often uses ticker as index)\n",
    "if \"ticker\" not in m.columns:\n",
    "    if m.index.name is None:\n",
    "        m = m.reset_index().rename(columns={\"index\": \"ticker\"})\n",
    "    else:\n",
    "        m = m.reset_index().rename(columns={m.index.name: \"ticker\"})\n",
    "\n",
    "need_cols = {\"ticker\", \"first_join_date\", \"last_exit_date\"}\n",
    "if not need_cols.issubset(m.columns):\n",
    "    raise ValueError(f\"Membership file missing columns: {need_cols - set(m.columns)}\")\n",
    "\n",
    "m[\"first_join_date\"] = pd.to_datetime(m[\"first_join_date\"])\n",
    "m[\"last_exit_date\"]  = pd.to_datetime(m[\"last_exit_date\"])\n",
    "m = m[[\"ticker\", \"first_join_date\", \"last_exit_date\"]].copy()\n",
    "\n",
    "print(f\"Loaded membership table: {len(m):,} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) DISCOVER INPUT UNIVERSES (PER LOOKBACK)\n",
    "# ============================================================\n",
    "\n",
    "lookbacks = discover_lookback_dirs(INPUT_ROOT)\n",
    "\n",
    "jobs = []\n",
    "if lookbacks:\n",
    "    for w, lb_dir in lookbacks:\n",
    "        f = os.path.join(INPUT_ROOT, lb_dir, INPUT_FILENAME)\n",
    "        if os.path.exists(f):\n",
    "            jobs.append((w, lb_dir, f))\n",
    "        else:\n",
    "            print(f\"⚠ Missing input universe for {lb_dir}: {f}\")\n",
    "else:\n",
    "    # Optional fallback if there is a root-level file (older pipeline)\n",
    "    root_file = os.path.join(INPUT_ROOT, INPUT_FILENAME)\n",
    "    if os.path.exists(root_file):\n",
    "        jobs.append((-1, \"root\", root_file))\n",
    "\n",
    "if not jobs:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No input universes found. Expected {INPUT_ROOT}/lookback_XXD/{INPUT_FILENAME} \"\n",
    "        f\"(or {INPUT_ROOT}/{INPUT_FILENAME}).\"\n",
    "    )\n",
    "\n",
    "print(f\"Found {len(jobs)} universes to trim.\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) PROCESS EACH UNIVERSE\n",
    "# ============================================================\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "summary_rows = []\n",
    "\n",
    "for w, lb_dir, input_path in jobs:\n",
    "    lookback_label = f\"{w}D\" if w > 0 else \"root\"\n",
    "\n",
    "    print(\"\\n----------------------------------------------------\")\n",
    "    print(f\"Processing lookback: {lb_dir}\")\n",
    "    print(f\"Input: {input_path}\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "\n",
    "    df = safe_read_parquet(input_path)\n",
    "    if df is None or df.empty:\n",
    "        print(f\"❌ Could not read or empty: {input_path}\")\n",
    "        continue\n",
    "\n",
    "    if \"date\" not in df.columns or \"ticker\" not in df.columns:\n",
    "        print(f\"❌ Missing required columns ('date','ticker') in: {input_path}\")\n",
    "        continue\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    rows_in = int(len(df))\n",
    "\n",
    "    # 1) date cutoff\n",
    "    df = df[df[\"date\"] >= DATE_CUTOFF].copy()\n",
    "    rows_after_cutoff = int(len(df))\n",
    "\n",
    "    # 2) merge membership join/exit dates\n",
    "    df = df.merge(m, on=\"ticker\", how=\"left\")\n",
    "\n",
    "    # 3) apply membership window (vectorized)\n",
    "    # in_sp500 = join exists AND date >= join AND (exit is NaT OR date <= exit)\n",
    "    join = df[\"first_join_date\"]\n",
    "    exit_ = df[\"last_exit_date\"]\n",
    "    d = df[\"date\"]\n",
    "\n",
    "    df[\"in_sp500\"] = join.notna() & (d >= join) & (exit_.isna() | (d <= exit_))\n",
    "\n",
    "    df2 = df[df[\"in_sp500\"]].copy()\n",
    "    rows_out = int(len(df2))\n",
    "\n",
    "    # Sort: date ↑ then slope_adj ↓ (if exists)\n",
    "    if \"slope_adj\" in df2.columns:\n",
    "        df2 = df2.sort_values([\"date\", \"slope_adj\"], ascending=[True, False])\n",
    "    else:\n",
    "        df2 = df2.sort_values([\"date\", \"ticker\"], ascending=[True, True])\n",
    "\n",
    "    # ============================================================\n",
    "    # SAVE OUTPUT (filename includes lookback)\n",
    "    # ============================================================\n",
    "\n",
    "    out_name = f\"12-tradable_sp500_universe_{lookback_label}.parquet\"\n",
    "    out_path = os.path.join(OUTPUT_DIR, out_name)\n",
    "    df2.to_parquet(out_path, index=False)\n",
    "\n",
    "    # Validation log per lookback (filename includes lookback)\n",
    "    missing_membership_rows = int(df[\"first_join_date\"].isna().sum())\n",
    "\n",
    "    val = pd.DataFrame([{\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"lookback_dir\": lb_dir,\n",
    "        \"lookback_label\": lookback_label,\n",
    "        \"input_file\": input_path,\n",
    "        \"output_file\": out_path,\n",
    "        \"rows_input\": rows_in,\n",
    "        \"rows_after_1998_cutoff\": rows_after_cutoff,\n",
    "        \"rows_output_in_sp500\": rows_out,\n",
    "        \"unique_tickers_input\": int(df[\"ticker\"].nunique()),\n",
    "        \"unique_tickers_output\": int(df2[\"ticker\"].nunique()) if not df2.empty else 0,\n",
    "        \"rows_missing_membership_info\": missing_membership_rows,\n",
    "    }])\n",
    "\n",
    "    val_path = os.path.join(VER_DIR, f\"validation_{lookback_label}_{ts}.csv\")\n",
    "    val.to_csv(val_path, index=False)\n",
    "\n",
    "    print(f\"✔ Saved tradable SP500 universe → {out_path}\")\n",
    "    print(f\"✔ Saved validation log          → {val_path}\")\n",
    "\n",
    "    summary_rows.append(val.iloc[0].to_dict())\n",
    "\n",
    "# ============================================================\n",
    "# 4) SAVE SUMMARY LOG\n",
    "# ============================================================\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_path = os.path.join(VER_DIR, f\"validation_summary_{ts}.csv\")\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"\\n=== COMPLETED ===\")\n",
    "print(f\"Saved summary validation log → {summary_path}\")\n",
    "print(f\"Processed datasets: {len(summary_df):,}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
