{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "919545c5",
      "metadata": {},
      "source": [
        "# 28 – Trade Execution & Reconciliation (updated)\n",
        "\n",
        "This notebook version computes `slippage_dollars` primarily as **signal-to-fill** slippage using your master trades file matched by `plan_id`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "754e363b",
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reconciliation complete\n",
            "New fills applied: 9\n",
            "Final cash: 192,402.51\n",
            "Open positions: 5\n",
            "MASTER COLUMNS: ['plan_id', 'created_ts', 'signal_date', 'exec_date', 'ticker', 'side', 'shares', 'est_exec_px', 'est_value', 'reason', 'slope_rank', 'spy_above_200dma', 'cash_before', 'cash_after', 'portfolio_after', 'num_positions_after']\n",
            "Has plan_id? True\n",
            "Has signal px col? True\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "28 – Trade Execution & Reconciliation (Robust, Plan-Aware, Append-Only)\n",
        "\n",
        "Fix: Defensive ingestion for broker_fills_manual.csv.\n",
        "- Normalizes column names (case/spacing) and maps common broker aliases\n",
        "- Ensures REQUIRED_FILL_COLS exist (creates safe defaults where possible)\n",
        "- Validates hard-required execution truth fields\n",
        "- Keeps executed_trades.csv append-only + idempotent via broker_order_id\n",
        "\n",
        "CHANGE (signal-to-fill slippage):\n",
        "- slippage_dollars is now computed vs *signal price* (from your master trades file) when available:\n",
        "    BUY  : (fill_price - signal_price) * shares\n",
        "    SELL : (signal_price - fill_price) * shares\n",
        "  (positive = cost/worse, negative = improvement/better)\n",
        "- Falls back to order_price-based slippage only if signal price is missing.\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================\n",
        "# Imports\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from pandas.errors import EmptyDataError\n",
        "\n",
        "# ============================================================\n",
        "# Configuration\n",
        "# ============================================================\n",
        "\n",
        "LIVE_ROOT = \"./27a-2G_live_trading\"\n",
        "\n",
        "LIVE_PORTFOLIO_FILE   = f\"{LIVE_ROOT}/live_portfolio.csv\"          # AUTHORITATIVE STATE\n",
        "MANUAL_FILLS_FILE     = f\"{LIVE_ROOT}/broker_fills_manual.csv\"     # HUMAN / BROKER INPUT\n",
        "EXECUTED_TRADES_FILE  = f\"{LIVE_ROOT}/executed_trades.csv\"         # IMMUTABLE LEDGER\n",
        "RECON_LOG_FILE        = f\"{LIVE_ROOT}/reconciliation_log.csv\"      # DIAGNOSTICS ONLY\n",
        "\n",
        "# ---- Master trades file (for signal price by plan_id) ----\n",
        "MASTER_TRADES_FILE     = \"./27a-2G_live_trading/master_trades.csv\"\n",
        "MASTER_PLAN_ID_COL     = \"plan_id\"\n",
        "MASTER_SIGNAL_PX_COL = \"est_exec_px\" # <-- change if your master file uses a different name\n",
        "\n",
        "os.makedirs(LIVE_ROOT, exist_ok=True)\n",
        "INITIAL_CASH = 342000   # sensible default; change as needed\n",
        "\n",
        "# ============================================================\n",
        "# Required Fill Columns (Minimum Execution Truth)\n",
        "# ============================================================\n",
        "\n",
        "REQUIRED_FILL_COLS = {\n",
        "    \"plan_id\",\n",
        "    \"signal_date\",\n",
        "    \"exec_date\",\n",
        "    \"ticker\",\n",
        "    \"side\",\n",
        "    \"shares_filled\",\n",
        "    \"order_type\",\n",
        "    \"order_price\",\n",
        "    \"fill_price\",\n",
        "    \"broker_fee\",\n",
        "    \"broker_order_id\",\n",
        "}\n",
        "\n",
        "# These are truly non-negotiable for correct reconciliation\n",
        "HARD_REQUIRED = {\"ticker\", \"side\", \"shares_filled\", \"fill_price\", \"broker_order_id\"}\n",
        "\n",
        "# ============================================================\n",
        "# Helpers: Column normalization + alias mapping\n",
        "# ============================================================\n",
        "\n",
        "def _snake(s: str) -> str:\n",
        "    \"\"\"Normalize headers to snake_case-ish.\"\"\"\n",
        "    s = str(s).strip().lower()\n",
        "    s = re.sub(r\"[^\\w]+\", \"_\", s)          # spaces, hyphens, etc -> underscore\n",
        "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
        "    return s\n",
        "\n",
        "# Canonical column -> common aliases you might see in broker exports / manual entry\n",
        "COL_ALIASES = {\n",
        "    \"plan_id\":        [\"plan\", \"strategy_id\", \"signal_id\", \"trade_plan_id\", \"plan\"],\n",
        "    \"signal_date\":    [\"signal_dt\", \"signal_time\", \"signal_timestamp\", \"signal_datetime\", \"signaldate\"],\n",
        "    \"exec_date\":      [\"execution_date\", \"execution_dt\", \"exec_dt\", \"fill_date\", \"filled_at\", \"timestamp\", \"time\", \"datetime\"],\n",
        "    \"ticker\":         [\"symbol\", \"underlying\", \"asset\", \"security\", \"instrument\"],\n",
        "    \"side\":           [\"buy_sell\", \"b_s\", \"direction\", \"action\", \"bs\"],\n",
        "    \"shares_filled\":  [\"qty\", \"quantity\", \"filled_qty\", \"filled_quantity\", \"shares\", \"filled_shares\", \"size\", \"units\"],\n",
        "    \"order_type\":     [\"type\", \"ord_type\", \"orderstyle\", \"order_style\"],\n",
        "    \"order_price\":    [\"limit_price\", \"limit\", \"lmt_price\", \"order_limit_price\", \"ref_price\", \"requested_price\"],\n",
        "    \"fill_price\":     [\"price\", \"avg_price\", \"average_price\", \"execution_price\", \"fill\", \"fill_px\", \"fillprice\"],\n",
        "    \"broker_fee\":     [\"commission\", \"commissions\", \"fees\", \"fee\", \"broker_commission\"],\n",
        "    \"broker_order_id\":[\"order_id\", \"id\", \"brokerid\", \"broker_orderid\", \"orderid\", \"trade_id\", \"execution_id\"],\n",
        "}\n",
        "\n",
        "def normalize_and_map_columns(df: pd.DataFrame) -> tuple[pd.DataFrame, dict]:\n",
        "    \"\"\"\n",
        "    Returns (df, mapping_info) where mapping_info shows what got renamed/created.\n",
        "    \"\"\"\n",
        "    mapping_info = {\"renamed\": {}, \"created\": []}\n",
        "\n",
        "    # Normalize existing columns\n",
        "    original_cols = list(df.columns)\n",
        "    normalized_cols = [_snake(c) for c in original_cols]\n",
        "    df.columns = normalized_cols\n",
        "\n",
        "    # Build reverse lookup: alias -> canonical\n",
        "    alias_to_canon = {}\n",
        "    for canon, aliases in COL_ALIASES.items():\n",
        "        alias_to_canon[_snake(canon)] = canon  # allow exact canonical name\n",
        "        for a in aliases:\n",
        "            alias_to_canon[_snake(a)] = canon\n",
        "\n",
        "    # Rename any columns that match an alias\n",
        "    rename_map = {}\n",
        "    for c in df.columns:\n",
        "        if c in alias_to_canon:\n",
        "            canon = alias_to_canon[c]\n",
        "            if c != canon and canon not in df.columns:\n",
        "                rename_map[c] = canon\n",
        "\n",
        "    if rename_map:\n",
        "        df = df.rename(columns=rename_map)\n",
        "        mapping_info[\"renamed\"] = dict(rename_map)\n",
        "\n",
        "    # Ensure all REQUIRED_FILL_COLS exist (create safe defaults if missing)\n",
        "    defaults = {\n",
        "        \"plan_id\": \"\",\n",
        "        \"signal_date\": pd.NaT,\n",
        "        \"exec_date\": pd.NaT,\n",
        "        \"order_type\": \"MKT\",\n",
        "        \"order_price\": np.nan,\n",
        "        \"broker_fee\": 0.0,\n",
        "    }\n",
        "    for col in REQUIRED_FILL_COLS:\n",
        "        if col not in df.columns:\n",
        "            df[col] = defaults.get(col, np.nan)\n",
        "            mapping_info[\"created\"].append(col)\n",
        "\n",
        "    return df, mapping_info\n",
        "\n",
        "def normalize_side(x: str) -> str:\n",
        "    s = str(x).strip().upper()\n",
        "    if s in {\"B\", \"BUY\", \"BOT\", \"LONG\"}:\n",
        "        return \"BUY\"\n",
        "    if s in {\"S\", \"SELL\", \"SLD\", \"SHORT\"}:\n",
        "        return \"SELL\"\n",
        "    return s\n",
        "\n",
        "def _clean_plan_id(x) -> str:\n",
        "    \"\"\"Plan id as a stable string key (empty string if missing).\"\"\"\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    s = str(x).strip()\n",
        "    if s.lower() in {\"nan\", \"none\"}:\n",
        "        return \"\"\n",
        "    return s\n",
        "\n",
        "# ============================================================\n",
        "# Signal price map (plan_id -> signal_price)\n",
        "# ============================================================\n",
        "\n",
        "def load_signal_price_map(path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Loads a master trades file and returns: {plan_id(str) : signal_price(float)}.\n",
        "\n",
        "    If plan_id duplicates exist, last row (in file order) wins.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return {}\n",
        "\n",
        "    df = pd.read_parquet(path) if path.lower().endswith(\".parquet\") else pd.read_csv(path)\n",
        "\n",
        "    if MASTER_PLAN_ID_COL not in df.columns or MASTER_SIGNAL_PX_COL not in df.columns:\n",
        "        return {}\n",
        "\n",
        "    df[MASTER_PLAN_ID_COL] = df[MASTER_PLAN_ID_COL].apply(_clean_plan_id)\n",
        "    df[MASTER_SIGNAL_PX_COL] = pd.to_numeric(df[MASTER_SIGNAL_PX_COL], errors=\"coerce\")\n",
        "\n",
        "    # last one wins if duplicates\n",
        "    return (\n",
        "        df.dropna(subset=[MASTER_PLAN_ID_COL, MASTER_SIGNAL_PX_COL])\n",
        "          .groupby(MASTER_PLAN_ID_COL)[MASTER_SIGNAL_PX_COL]\n",
        "          .last()\n",
        "          .to_dict()\n",
        "    )\n",
        "\n",
        "signal_px_map = load_signal_price_map(MASTER_TRADES_FILE)\n",
        "if not signal_px_map:\n",
        "    print(\"WARNING: No signal price map loaded (missing file or columns). Slippage will fall back to order_price where available.\")\n",
        "\n",
        "# ============================================================\n",
        "# Load Manual Fills (Defensive Ingestion)\n",
        "# ============================================================\n",
        "\n",
        "if not os.path.exists(MANUAL_FILLS_FILE):\n",
        "    raise FileNotFoundError(f\"Missing broker fills file: {MANUAL_FILLS_FILE}\")\n",
        "\n",
        "fills_raw = pd.read_csv(MANUAL_FILLS_FILE)\n",
        "fills, ingest_info = normalize_and_map_columns(fills_raw)\n",
        "\n",
        "# Optional: show what happened (helps you debug broker export changes)\n",
        "if ingest_info[\"renamed\"] or ingest_info[\"created\"]:\n",
        "    print(\"Ingestion mapping:\")\n",
        "    if ingest_info[\"renamed\"]:\n",
        "        print(\"  Renamed columns:\", ingest_info[\"renamed\"])\n",
        "    if ingest_info[\"created\"]:\n",
        "        print(\"  Created missing required cols w/ defaults:\", ingest_info[\"created\"])\n",
        "\n",
        "# Coerce dates\n",
        "fills[\"signal_date\"] = pd.to_datetime(fills[\"signal_date\"], errors=\"coerce\")\n",
        "fills[\"exec_date\"] = pd.to_datetime(fills[\"exec_date\"], errors=\"coerce\")\n",
        "\n",
        "# Normalize fields / types\n",
        "fills[\"plan_id\"] = fills[\"plan_id\"].apply(_clean_plan_id)\n",
        "fills[\"ticker\"] = fills[\"ticker\"].astype(str).str.strip().str.upper()\n",
        "fills[\"side\"] = fills[\"side\"].apply(normalize_side)\n",
        "fills[\"shares_filled\"] = pd.to_numeric(fills[\"shares_filled\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "fills[\"order_type\"] = fills[\"order_type\"].astype(str).str.strip().str.upper()\n",
        "fills[\"order_price\"] = pd.to_numeric(fills[\"order_price\"], errors=\"coerce\")\n",
        "fills[\"fill_price\"] = pd.to_numeric(fills[\"fill_price\"], errors=\"coerce\")\n",
        "\n",
        "fills[\"broker_fee\"] = pd.to_numeric(fills[\"broker_fee\"], errors=\"coerce\").fillna(0.0)\n",
        "fills[\"broker_order_id\"] = fills[\"broker_order_id\"].astype(str).str.strip()\n",
        "\n",
        "# Validate hard-required truth fields\n",
        "missing_hard = [c for c in HARD_REQUIRED if c not in fills.columns]\n",
        "if missing_hard:\n",
        "    raise ValueError(f\"Still missing hard-required columns after mapping: {sorted(missing_hard)}\")\n",
        "\n",
        "invalid = (\n",
        "    (fills[\"ticker\"] == \"\") |\n",
        "    (~fills[\"side\"].isin([\"BUY\", \"SELL\"])) |\n",
        "    (fills[\"shares_filled\"] <= 0) |\n",
        "    (fills[\"fill_price\"].isna()) | (fills[\"fill_price\"] <= 0) |\n",
        "    (fills[\"broker_order_id\"] == \"\")\n",
        ")\n",
        "fills = fills[~invalid].copy()\n",
        "\n",
        "# Stable sort for deterministic replay; fill NaT exec_date so sort is stable\n",
        "fills[\"_exec_sort\"] = fills[\"exec_date\"].fillna(pd.Timestamp(\"1970-01-01\"))\n",
        "fills = fills.sort_values([\"_exec_sort\", \"ticker\", \"side\", \"broker_order_id\"]).drop(columns=[\"_exec_sort\"]).reset_index(drop=True)\n",
        "\n",
        "# ============================================================\n",
        "# Load Live Portfolio (Authoritative Input) — robust bootstrap\n",
        "# ============================================================\n",
        "\n",
        "def init_live_portfolio(path: str, cash: float) -> pd.DataFrame:\n",
        "    asof_ts = datetime.now().isoformat(timespec=\"seconds\")\n",
        "    df = pd.DataFrame([{\n",
        "        \"asof_ts\": asof_ts,\n",
        "        \"cash\": float(cash),\n",
        "        \"ticker\": \"\",\n",
        "        \"shares\": 0,\n",
        "    }])\n",
        "    df.to_csv(path, index=False)\n",
        "    return df\n",
        "\n",
        "if not os.path.exists(LIVE_PORTFOLIO_FILE) or os.path.getsize(LIVE_PORTFOLIO_FILE) == 0:\n",
        "    portfolio = init_live_portfolio(LIVE_PORTFOLIO_FILE, INITIAL_CASH)\n",
        "else:\n",
        "    try:\n",
        "        portfolio = pd.read_csv(LIVE_PORTFOLIO_FILE)\n",
        "    except EmptyDataError:\n",
        "        portfolio = init_live_portfolio(LIVE_PORTFOLIO_FILE, INITIAL_CASH)\n",
        "\n",
        "if portfolio.empty:\n",
        "    portfolio = init_live_portfolio(LIVE_PORTFOLIO_FILE, INITIAL_CASH)\n",
        "\n",
        "needed_port_cols = {\"cash\", \"ticker\", \"shares\"}\n",
        "missing_port = needed_port_cols - set(portfolio.columns)\n",
        "if missing_port:\n",
        "    raise ValueError(f\"live_portfolio.csv missing required columns: {sorted(missing_port)}\")\n",
        "\n",
        "cash = float(portfolio.iloc[0][\"cash\"])\n",
        "\n",
        "positions = {}\n",
        "for _, r in portfolio.iterrows():\n",
        "    t = str(r.get(\"ticker\", \"\")).strip().upper()\n",
        "    sh = int(r.get(\"shares\", 0) or 0)\n",
        "    if t and sh > 0:\n",
        "        positions[t] = sh\n",
        "\n",
        "# ============================================================\n",
        "# Idempotency: Skip Already Processed Fills\n",
        "# ============================================================\n",
        "\n",
        "already = set()\n",
        "if os.path.exists(EXECUTED_TRADES_FILE):\n",
        "    prior_exec = pd.read_csv(EXECUTED_TRADES_FILE)\n",
        "    if \"broker_order_id\" in prior_exec.columns:\n",
        "        already = set(prior_exec[\"broker_order_id\"].astype(str))\n",
        "\n",
        "fills = fills[~fills[\"broker_order_id\"].isin(already)].copy()\n",
        "\n",
        "# ============================================================\n",
        "# Apply Fills (Core Reconciliation)\n",
        "# ============================================================\n",
        "\n",
        "exec_rows = []\n",
        "log_rows = []\n",
        "\n",
        "for _, f in fills.iterrows():\n",
        "    tkr = f[\"ticker\"]\n",
        "    side = f[\"side\"]\n",
        "    sh = int(f[\"shares_filled\"])\n",
        "    fill_px = float(f[\"fill_price\"])\n",
        "    fee = float(f[\"broker_fee\"])\n",
        "\n",
        "    order_px = f.get(\"order_price\", np.nan)\n",
        "    order_px = float(order_px) if pd.notna(order_px) else np.nan\n",
        "\n",
        "    plan_id = _clean_plan_id(f.get(\"plan_id\", \"\"))\n",
        "\n",
        "    exec_date = (\n",
        "        f[\"exec_date\"].date().isoformat()\n",
        "        if pd.notna(f.get(\"exec_date\"))\n",
        "        else datetime.now().date().isoformat()\n",
        "    )\n",
        "\n",
        "    signal_date = (\n",
        "        f[\"signal_date\"].date().isoformat()\n",
        "        if pd.notna(f.get(\"signal_date\"))\n",
        "        else np.nan\n",
        "    )\n",
        "\n",
        "    gross = sh * fill_px\n",
        "    cash_before = cash\n",
        "    pos_before = positions.get(tkr, 0)\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Slippage (primary): vs SIGNAL price (system expectation)\n",
        "    # Fallback: vs order reference price (legacy)\n",
        "    # --------------------------------------------------------\n",
        "    signal_px = signal_px_map.get(plan_id, np.nan)\n",
        "    signal_px = float(signal_px) if pd.notna(signal_px) else np.nan\n",
        "\n",
        "    slippage = np.nan\n",
        "    slippage_ref = \"NA\"\n",
        "    if pd.notna(signal_px):\n",
        "        slippage_ref = \"SIGNAL\"\n",
        "        if side == \"BUY\":\n",
        "            slippage = (fill_px - signal_px) * sh\n",
        "        elif side == \"SELL\":\n",
        "            slippage = (signal_px - fill_px) * sh\n",
        "    elif pd.notna(order_px):\n",
        "        slippage_ref = \"ORDER\"\n",
        "        if side == \"BUY\":\n",
        "            slippage = (fill_px - order_px) * sh\n",
        "        elif side == \"SELL\":\n",
        "            slippage = (order_px - fill_px) * sh\n",
        "\n",
        "    if side == \"BUY\":\n",
        "        cash -= (gross + fee)\n",
        "        positions[tkr] = pos_before + sh\n",
        "\n",
        "    elif side == \"SELL\":\n",
        "        cash += (gross - fee)\n",
        "        new_sh = pos_before - sh\n",
        "        if new_sh < 0:\n",
        "            raise RuntimeError(f\"Oversell detected for {tkr}: have {pos_before}, tried to sell {sh}\")\n",
        "        if new_sh == 0:\n",
        "            positions.pop(tkr, None)\n",
        "        else:\n",
        "            positions[tkr] = new_sh\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid side: {side}\")\n",
        "\n",
        "    exec_rows.append({\n",
        "        \"exec_ts\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "        \"signal_date\": signal_date,\n",
        "        \"exec_date\": exec_date,\n",
        "        \"plan_id\": plan_id,\n",
        "        \"ticker\": tkr,\n",
        "        \"side\": side,\n",
        "        \"shares\": sh,\n",
        "        \"order_type\": f.get(\"order_type\", \"MKT\"),\n",
        "        \"signal_price\": signal_px,           # NEW\n",
        "        \"order_price\": order_px,\n",
        "        \"fill_price\": fill_px,\n",
        "        \"gross_notional\": gross,\n",
        "        \"broker_fee\": fee,\n",
        "        \"slippage_ref\": slippage_ref,        # NEW: SIGNAL / ORDER / NA\n",
        "        \"slippage_dollars\": slippage,        # NOW: signal-to-fill when available\n",
        "        \"net_cash_impact\": (-(gross + fee) if side == \"BUY\" else (gross - fee)),\n",
        "        \"broker_order_id\": f[\"broker_order_id\"],\n",
        "        \"cash_before\": cash_before,\n",
        "        \"cash_after\": cash,\n",
        "        \"pos_before\": pos_before,\n",
        "        \"pos_after\": positions.get(tkr, 0),\n",
        "    })\n",
        "\n",
        "    log_rows.append(exec_rows[-1])\n",
        "\n",
        "# ============================================================\n",
        "# Save Executed Trades (Append-Only)\n",
        "# ============================================================\n",
        "\n",
        "exec_df = pd.DataFrame(exec_rows)\n",
        "if os.path.exists(EXECUTED_TRADES_FILE):\n",
        "    prior = pd.read_csv(EXECUTED_TRADES_FILE)\n",
        "    exec_df = pd.concat([prior, exec_df], ignore_index=True)\n",
        "\n",
        "exec_df.to_csv(EXECUTED_TRADES_FILE, index=False)\n",
        "\n",
        "# ============================================================\n",
        "# Save Updated Live Portfolio\n",
        "# ============================================================\n",
        "\n",
        "asof_ts = datetime.now().isoformat(timespec=\"seconds\")\n",
        "port_rows = []\n",
        "\n",
        "if positions:\n",
        "    for t, sh in sorted(positions.items()):\n",
        "        port_rows.append({\"asof_ts\": asof_ts, \"cash\": cash, \"ticker\": t, \"shares\": sh})\n",
        "else:\n",
        "    port_rows.append({\"asof_ts\": asof_ts, \"cash\": cash, \"ticker\": \"\", \"shares\": 0})\n",
        "\n",
        "pd.DataFrame(port_rows).to_csv(LIVE_PORTFOLIO_FILE, index=False)\n",
        "\n",
        "# ============================================================\n",
        "# Save Reconciliation Log\n",
        "# ============================================================\n",
        "\n",
        "pd.DataFrame(log_rows).to_csv(RECON_LOG_FILE, index=False)\n",
        "\n",
        "print(\"Reconciliation complete\")\n",
        "print(f\"New fills applied: {len(exec_rows)}\")\n",
        "print(f\"Final cash: {cash:,.2f}\")\n",
        "print(f\"Open positions: {len(positions)}\")\n",
        "\n",
        "if str(MASTER_TRADES_FILE).lower().endswith(\".parquet\"):\n",
        "    df = pd.read_parquet(MASTER_TRADES_FILE)\n",
        "else:\n",
        "    df = pd.read_csv(MASTER_TRADES_FILE)\n",
        "\n",
        "print(\"MASTER COLUMNS:\", df.columns.tolist())\n",
        "print(\"Has plan_id?\", \"plan_id\" in df.columns)\n",
        "print(\"Has signal px col?\", MASTER_SIGNAL_PX_COL in df.columns)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "my_quant_lab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
