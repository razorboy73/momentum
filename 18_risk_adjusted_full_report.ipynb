{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c7fede0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading universe…\n",
      "Loaded universe: 3,585,390 rows\n",
      "Loading SPY regime file…\n",
      "Merging ATR20 per ticker…\n",
      "Universe with ATR20 merged: 3,585,390 rows\n",
      "\n",
      "=== LOOKAHEAD AUDIT FOR REGRESSION+SPYFILTER SYSTEM ===\n",
      "Start trading: 1999-01-01\n",
      "Initial capital: $1,000,000.00\n",
      "Top percentile (slope_adj): 0.90\n",
      "Rebalance day: Wednesday\n",
      "Lag scenarios: [0, 1, 3, 5], random lag 0–3\n",
      "\n",
      "\n",
      "=== RUNNING LOOKAHEAD AUDIT: lag_0d ===\n",
      "Saved trades       → ./18-risk_report_output\\trades_lag_0d.parquet\n",
      "Saved equity curve → ./18-risk_report_output\\equity_lag_0d.parquet\n",
      "Saved rebalance    → ./18-risk_report_output\\rebal_summary_lag_0d.parquet\n",
      "lag_0d:\n",
      "  CAGR        :  17.78%\n",
      "  Max DD      : -29.55%\n",
      "  Sharpe      :   1.06\n",
      "  Sortino     :   1.32\n",
      "  Trades      : 12760 (Buys=6262, Sells=6498)\n",
      "  Avg Trade $ : 565,217\n",
      "  Turnover/yr :   6.22x\n",
      "\n",
      "\n",
      "=== RUNNING LOOKAHEAD AUDIT: lag_1d ===\n",
      "Saved trades       → ./18-risk_report_output\\trades_lag_1d.parquet\n",
      "Saved equity curve → ./18-risk_report_output\\equity_lag_1d.parquet\n",
      "Saved rebalance    → ./18-risk_report_output\\rebal_summary_lag_1d.parquet\n",
      "lag_1d:\n",
      "  CAGR        :  17.10%\n",
      "  Max DD      : -28.75%\n",
      "  Sharpe      :   1.02\n",
      "  Sortino     :   1.32\n",
      "  Trades      : 12947 (Buys=6363, Sells=6584)\n",
      "  Avg Trade $ : 490,463\n",
      "  Turnover/yr :   6.32x\n",
      "\n",
      "\n",
      "=== RUNNING LOOKAHEAD AUDIT: lag_3d ===\n",
      "Saved trades       → ./18-risk_report_output\\trades_lag_3d.parquet\n",
      "Saved equity curve → ./18-risk_report_output\\equity_lag_3d.parquet\n",
      "Saved rebalance    → ./18-risk_report_output\\rebal_summary_lag_3d.parquet\n",
      "lag_3d:\n",
      "  CAGR        :  17.34%\n",
      "  Max DD      : -27.51%\n",
      "  Sharpe      :   1.02\n",
      "  Sortino     :   1.29\n",
      "  Trades      : 12918 (Buys=6342, Sells=6576)\n",
      "  Avg Trade $ : 494,070\n",
      "  Turnover/yr :   6.17x\n",
      "\n",
      "\n",
      "=== RUNNING LOOKAHEAD AUDIT: lag_5d ===\n",
      "Saved trades       → ./18-risk_report_output\\trades_lag_5d.parquet\n",
      "Saved equity curve → ./18-risk_report_output\\equity_lag_5d.parquet\n",
      "Saved rebalance    → ./18-risk_report_output\\rebal_summary_lag_5d.parquet\n",
      "lag_5d:\n",
      "  CAGR        :  16.19%\n",
      "  Max DD      : -32.44%\n",
      "  Sharpe      :   0.95\n",
      "  Sortino     :   1.18\n",
      "  Trades      : 12791 (Buys=6281, Sells=6510)\n",
      "  Avg Trade $ : 396,566\n",
      "  Turnover/yr :   6.21x\n",
      "\n",
      "\n",
      "=== RUNNING LOOKAHEAD AUDIT: rand_lag_0_3 ===\n",
      "Saved trades       → ./18-risk_report_output\\trades_rand_lag_0_3.parquet\n",
      "Saved equity curve → ./18-risk_report_output\\equity_rand_lag_0_3.parquet\n",
      "Saved rebalance    → ./18-risk_report_output\\rebal_summary_rand_lag_0_3.parquet\n",
      "rand_lag_0_3:\n",
      "  CAGR        :  17.95%\n",
      "  Max DD      : -28.65%\n",
      "  Sharpe      :   1.06\n",
      "  Sortino     :   1.35\n",
      "  Trades      : 12605 (Buys=6204, Sells=6401)\n",
      "  Avg Trade $ : 580,097\n",
      "  Turnover/yr :   6.16x\n",
      "\n",
      "\n",
      "=== LOOKAHEAD AUDIT SUMMARY ===\n",
      "       label  CAGR_%  MaxDD_%  Sharpe  Sortino  Trades  Buys  Sells  AvgTradeValue  TurnoverPerYear  AvgBuyRank  MedBuyRank  BuyRankP90  AvgPositions  MedPositions  AvgMaxWeight  AvgTop5WeightSum\n",
      "rand_lag_0_3   17.95   -28.65    1.06     1.35   12605  6204   6401      580096.76             6.16       35.65       40.00       50.00         28.95         34.00          0.15              0.35\n",
      "      lag_0d   17.78   -29.55    1.06     1.32   12760  6262   6498      565217.49             6.22       35.93       40.00       50.00         29.04         34.00          0.15              0.34\n",
      "      lag_3d   17.34   -27.51    1.02     1.29   12918  6342   6576      494070.19             6.17       35.66       40.00       50.00         29.40         34.00          0.15              0.35\n",
      "      lag_1d   17.10   -28.75    1.02     1.32   12947  6363   6584      490462.86             6.32       35.75       40.00       50.00         29.33         34.00          0.15              0.34\n",
      "      lag_5d   16.19   -32.44    0.95     1.18   12791  6281   6510      396565.78             6.21       35.74       40.00       50.00         29.22         34.00          0.15              0.35\n",
      "\n",
      "Saved audit stats → ./18-risk_report_output\\lookahead_audit_stats_regression_spyfilter.csv\n",
      "\n",
      "=== LOOKAHEAD AUDIT COMPLETE ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "UNIVERSE_FILE   = \"./12-tradable_sp500_universe/12-tradable_sp500_universe.parquet\"\n",
    "SPY_REGIME_FILE = \"./8-SPY_200DMA_market_regime/8-SPY_200DMA_regime.parquet\"\n",
    "ATR20_DIR       = \"./4-ATR20_adjusted_All_Prices\"\n",
    "\n",
    "OUTPUT_DIR      = \"./18-risk_report_output\"  \n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "START_TRADING   = pd.Timestamp(\"1999-01-01\")\n",
    "\n",
    "# Match live engine capital / params\n",
    "INITIAL_CAPITAL       = 1_000_000\n",
    "TOP_PERCENTILE        = 0.90      # top 10% by slope_adj\n",
    "REBALANCE_DAY         = \"Wednesday\"\n",
    "TRADING_DAYS_PER_YEAR = 252\n",
    "\n",
    "# Turnover / trade throttling – keep in sync with live engine\n",
    "DRIFT_THRESHOLD          = 0.01      # min abs weight diff to rebalance (~1%)\n",
    "MIN_TRADE_VALUE          = 3_000.0   # skip trades smaller than this\n",
    "MIN_NEW_POSITION_WEIGHT  = 0.005     # don't open new positions < 0.5% of equity\n",
    "\n",
    "# Lookahead audit scenarios\n",
    "LAG_SCENARIOS       = [0, 1, 3, 5]   # deterministic lags (in trading days)\n",
    "RAND_LAG_MAX        = 3              # random lag between 0 and RAND_LAG_MAX (inclusive)\n",
    "\n",
    "# Debug ticker (optional; set to None to disable)\n",
    "DEBUG_TICKER = None  # e.g. \"TWX\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FAST PRICE LOOKUP\n",
    "# ============================================================\n",
    "\n",
    "def fast_price_lookup(px_array, date_val):\n",
    "    \"\"\"\n",
    "    Given a structured array for a ticker:\n",
    "        px_array['date'] (datetime64[ns])\n",
    "        px_array['px']   (float)\n",
    "    return the latest price <= date_val.\n",
    "    \"\"\"\n",
    "    date_val = np.datetime64(date_val, \"ns\")\n",
    "    dates = px_array[\"date\"]\n",
    "    idx = np.searchsorted(dates, date_val, side=\"right\") - 1\n",
    "    if idx < 0:\n",
    "        return np.nan\n",
    "    return px_array[\"px\"][idx]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOAD UNIVERSE\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading universe…\")\n",
    "df = pd.read_parquet(UNIVERSE_FILE)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "df[\"slope_adj\"] = pd.to_numeric(df[\"slope_adj\"], errors=\"coerce\")\n",
    "df[\"close_adj\"] = pd.to_numeric(df[\"close_adj\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"Loaded universe: {len(df):,} rows\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD SPY REGIME AND MERGE\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading SPY regime file…\")\n",
    "spy = pd.read_parquet(SPY_REGIME_FILE)\n",
    "\n",
    "# Ensure we have a 'date' column\n",
    "if spy.index.name in [\"Date\", \"date\", None]:\n",
    "    spy = spy.reset_index().rename(columns={\"index\": \"date\", \"Date\": \"date\"})\n",
    "\n",
    "spy[\"date\"] = pd.to_datetime(spy[\"date\"])\n",
    "\n",
    "# Expect columns: spy_close, spy_ma200, market_regime (0/1)\n",
    "required_cols = [\"spy_close\", \"spy_ma200\", \"market_regime\"]\n",
    "missing = [c for c in required_cols if c not in spy.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"SPY regime file missing columns: {missing}\")\n",
    "\n",
    "# Merge SPY info into universe for each date\n",
    "df = df.merge(\n",
    "    spy[[\"date\", \"spy_close\", \"spy_ma200\", \"market_regime\"]],\n",
    "    on=\"date\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# MERGE ATR20 PER-TICKER\n",
    "# ============================================================\n",
    "\n",
    "print(\"Merging ATR20 per ticker…\")\n",
    "atr20_map = {}\n",
    "for f in os.listdir(ATR20_DIR):\n",
    "    if not f.endswith(\".parquet\"):\n",
    "        continue\n",
    "    t = f.replace(\".parquet\", \"\")\n",
    "    tmp = pd.read_parquet(os.path.join(ATR20_DIR, f))\n",
    "    if \"atr20\" not in tmp:\n",
    "        continue\n",
    "    tmp[\"date\"] = pd.to_datetime(tmp[\"date\"])\n",
    "    atr20_map[t] = tmp[[\"date\", \"atr20\"]]\n",
    "\n",
    "rows = []\n",
    "for t, sub in df.groupby(\"ticker\", sort=False):\n",
    "    if t in atr20_map:\n",
    "        rows.append(sub.merge(atr20_map[t], on=\"date\", how=\"left\"))\n",
    "    else:\n",
    "        sub = sub.copy()\n",
    "        sub[\"atr20\"] = np.nan\n",
    "        rows.append(sub)\n",
    "\n",
    "df = pd.concat(rows, ignore_index=True)\n",
    "print(f\"Universe with ATR20 merged: {len(df):,} rows\")\n",
    "\n",
    "# ============================================================\n",
    "# PREP GROUPED DATA\n",
    "# ============================================================\n",
    "\n",
    "df_by_date = {d: sub for d, sub in df.groupby(\"date\")}\n",
    "\n",
    "px_by_ticker = {}\n",
    "for t, sub in df.groupby(\"ticker\", sort=False):\n",
    "    sub = sub.sort_values(\"date\")\n",
    "    arr = np.zeros(\n",
    "        len(sub),\n",
    "        dtype=[(\"date\", \"datetime64[ns]\"), (\"px\", \"float64\")]\n",
    "    )\n",
    "    arr[\"date\"] = sub[\"date\"].values.astype(\"datetime64[ns]\")\n",
    "    arr[\"px\"]   = sub[\"close_adj\"].astype(float).values\n",
    "    px_by_ticker[t] = arr\n",
    "\n",
    "dates = sorted(df_by_date.keys())\n",
    "date_idx = {d: i for i, d in enumerate(dates)}  # for quick lag lookups\n",
    "\n",
    "# ============================================================\n",
    "# METRIC HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def cagr_from_curve(eq: pd.Series):\n",
    "    eq = eq.dropna()\n",
    "    if len(eq) < 2 or eq.iloc[0] <= 0:\n",
    "        return np.nan\n",
    "    years = (eq.index[-1] - eq.index[0]).days / 365.25\n",
    "    if years <= 0:\n",
    "        return np.nan\n",
    "    return (eq.iloc[-1] / eq.iloc[0]) ** (1.0 / years) - 1.0\n",
    "\n",
    "\n",
    "def max_dd(series: pd.Series):\n",
    "    series = series.dropna()\n",
    "    if series.empty:\n",
    "        return np.nan\n",
    "    rollmax = series.cummax()\n",
    "    dd = series / rollmax - 1.0\n",
    "    return float(dd.min())\n",
    "\n",
    "def sharpe_from_curve(series: pd.Series):\n",
    "    r = series.pct_change().dropna()\n",
    "    if len(r) < 2 or r.std(ddof=1) == 0:\n",
    "        return np.nan\n",
    "    return float(r.mean() / r.std(ddof=1) * np.sqrt(TRADING_DAYS_PER_YEAR))\n",
    "\n",
    "\n",
    "def sortino_from_curve(series: pd.Series):\n",
    "    r = series.pct_change().dropna()\n",
    "    downside = r[r < 0]\n",
    "    if len(downside) < 2 or downside.std(ddof=1) == 0:\n",
    "        return np.nan\n",
    "    return float(r.mean() / downside.std(ddof=1) * np.sqrt(TRADING_DAYS_PER_YEAR))\n",
    "\n",
    "\n",
    "def show_stats(label, eq: pd.Series, trades: pd.DataFrame):\n",
    "    c = cagr_from_curve(eq)\n",
    "    dd = max_dd(eq)\n",
    "    sh = sharpe_from_curve(eq)\n",
    "    so = sortino_from_curve(eq)\n",
    "\n",
    "    # Turnover stats\n",
    "    if len(eq) > 1:\n",
    "        years = (eq.index[-1] - eq.index[0]).days / 365.25\n",
    "    else:\n",
    "        years = np.nan\n",
    "    total_traded_val = trades[\"value\"].abs().sum() if not trades.empty else 0.0\n",
    "    avg_equity = eq.mean() if not eq.empty else np.nan\n",
    "    turnover_per_year = ((total_traded_val / 2.0) / avg_equity) / years if (years and avg_equity) else np.nan\n",
    "\n",
    "\n",
    "    n_trades = len(trades)\n",
    "    n_buys   = (trades[\"type\"] == \"BUY\").sum()\n",
    "    n_sells  = (trades[\"type\"] == \"SELL\").sum()\n",
    "    avg_trade_val = trades[\"value\"].abs().mean() if n_trades > 0 else np.nan\n",
    "\n",
    "    print(f\"{label}:\")\n",
    "    print(f\"  CAGR        : {c*100:6.2f}%\")\n",
    "    print(f\"  Max DD      : {dd*100:6.2f}%\")\n",
    "    print(f\"  Sharpe      : {sh:6.2f}\")\n",
    "    print(f\"  Sortino     : {so:6.2f}\")\n",
    "    print(f\"  Trades      : {n_trades} (Buys={n_buys}, Sells={n_sells})\")\n",
    "    print(f\"  Avg Trade $ : {avg_trade_val:,.0f}\")\n",
    "    print(f\"  Turnover/yr : {turnover_per_year:6.2f}x\\n\")\n",
    "\n",
    "    stats = {\n",
    "        \"label\": label,\n",
    "        \"CAGR\": c,\n",
    "        \"MaxDD\": dd,\n",
    "        \"Sharpe\": sh,\n",
    "        \"Sortino\": so,\n",
    "        \"Trades\": n_trades,\n",
    "        \"Buys\": n_buys,\n",
    "        \"Sells\": n_sells,\n",
    "        \"AvgTradeValue\": avg_trade_val,\n",
    "        \"TurnoverPerYear\": turnover_per_year,\n",
    "    }\n",
    "\n",
    "    # Buy-rank diagnostics if we recorded signal ranks\n",
    "    if \"signal_rank_within_top\" in trades.columns:\n",
    "        buys = trades[trades[\"type\"] == \"BUY\"]\n",
    "        if not buys.empty:\n",
    "            stats[\"AvgBuyRank\"] = buys[\"signal_rank_within_top\"].mean()\n",
    "            stats[\"MedBuyRank\"] = buys[\"signal_rank_within_top\"].median()\n",
    "            stats[\"BuyRankP90\"] = buys[\"signal_rank_within_top\"].quantile(0.9)\n",
    "        else:\n",
    "            stats[\"AvgBuyRank\"] = np.nan\n",
    "            stats[\"MedBuyRank\"] = np.nan\n",
    "            stats[\"BuyRankP90\"] = np.nan\n",
    "    else:\n",
    "        stats[\"AvgBuyRank\"] = np.nan\n",
    "        stats[\"MedBuyRank\"] = np.nan\n",
    "        stats[\"BuyRankP90\"] = np.nan\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# UTILITIES\n",
    "# ============================================================\n",
    "\n",
    "def is_rebalance_day(date):\n",
    "    \"\"\"Weekly rebalance on a specific weekday (e.g. Wednesday).\"\"\"\n",
    "    return date.day_name() == REBALANCE_DAY\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TRADING ENGINE WITH SIGNAL LAG (MATCHES LIVE ENGINE LOGIC)\n",
    "# ============================================================\n",
    "\n",
    "def run_strategy_with_lag(\n",
    "    signal_lag_days=0,\n",
    "    random_lag=False,\n",
    "    max_random_lag=RAND_LAG_MAX,\n",
    "    label=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the weekly regression-only / vol-based / SPY-filter strategy,\n",
    "    but with **signals computed on an earlier date**.\n",
    "\n",
    "    For every rebalance date D:\n",
    "        - Choose signal_date S:\n",
    "            * deterministic: S = D - signal_lag_days (by trading days)\n",
    "            * random:        S = D - U(0, max_random_lag)\n",
    "        - Use S's universe to:\n",
    "            * rank by slope_adj\n",
    "            * choose top percentile group\n",
    "            * use S's atr20 & close_adj for sizing\n",
    "            * use S's market_regime for SPY filter\n",
    "        - Execute trades at **D's prices**.\n",
    "        - Drift thresholds, min trade size, and min new position weight\n",
    "          are applied using D's prices and portfolio value,\n",
    "          exactly like the live engine.\n",
    "    \"\"\"\n",
    "\n",
    "    if label is None:\n",
    "        if random_lag:\n",
    "            label = f\"rand_lag_0_{max_random_lag}\"\n",
    "        else:\n",
    "            label = f\"lag_{signal_lag_days}d\"\n",
    "\n",
    "    print(f\"\\n=== RUNNING LOOKAHEAD AUDIT: {label} ===\")\n",
    "\n",
    "    cash = INITIAL_CAPITAL\n",
    "    positions = {}   # ticker -> {\"shares\": int, \"entry\": float}\n",
    "    history = []\n",
    "    equity_curve = []\n",
    "    rebal_summaries = []\n",
    "\n",
    "    last_equity = INITIAL_CAPITAL\n",
    "\n",
    "    for date in dates:\n",
    "        if date < START_TRADING:\n",
    "            continue\n",
    "\n",
    "        day_prices = df_by_date.get(date)\n",
    "        if day_prices is None or day_prices.empty:\n",
    "            continue\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # DETERMINE SIGNAL DATE FOR THIS REBALANCE\n",
    "        # --------------------------------------------------------\n",
    "        signal_day = None\n",
    "        signal_tg = None\n",
    "        spy_regime_signal = None  # 1/bull, 0/bear or None\n",
    "\n",
    "        do_rebalance = is_rebalance_day(date)\n",
    "        if do_rebalance:\n",
    "            idx = date_idx[date]\n",
    "            if random_lag:\n",
    "                lag = np.random.randint(0, max_random_lag + 1)\n",
    "            else:\n",
    "                lag = signal_lag_days\n",
    "\n",
    "            signal_idx = idx - lag\n",
    "            if signal_idx >= 0:\n",
    "                signal_date = dates[signal_idx]\n",
    "                signal_day = df_by_date.get(signal_date)\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # REBALANCE USING SIGNALS FROM signal_day\n",
    "        # --------------------------------------------------------\n",
    "        if do_rebalance and (signal_day is not None) and (not signal_day.empty):\n",
    "\n",
    "            univ = signal_day.copy()\n",
    "\n",
    "            # SPY regime at signal date\n",
    "            if \"market_regime\" in univ:\n",
    "                spy_regime_signal = int(univ[\"market_regime\"].iloc[0])\n",
    "            else:\n",
    "                spy_regime_signal = None\n",
    "\n",
    "            if spy_regime_signal is None or pd.isna(spy_regime_signal):\n",
    "                # Conservative default: allow buys (like live engine default)\n",
    "                spy_above_200 = True\n",
    "            else:\n",
    "                spy_above_200 = (spy_regime_signal > 0)\n",
    "\n",
    "            # Rankable universe by slope_adj at signal date\n",
    "            rankable = univ[univ[\"slope_adj\"].notna()].copy()\n",
    "            rankable = rankable.sort_values(\"slope_adj\", ascending=False)\n",
    "\n",
    "            if len(rankable) > 0:\n",
    "                cutoff = rankable[\"slope_adj\"].quantile(TOP_PERCENTILE)\n",
    "                top_group = rankable[rankable[\"slope_adj\"] >= cutoff].copy()\n",
    "            else:\n",
    "                top_group = rankable.iloc[0:0].copy()\n",
    "\n",
    "            # Assign ranks within top group\n",
    "            if not top_group.empty:\n",
    "                top_group = top_group.sort_values(\"slope_adj\", ascending=False)\n",
    "                top_group[\"slope_rank_within_top\"] = np.arange(1, len(top_group) + 1)\n",
    "                top_tickers = set(top_group[\"ticker\"].values)\n",
    "                rank_map = dict(zip(top_group[\"ticker\"], top_group[\"slope_rank_within_top\"]))\n",
    "            else:\n",
    "                top_tickers = set()\n",
    "                rank_map = {}\n",
    "\n",
    "            # Filter for valid ATR and price (on signal date)\n",
    "            tg = top_group[\n",
    "                top_group[\"atr20\"].notna() &\n",
    "                (top_group[\"atr20\"] > 0) &\n",
    "                top_group[\"close_adj\"].notna() &\n",
    "                (top_group[\"close_adj\"] > 0)\n",
    "            ].copy()\n",
    "\n",
    "            if not tg.empty:\n",
    "                inv_vol = 1.0 / tg[\"atr20\"].astype(float)\n",
    "                total_inv_vol = inv_vol.sum()\n",
    "            else:\n",
    "                inv_vol = None\n",
    "                total_inv_vol = 0.0\n",
    "\n",
    "            if total_inv_vol > 0:\n",
    "                tg = tg.assign(inv_vol=inv_vol)\n",
    "                # Use last_equity for sizing same as live engine\n",
    "                tg[\"target_value_signal\"] = last_equity * tg[\"inv_vol\"] / total_inv_vol\n",
    "                # Sizing based on signal-date close_adj\n",
    "                tg[\"target_shares_signal\"] = np.floor(\n",
    "                    tg[\"target_value_signal\"] / tg[\"close_adj\"]\n",
    "                ).astype(int)\n",
    "                tg = tg[tg[\"target_shares_signal\"] > 0]\n",
    "            else:\n",
    "                tg = tg.iloc[0:0].copy()\n",
    "\n",
    "            # Quick lookup for signal-day sizing\n",
    "            signal_sizing = {\n",
    "                row[\"ticker\"]: row\n",
    "                for _, row in tg.iterrows()\n",
    "            }\n",
    "\n",
    "            # ------------------------\n",
    "            # SELL LOGIC\n",
    "            # ------------------------\n",
    "            sells = []\n",
    "\n",
    "            for t, pos in list(positions.items()):\n",
    "                # If a name drops OUT of the signal top group -> sell all\n",
    "                if t not in top_tickers:\n",
    "                    sells.append(t)\n",
    "\n",
    "            for t in sells:\n",
    "                px_exec = fast_price_lookup(px_by_ticker[t], date)\n",
    "                value = float(positions[t][\"shares\"] * px_exec) if not np.isnan(px_exec) else 0.0\n",
    "                cash += value\n",
    "\n",
    "                history.append({\n",
    "                    \"date\": date,\n",
    "                    \"ticker\": t,\n",
    "                    \"type\": \"SELL\",\n",
    "                    \"shares\": positions[t][\"shares\"],\n",
    "                    \"price\": float(px_exec),\n",
    "                    \"value\": value,\n",
    "                    \"reason\": \"not_in_signal_top_group\",\n",
    "                    \"signal_lag_days\": lag if random_lag else signal_lag_days,\n",
    "                    \"signal_date\": signal_day[\"date\"].iloc[0],\n",
    "                    \"spy_above_200dma_signal\": spy_above_200,\n",
    "                    \"signal_slope_adj\": np.nan,\n",
    "                    \"signal_rank_within_top\": np.nan,\n",
    "                })\n",
    "\n",
    "                del positions[t]\n",
    "\n",
    "            # ------------------------\n",
    "            # BUY / REBALANCE LOGIC\n",
    "            # ------------------------\n",
    "            # Only consider buys / rebalancing if SPY is bullish at signal date\n",
    "            if spy_above_200 and not tg.empty:\n",
    "\n",
    "                # Compute portfolio value *as of execution date* to get weights\n",
    "                total_portfolio_value = cash\n",
    "                for t, pos in positions.items():\n",
    "                    px_today = fast_price_lookup(px_by_ticker[t], date)\n",
    "                    if not np.isnan(px_today):\n",
    "                        total_portfolio_value += pos[\"shares\"] * px_today\n",
    "\n",
    "                if total_portfolio_value <= 0:\n",
    "                    total_portfolio_value = last_equity\n",
    "\n",
    "                # Loop over each signal-day top ticker, apply drift / min-size rules\n",
    "                for ticker, row_sig in signal_sizing.items():\n",
    "                    # Signal-day sizing\n",
    "                    target_sh_signal = int(row_sig[\"target_shares_signal\"])\n",
    "                    signal_price = float(row_sig[\"close_adj\"])\n",
    "                    signal_slope = float(row_sig[\"slope_adj\"])\n",
    "                    signal_rank = int(row_sig[\"slope_rank_within_top\"])\n",
    "\n",
    "                    # Execution price today\n",
    "                    px_today = fast_price_lookup(px_by_ticker[ticker], date)\n",
    "                    if np.isnan(px_today):\n",
    "                        continue\n",
    "\n",
    "                    # Value of target position at today's prices\n",
    "                    target_value_today = target_sh_signal * px_today\n",
    "                    target_weight = target_value_today / total_portfolio_value\n",
    "\n",
    "                    current_sh = positions.get(ticker, {}).get(\"shares\", 0)\n",
    "                    current_value_today = current_sh * px_today\n",
    "                    current_weight = current_value_today / total_portfolio_value if total_portfolio_value > 0 else 0.0\n",
    "\n",
    "                    weight_diff = abs(target_weight - current_weight)\n",
    "\n",
    "                    # --- Drift threshold ---\n",
    "                    if weight_diff < DRIFT_THRESHOLD:\n",
    "                        continue\n",
    "\n",
    "                    # --- For NEW positions, enforce min weight ---\n",
    "                    if current_sh == 0 and target_weight < MIN_NEW_POSITION_WEIGHT:\n",
    "                        continue\n",
    "\n",
    "                    # Determine direction\n",
    "                    if target_sh_signal > current_sh:\n",
    "                        # BUY\n",
    "                        diff_sh = target_sh_signal - current_sh\n",
    "                        trade_value = diff_sh * px_today\n",
    "\n",
    "                        # --- Min trade size ---\n",
    "                        if abs(trade_value) < MIN_TRADE_VALUE:\n",
    "                            continue\n",
    "\n",
    "                        if trade_value > cash:\n",
    "                            continue\n",
    "\n",
    "                        cash -= trade_value\n",
    "                        positions[ticker] = {\n",
    "                            \"shares\": current_sh + diff_sh,\n",
    "                            \"entry\": px_today if current_sh == 0 else positions[ticker][\"entry\"],\n",
    "                        }\n",
    "\n",
    "                        history.append({\n",
    "                            \"date\": date,\n",
    "                            \"ticker\": ticker,\n",
    "                            \"type\": \"BUY\",\n",
    "                            \"shares\": diff_sh,\n",
    "                            \"price\": float(px_today),\n",
    "                            \"value\": float(trade_value),\n",
    "                            \"reason\": \"rebalance_up\" if current_sh > 0 else \"new_entry\",\n",
    "                            \"signal_lag_days\": lag if random_lag else signal_lag_days,\n",
    "                            \"signal_date\": signal_day[\"date\"].iloc[0],\n",
    "                            \"spy_above_200dma_signal\": spy_above_200,\n",
    "                            \"signal_slope_adj\": signal_slope,\n",
    "                            \"signal_rank_within_top\": signal_rank,\n",
    "                        })\n",
    "\n",
    "                    elif target_sh_signal < current_sh:\n",
    "                        # SELL (partial)\n",
    "                        diff_sh = current_sh - target_sh_signal\n",
    "                        trade_value = diff_sh * px_today\n",
    "\n",
    "                        # --- Min trade size ---\n",
    "                        if abs(trade_value) < MIN_TRADE_VALUE:\n",
    "                            continue\n",
    "\n",
    "                        cash += trade_value\n",
    "                        new_sh = current_sh - diff_sh\n",
    "\n",
    "                        if new_sh <= 0:\n",
    "                            positions.pop(ticker, None)\n",
    "                        else:\n",
    "                            positions[ticker][\"shares\"] = new_sh\n",
    "\n",
    "                        history.append({\n",
    "                            \"date\": date,\n",
    "                            \"ticker\": ticker,\n",
    "                            \"type\": \"SELL\",\n",
    "                            \"shares\": diff_sh,\n",
    "                            \"price\": float(px_today),\n",
    "                            \"value\": float(trade_value),\n",
    "                            \"reason\": \"rebalance_down\",\n",
    "                            \"signal_lag_days\": lag if random_lag else signal_lag_days,\n",
    "                            \"signal_date\": signal_day[\"date\"].iloc[0],\n",
    "                            \"spy_above_200dma_signal\": spy_above_200,\n",
    "                            \"signal_slope_adj\": signal_slope,\n",
    "                            \"signal_rank_within_top\": signal_rank,\n",
    "                        })\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # REBALANCE-DAY POSITION / WEIGHT SUMMARY\n",
    "            # ------------------------------------------------\n",
    "            # Use post-trade positions & today's prices\n",
    "            total_value_today = cash\n",
    "            weights = []\n",
    "            for t, pos in positions.items():\n",
    "                px_today = fast_price_lookup(px_by_ticker[t], date)\n",
    "                if not np.isnan(px_today):\n",
    "                    total_value_today += pos[\"shares\"] * px_today\n",
    "\n",
    "            if total_value_today > 0:\n",
    "                for t, pos in positions.items():\n",
    "                    px_today = fast_price_lookup(px_by_ticker[t], date)\n",
    "                    if not np.isnan(px_today):\n",
    "                        w = (pos[\"shares\"] * px_today) / total_value_today\n",
    "                        weights.append(w)\n",
    "\n",
    "            rebal_summaries.append({\n",
    "                \"date\": date,\n",
    "                \"label\": label,\n",
    "                \"num_positions\": len(positions),\n",
    "                \"spy_above_200dma_signal\": spy_above_200,\n",
    "                \"avg_weight\": float(np.mean(weights)) if weights else np.nan,\n",
    "                \"max_weight\": float(np.max(weights)) if weights else np.nan,\n",
    "                \"min_weight\": float(np.min(weights)) if weights else np.nan,\n",
    "                \"top5_weight_sum\": float(np.sum(sorted(weights, reverse=True)[:5])) if len(weights) >= 1 else np.nan,\n",
    "            })\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # DAILY MARK-TO-MARKET\n",
    "        # --------------------------------------------------------\n",
    "        equity_val = 0.0\n",
    "        for t, pos in positions.items():\n",
    "            px = fast_price_lookup(px_by_ticker[t], date)\n",
    "            if np.isnan(px):\n",
    "                continue\n",
    "            equity_val += pos[\"shares\"] * px\n",
    "\n",
    "        portfolio_value = cash + equity_val\n",
    "        last_equity = portfolio_value\n",
    "\n",
    "        equity_curve.append({\n",
    "            \"date\": date,\n",
    "            \"portfolio_value\": portfolio_value,\n",
    "            \"cash\": cash,\n",
    "            \"num_positions\": len(positions),\n",
    "            \"label\": label,\n",
    "        })\n",
    "\n",
    "    trades = pd.DataFrame(history)\n",
    "    eq_df  = pd.DataFrame(equity_curve)\n",
    "    rebal_df = pd.DataFrame(rebal_summaries)\n",
    "\n",
    "    # Save outputs\n",
    "    trades_path = os.path.join(OUTPUT_DIR, f\"trades_{label}.parquet\")\n",
    "    eq_path     = os.path.join(OUTPUT_DIR, f\"equity_{label}.parquet\")\n",
    "    rebal_path  = os.path.join(OUTPUT_DIR, f\"rebal_summary_{label}.parquet\")\n",
    "\n",
    "    trades.to_parquet(trades_path, index=False)\n",
    "    eq_df.to_parquet(eq_path, index=False)\n",
    "    rebal_df.to_parquet(rebal_path, index=False)\n",
    "\n",
    "    print(f\"Saved trades       → {trades_path}\")\n",
    "    print(f\"Saved equity curve → {eq_path}\")\n",
    "    print(f\"Saved rebalance    → {rebal_path}\")\n",
    "\n",
    "    # Stats\n",
    "    eq_series = eq_df.set_index(\"date\")[\"portfolio_value\"]\n",
    "    stats = show_stats(label, eq_series, trades)\n",
    "\n",
    "    # Rebalance stats: average positions, max weight, etc.\n",
    "    if not rebal_df.empty:\n",
    "        stats[\"AvgPositions\"] = rebal_df[\"num_positions\"].mean()\n",
    "        stats[\"MedPositions\"] = rebal_df[\"num_positions\"].median()\n",
    "        stats[\"AvgMaxWeight\"] = rebal_df[\"max_weight\"].mean()\n",
    "        stats[\"AvgTop5WeightSum\"] = rebal_df[\"top5_weight_sum\"].mean()\n",
    "    else:\n",
    "        stats[\"AvgPositions\"] = np.nan\n",
    "        stats[\"MedPositions\"] = np.nan\n",
    "        stats[\"AvgMaxWeight\"] = np.nan\n",
    "        stats[\"AvgTop5WeightSum\"] = np.nan\n",
    "\n",
    "    return eq_df, trades, rebal_df, stats\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN: RUN LOOKAHEAD AUDIT SCENARIOS\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    print(\"\\n=== LOOKAHEAD AUDIT FOR REGRESSION+SPYFILTER SYSTEM ===\")\n",
    "    print(f\"Start trading: {START_TRADING.date()}\")\n",
    "    print(f\"Initial capital: ${INITIAL_CAPITAL:,.2f}\")\n",
    "    print(f\"Top percentile (slope_adj): {TOP_PERCENTILE:.2f}\")\n",
    "    print(f\"Rebalance day: {REBALANCE_DAY}\")\n",
    "    print(f\"Lag scenarios: {LAG_SCENARIOS}, random lag 0–{RAND_LAG_MAX}\\n\")\n",
    "\n",
    "    all_stats = []\n",
    "\n",
    "    # 1) Deterministic lag scenarios: 0, 1, 3, 5 days\n",
    "    for lag in LAG_SCENARIOS:\n",
    "        eq_df, trades, rebal_df, st = run_strategy_with_lag(\n",
    "            signal_lag_days=lag,\n",
    "            random_lag=False,\n",
    "            label=f\"lag_{lag}d\"\n",
    "        )\n",
    "        all_stats.append(st)\n",
    "\n",
    "    # 2) Random lag scenario: 0–RAND_LAG_MAX days\n",
    "    eq_rand, trades_rand, rebal_rand, st_rand = run_strategy_with_lag(\n",
    "        random_lag=True,\n",
    "        max_random_lag=RAND_LAG_MAX,\n",
    "        label=f\"rand_lag_0_{RAND_LAG_MAX}\"\n",
    "    )\n",
    "    all_stats.append(st_rand)\n",
    "\n",
    "    # Summary table\n",
    "    stats_df = pd.DataFrame(all_stats)\n",
    "    stats_df[\"CAGR_%\"]   = stats_df[\"CAGR\"]   * 100.0\n",
    "    stats_df[\"MaxDD_%\"]  = stats_df[\"MaxDD\"]  * 100.0\n",
    "\n",
    "    cols_order = [\n",
    "        \"label\",\n",
    "        \"CAGR_%\", \"MaxDD_%\", \"Sharpe\", \"Sortino\",\n",
    "        \"Trades\", \"Buys\", \"Sells\", \"AvgTradeValue\", \"TurnoverPerYear\",\n",
    "        \"AvgBuyRank\", \"MedBuyRank\", \"BuyRankP90\",\n",
    "        \"AvgPositions\", \"MedPositions\",\n",
    "        \"AvgMaxWeight\", \"AvgTop5WeightSum\",\n",
    "    ]\n",
    "    # Only keep columns that exist\n",
    "    cols_order = [c for c in cols_order if c in stats_df.columns]\n",
    "    stats_df = stats_df[cols_order].sort_values(\"CAGR_%\", ascending=False)\n",
    "\n",
    "    print(\"\\n=== LOOKAHEAD AUDIT SUMMARY ===\")\n",
    "    print(stats_df.to_string(index=False, float_format=lambda x: f\"{x:6.2f}\"))\n",
    "\n",
    "    summary_path = os.path.join(OUTPUT_DIR, \"lookahead_audit_stats_regression_spyfilter.csv\")\n",
    "    stats_df.to_csv(summary_path, index=False)\n",
    "    print(f\"\\nSaved audit stats → {summary_path}\")\n",
    "    print(\"\\n=== LOOKAHEAD AUDIT COMPLETE ===\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
