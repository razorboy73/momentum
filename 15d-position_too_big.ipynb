{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d814d587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Flagged rows: 2,627\n",
      "✔ Wrote: 15d-position_too_big_concentration_flags\\position_over_12pct_flags.csv\n",
      "✔ Wrote: 15d-position_too_big_concentration_flags\\position_over_12pct_flags.parquet\n",
      "\n",
      "Top 20 by weight:\n",
      "      date ticker   shares  price_used  position_value  portfolio_value   weight  threshold       price_source\n",
      "2013-01-29   NFLX 271382.0       2.416      655658.912     3.581358e+06 0.183075       0.12 UNIVERSE_close_adj\n",
      "2013-01-30   NFLX 271382.0       2.396      650231.272     3.561932e+06 0.182550       0.12 UNIVERSE_close_adj\n",
      "2013-01-25   NFLX 271382.0       2.422      657287.204     3.611174e+06 0.182015       0.12 UNIVERSE_close_adj\n",
      "2013-01-28   NFLX 271382.0       2.316      628520.712     3.577133e+06 0.175705       0.12 UNIVERSE_close_adj\n",
      "2013-01-24   NFLX 271382.0       2.098      569359.436     3.521752e+06 0.161669       0.12 UNIVERSE_close_adj\n",
      "2009-09-16    GGP 118381.0       2.792      330519.752     2.050077e+06 0.161223       0.12 UNIVERSE_close_adj\n",
      "2011-01-12   NVDA 805744.0       0.535      431073.040     2.758821e+06 0.156253       0.12 UNIVERSE_close_adj\n",
      "1999-11-08   AAPL 106582.0       0.723       77058.786     4.974728e+05 0.154900       0.12 UNIVERSE_close_adj\n",
      "2003-05-09   NVDA 758700.0       0.163      123668.100     8.142282e+05 0.151884       0.12 UNIVERSE_close_adj\n",
      "1999-11-10   AAPL 106582.0       0.686       73115.252     4.954218e+05 0.147582       0.12 UNIVERSE_close_adj\n",
      "2003-05-12   NVDA 758700.0       0.159      120633.300     8.200845e+05 0.147099       0.12 UNIVERSE_close_adj\n",
      "2000-03-14   ADSK   8824.0      12.757      112567.768     7.662714e+05 0.146903       0.12 UNIVERSE_close_adj\n",
      "1999-11-09   AAPL 106582.0       0.672       71623.104     4.916484e+05 0.145680       0.12 UNIVERSE_close_adj\n",
      "2000-03-15    LHX   9727.0      11.423      111111.521     7.644544e+05 0.145347       0.12 UNIVERSE_close_adj\n",
      "2003-05-30   NVDA 634178.0       0.200      126835.600     8.733574e+05 0.145228       0.12 UNIVERSE_close_adj\n",
      "1999-11-05   AAPL 106582.0       0.662       70557.284     4.861124e+05 0.145146       0.12 UNIVERSE_close_adj\n",
      "2007-07-25   AMZN  54774.0       4.309      236021.166     1.631861e+06 0.144633       0.12 UNIVERSE_close_adj\n",
      "2003-05-13   NVDA 758700.0       0.156      118357.200     8.184728e+05 0.144607       0.12 UNIVERSE_close_adj\n",
      "2000-02-16  COMS1   6397.0      15.197       97215.209     6.735212e+05 0.144339       0.12 UNIVERSE_close_adj\n",
      "2000-03-10   ADSK   8824.0      13.272      117112.128     8.117817e+05 0.144266       0.12 UNIVERSE_close_adj\n",
      "\n",
      "✔ Wrote summary: 15d-position_too_big_concentration_flags\\position_over_12pct_summary.csv\n",
      "\n",
      "Top 20 tickers by max_weight:\n",
      "ticker first_date  last_date  max_weight  max_position_value  n_days_flagged\n",
      "  NFLX 2011-07-07 2015-07-22    0.183075          700597.825             149\n",
      "   GGP 2009-08-12 2009-11-25    0.161223          330519.752              10\n",
      "  NVDA 2003-05-08 2023-05-31    0.156253         2152950.082             493\n",
      "  AAPL 1999-08-26 2008-01-03    0.154900          226608.387             289\n",
      "  ADSK 2000-02-07 2000-03-15    0.146903          117112.128              23\n",
      "   LHX 2000-03-02 2000-03-15    0.145347          111111.521              10\n",
      "  AMZN 2007-05-21 2009-12-02    0.144633          263072.880              47\n",
      " COMS1 1999-12-14 2000-03-01    0.144339          107515.616              12\n",
      "   FCX 1999-06-03 2020-11-11    0.140129         1341783.384              47\n",
      "  INTC 2025-11-28 2025-12-03    0.139587         2909558.640               4\n",
      "   MRO 2018-01-12 2023-11-02    0.139354         2002271.634              14\n",
      "    HP 2000-04-20 2000-05-17    0.139139          101036.400              14\n",
      "   WBD 2025-01-27 2025-12-24    0.138843         3005651.990              30\n",
      "   AMD 2015-12-17 2016-07-27    0.135917          714178.900              23\n",
      "  AVGO 2015-02-05 2015-03-18    0.135297          650915.453              22\n",
      "  FDC1 1999-03-04 2007-06-12    0.134717          189667.380              14\n",
      "     F 2009-07-31 2021-04-16    0.134257         1734926.184              13\n",
      "   RAI 2003-11-20 2003-12-22    0.133979          122137.452              20\n",
      "  BBWI 1999-05-03 1999-05-12    0.133699           51069.135               8\n",
      "   GME 2011-05-19 2015-08-27    0.133348          641496.056              33\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\"\"\"\n",
    "Detect position concentration breaches (>THRESHOLD of portfolio value).\n",
    "\n",
    "Fixes vs original:\n",
    "1) Computes weights for EVERY day in the equity curve (not just trade days).\n",
    "2) Uses DAILY close_adj pricing (from UNIVERSE_FILE or PRICE_DIR) instead of stale last-trade price.\n",
    "3) Uses \"last known close at or before date\" fallback for missing prices (e.g., delistings / gaps).\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# =========================\n",
    "# USER SETTINGS\n",
    "# =========================\n",
    "TRADES_FILE   = Path(\"./13-trading_output_regression_insp500_spyfilter_cap15/13-trades_regression_insp500_spyfilter_cap15.parquet\")\n",
    "EQUITY_FILE   = Path(\"./13-trading_output_regression_insp500_spyfilter_cap15/13-equity_curve_regression_insp500_spyfilter_cap15.parquet\")\n",
    "UNIVERSE_FILE = Path(\"./12-tradable_sp500_universe/12-tradable_sp500_universe.parquet\")  # used for close_adj prices if PRICE_DIR is None\n",
    "\n",
    "# Optional folder containing per-ticker parquet OHLC/adj prices like \"APA.parquet\", \"NVDA.parquet\", etc.\n",
    "# If set, the script will use that day's close (or best available column) to compute position weights.\n",
    "PRICE_DIR: Optional[Path] = None\n",
    "\n",
    "THRESHOLD = 0.12   # 15%\n",
    "OUT_DIR = Path(\"./15d-position_too_big_concentration_flags\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def pick_first_existing(cols: List[str], available: List[str]) -> Optional[str]:\n",
    "    for c in cols:\n",
    "        if c in available:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def fast_price_lookup(px_array: np.ndarray, date_val: pd.Timestamp) -> float:\n",
    "    \"\"\"\n",
    "    Structured array with fields ['date','px'], return last known px at or before date_val.\n",
    "    \"\"\"\n",
    "    date64 = np.datetime64(date_val.normalize(), \"ns\")\n",
    "    dates = px_array[\"date\"]\n",
    "    idx = np.searchsorted(dates, date64, side=\"right\") - 1\n",
    "    if idx < 0:\n",
    "        return np.nan\n",
    "    return float(px_array[\"px\"][idx])\n",
    "\n",
    "\n",
    "def load_trades(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(path)\n",
    "\n",
    "    date_col = pick_first_existing([\"exec_date\", \"date\", \"signal_date\"], list(df.columns))\n",
    "    if not date_col:\n",
    "        raise KeyError(f\"No trade date column found. Columns: {list(df.columns)}\")\n",
    "\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[date_col]).copy()\n",
    "    df.rename(columns={date_col: \"trade_dt\"}, inplace=True)\n",
    "\n",
    "    if \"ticker\" not in df.columns:\n",
    "        raise KeyError(\"Trades file missing 'ticker' column.\")\n",
    "    if \"type\" not in df.columns:\n",
    "        raise KeyError(\"Trades file missing 'type' column (BUY/SELL).\")\n",
    "    if \"shares\" not in df.columns:\n",
    "        raise KeyError(\"Trades file missing 'shares' column.\")\n",
    "\n",
    "    price_col = pick_first_existing([\"price\", \"exec_open_adj\", \"signal_close_adj\"], list(df.columns))\n",
    "    if not price_col:\n",
    "        raise KeyError(f\"No price-like column found. Columns: {list(df.columns)}\")\n",
    "    df.rename(columns={price_col: \"exec_price\"}, inplace=True)\n",
    "\n",
    "    port_col = pick_first_existing(\n",
    "        [\"portfolio_after\", \"equity_after\", \"portfolio_before\", \"equity_before\"],\n",
    "        list(df.columns),\n",
    "    )\n",
    "    if port_col:\n",
    "        df.rename(columns={port_col: \"portfolio_fallback\"}, inplace=True)\n",
    "    else:\n",
    "        df[\"portfolio_fallback\"] = pd.NA\n",
    "\n",
    "    df[\"side\"] = df[\"type\"].astype(str).str.upper().str.strip()\n",
    "    df[\"shares\"] = pd.to_numeric(df[\"shares\"], errors=\"coerce\").fillna(0.0)\n",
    "    df[\"exec_price\"] = pd.to_numeric(df[\"exec_price\"], errors=\"coerce\")\n",
    "\n",
    "    sort_cols = [\"trade_dt\"]\n",
    "    if \"trade_id\" in df.columns:\n",
    "        sort_cols.append(\"trade_id\")\n",
    "    df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    df[\"trade_date\"] = df[\"trade_dt\"].dt.normalize()\n",
    "    df[\"ticker\"] = df[\"ticker\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_equity_curve(path: Path) -> pd.DataFrame:\n",
    "    eq = pd.read_parquet(path)\n",
    "\n",
    "    date_col = pick_first_existing([\"date\", \"Date\", \"datetime\", \"timestamp\"], list(eq.columns))\n",
    "    if not date_col:\n",
    "        raise KeyError(f\"Equity curve missing date column. Columns: {list(eq.columns)}\")\n",
    "\n",
    "    eq[date_col] = pd.to_datetime(eq[date_col], errors=\"coerce\")\n",
    "    eq = eq.dropna(subset=[date_col]).copy()\n",
    "    eq.rename(columns={date_col: \"date\"}, inplace=True)\n",
    "    eq[\"date\"] = eq[\"date\"].dt.normalize()\n",
    "\n",
    "    val_col = pick_first_existing(\n",
    "        [\"portfolio_value\", \"portfolio\", \"equity\", \"equity_value\", \"total\", \"account_value\"],\n",
    "        list(eq.columns),\n",
    "    )\n",
    "    if not val_col:\n",
    "        raise KeyError(f\"Equity curve missing portfolio value column. Columns: {list(eq.columns)}\")\n",
    "\n",
    "    eq.rename(columns={val_col: \"portfolio_value\"}, inplace=True)\n",
    "    eq[\"portfolio_value\"] = pd.to_numeric(eq[\"portfolio_value\"], errors=\"coerce\")\n",
    "    eq = eq.dropna(subset=[\"portfolio_value\"])\n",
    "\n",
    "    return eq[[\"date\", \"portfolio_value\"]].drop_duplicates(\"date\").set_index(\"date\").sort_index()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PriceCache:\n",
    "    price_dir: Path\n",
    "    cache: Dict[str, pd.DataFrame]\n",
    "\n",
    "    def __init__(self, price_dir: Path):\n",
    "        self.price_dir = price_dir\n",
    "        self.cache = {}\n",
    "\n",
    "    def _load(self, ticker: str) -> Optional[pd.DataFrame]:\n",
    "        p = self.price_dir / f\"{ticker}.parquet\"\n",
    "        if not p.exists():\n",
    "            return None\n",
    "\n",
    "        df = pd.read_parquet(p)\n",
    "        date_col = pick_first_existing([\"date\", \"Date\", \"datetime\", \"timestamp\"], list(df.columns))\n",
    "        if not date_col:\n",
    "            return None\n",
    "\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[date_col]).copy()\n",
    "        df.rename(columns={date_col: \"date\"}, inplace=True)\n",
    "        df[\"date\"] = df[\"date\"].dt.normalize()\n",
    "\n",
    "        px_col = pick_first_existing(\n",
    "            [\"close_adj\", \"adj_close\", \"close\", \"Close\", \"close_price\", \"price\"],\n",
    "            list(df.columns),\n",
    "        )\n",
    "        if not px_col:\n",
    "            px_col = pick_first_existing([\"open_adj\", \"open\", \"Open\"], list(df.columns))\n",
    "        if not px_col:\n",
    "            return None\n",
    "\n",
    "        df.rename(columns={px_col: \"px\"}, inplace=True)\n",
    "        df[\"px\"] = pd.to_numeric(df[\"px\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"px\"])\n",
    "\n",
    "        return df[[\"date\", \"px\"]].drop_duplicates(\"date\").set_index(\"date\").sort_index()\n",
    "\n",
    "    def get_price(self, ticker: str, date: pd.Timestamp) -> Optional[float]:\n",
    "        if ticker not in self.cache:\n",
    "            self.cache[ticker] = self._load(ticker)  # may be None\n",
    "        tdf = self.cache.get(ticker)\n",
    "        if tdf is None:\n",
    "            return None\n",
    "        # last known at or before date\n",
    "        try:\n",
    "            loc = tdf.index.searchsorted(date, side=\"right\") - 1\n",
    "            if loc < 0:\n",
    "                return None\n",
    "            return float(tdf.iloc[loc][\"px\"])\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "def build_universe_price_arrays(universe_file: Path) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build per-ticker structured arrays for close_adj prices:\n",
    "      arr dtype=[('date','datetime64[ns]'),('px','float64')]\n",
    "    \"\"\"\n",
    "    if not universe_file.exists():\n",
    "        return {}\n",
    "\n",
    "    u = pd.read_parquet(universe_file, columns=[\"date\", \"ticker\", \"close_adj\"])\n",
    "    u[\"date\"] = pd.to_datetime(u[\"date\"])\n",
    "    u[\"ticker\"] = u[\"ticker\"].astype(str).str.upper().str.strip()\n",
    "    u[\"close_adj\"] = pd.to_numeric(u[\"close_adj\"], errors=\"coerce\")\n",
    "    u = u.dropna(subset=[\"date\", \"ticker\", \"close_adj\"])\n",
    "    u = u.sort_values([\"ticker\", \"date\"])\n",
    "\n",
    "    px_by_ticker: Dict[str, np.ndarray] = {}\n",
    "    for t, sub in u.groupby(\"ticker\", sort=False):\n",
    "        arr = np.zeros(len(sub), dtype=[(\"date\", \"datetime64[ns]\"), (\"px\", \"float64\")])\n",
    "        arr[\"date\"] = sub[\"date\"].values.astype(\"datetime64[ns]\")\n",
    "        arr[\"px\"] = sub[\"close_adj\"].astype(float).values\n",
    "        px_by_ticker[t] = arr\n",
    "\n",
    "    return px_by_ticker\n",
    "\n",
    "\n",
    "def main():\n",
    "    trades = load_trades(TRADES_FILE)\n",
    "    eq_index = load_equity_curve(EQUITY_FILE)\n",
    "\n",
    "    # Group trades by day for fast access\n",
    "    trades_by_day = {d: g for d, g in trades.groupby(\"trade_date\", sort=True)}\n",
    "\n",
    "    # Price sources\n",
    "    price_cache = PriceCache(PRICE_DIR) if PRICE_DIR else None\n",
    "    universe_px = {} if PRICE_DIR else build_universe_price_arrays(UNIVERSE_FILE)\n",
    "\n",
    "    holdings_shares: Dict[str, float] = {}\n",
    "    last_trade_px: Dict[str, float] = {}\n",
    "\n",
    "    flags: List[Dict[str, Any]] = []\n",
    "\n",
    "    # IMPORTANT FIX: iterate ALL equity days (not just trade days)\n",
    "    for day in eq_index.index:\n",
    "        # Apply trades (if any) that occurred on this day\n",
    "        day_trades = trades_by_day.get(day)\n",
    "        if day_trades is not None:\n",
    "            for _, r in day_trades.iterrows():\n",
    "                ticker = str(r[\"ticker\"]).upper().strip()\n",
    "                side = str(r[\"side\"])\n",
    "                shares = float(r[\"shares\"])\n",
    "                px = r[\"exec_price\"]\n",
    "\n",
    "                if pd.notna(px):\n",
    "                    last_trade_px[ticker] = float(px)\n",
    "\n",
    "                cur = holdings_shares.get(ticker, 0.0)\n",
    "                if \"BUY\" in side:\n",
    "                    cur += shares\n",
    "                elif \"SELL\" in side:\n",
    "                    cur -= shares\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if abs(cur) < 1e-9:\n",
    "                    cur = 0.0\n",
    "                holdings_shares[ticker] = cur\n",
    "\n",
    "        portfolio_value = float(eq_index.loc[day, \"portfolio_value\"])\n",
    "        if not np.isfinite(portfolio_value) or portfolio_value <= 0:\n",
    "            continue\n",
    "\n",
    "        # Compute weights for ALL currently-held tickers\n",
    "        for ticker, sh in holdings_shares.items():\n",
    "            if sh == 0.0:\n",
    "                continue\n",
    "\n",
    "            px_today: Optional[float] = None\n",
    "            price_source = None\n",
    "\n",
    "            # 1) Preferred: PRICE_DIR daily close (if configured)\n",
    "            if price_cache is not None:\n",
    "                px_today = price_cache.get_price(ticker, day)\n",
    "                if px_today is not None and px_today > 0:\n",
    "                    price_source = \"PRICE_DIR\"\n",
    "\n",
    "            # 2) Next best: UNIVERSE_FILE close_adj (matches your strategy)\n",
    "            if (px_today is None or px_today <= 0) and (ticker in universe_px):\n",
    "                p = fast_price_lookup(universe_px[ticker], day)\n",
    "                if np.isfinite(p) and p > 0:\n",
    "                    px_today = float(p)\n",
    "                    price_source = \"UNIVERSE_close_adj\"\n",
    "\n",
    "            # 3) Last resort: last trade price (stale; only use if no daily data)\n",
    "            if px_today is None or px_today <= 0:\n",
    "                p = last_trade_px.get(ticker)\n",
    "                if p is not None and p > 0:\n",
    "                    px_today = float(p)\n",
    "                    price_source = \"last_trade_price\"\n",
    "\n",
    "            if px_today is None or px_today <= 0:\n",
    "                continue\n",
    "\n",
    "            pos_value = abs(sh) * float(px_today)\n",
    "            weight = pos_value / portfolio_value\n",
    "\n",
    "            if weight > THRESHOLD:\n",
    "                flags.append({\n",
    "                    \"date\": day,\n",
    "                    \"ticker\": ticker,\n",
    "                    \"shares\": sh,\n",
    "                    \"price_used\": px_today,\n",
    "                    \"position_value\": pos_value,\n",
    "                    \"portfolio_value\": portfolio_value,\n",
    "                    \"weight\": weight,\n",
    "                    \"threshold\": THRESHOLD,\n",
    "                    \"price_source\": price_source,\n",
    "                })\n",
    "\n",
    "    flags_df = pd.DataFrame(flags)\n",
    "    if not flags_df.empty:\n",
    "        flags_df = flags_df.sort_values([\"date\", \"weight\"], ascending=[True, False])\n",
    "\n",
    "    out_csv = OUT_DIR / f\"position_over_{int(THRESHOLD*100)}pct_flags.csv\"\n",
    "    out_parquet = OUT_DIR / f\"position_over_{int(THRESHOLD*100)}pct_flags.parquet\"\n",
    "    flags_df.to_csv(out_csv, index=False)\n",
    "    flags_df.to_parquet(out_parquet, index=False)\n",
    "\n",
    "    print(f\"✔ Flagged rows: {len(flags_df):,}\")\n",
    "    print(f\"✔ Wrote: {out_csv}\")\n",
    "    print(f\"✔ Wrote: {out_parquet}\")\n",
    "\n",
    "    if len(flags_df) > 0:\n",
    "        print(\"\\nTop 20 by weight:\")\n",
    "        print(flags_df.sort_values(\"weight\", ascending=False).head(20).to_string(index=False))\n",
    "\n",
    "        summary = (\n",
    "            flags_df.groupby(\"ticker\", as_index=False)\n",
    "            .agg(\n",
    "                first_date=(\"date\", \"min\"),\n",
    "                last_date=(\"date\", \"max\"),\n",
    "                max_weight=(\"weight\", \"max\"),\n",
    "                max_position_value=(\"position_value\", \"max\"),\n",
    "                n_days_flagged=(\"date\", \"nunique\"),\n",
    "            )\n",
    "            .sort_values(\"max_weight\", ascending=False)\n",
    "        )\n",
    "        summary_csv = OUT_DIR / f\"position_over_{int(THRESHOLD*100)}pct_summary.csv\"\n",
    "        summary.to_csv(summary_csv, index=False)\n",
    "        print(f\"\\n✔ Wrote summary: {summary_csv}\")\n",
    "        print(\"\\nTop 20 tickers by max_weight:\")\n",
    "        print(summary.head(20).to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_quant_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
